{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, SparkSession\n",
    "from pyspark.sql.types import IntegerType, LongType, DoubleType, StringType, DoubleType\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.preliminary_analysis import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/09/17 04:32:24 WARN Utils: Your hostname, DESKTOP-H0CEB6G resolves to a loopback address: 127.0.1.1; using 172.29.253.151 instead (on interface eth0)\n",
      "24/09/17 04:32:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/17 04:32:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/17 04:32:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Merchant Fraud Model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.abs.gov.au/statistics/people/people-and-communities/snapshot-australia/2021/Snapshot%20of%20Australia%20data%20summary.xlsx\"\n",
    "\n",
    "\n",
    "# urlretrieve(url, \"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.csv', 'w') as f_out:\n",
    "    f_out.write(requests.get(url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad = spark.read.parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the necessary curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"../data/curated\"\n",
    "\n",
    "# Read merchant datasets in\n",
    "merchant_info = spark.read.parquet(f\"{path}/merchant_info.parquet\")\n",
    "merchant_fp = spark.read.parquet(f\"{path}/merchant_fp.parquet\")\n",
    "\n",
    "# Read in transactions dataset\n",
    "transactions = spark.read.parquet(f\"{path}/transactions.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the necessary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset count is  14195505\n",
      "\n",
      "\n",
      "After merchant_fp join:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset count is  14195505\n",
      "+------------+--------------+------------------+--------------------+-----------+-----------------+\n",
      "|merchant_abn|order_datetime|      dollar_value|            order_id|consumer_id|fraud_probability|\n",
      "+------------+--------------+------------------+--------------------+-----------+-----------------+\n",
      "| 79417999332|    2021-11-26|136.06570809815838|23acbb7b-cf98-458...|    1059280|             NULL|\n",
      "| 46451548968|    2021-11-26| 72.61581642788431|76bab304-fa2d-400...|    1195503|             NULL|\n",
      "| 89518629617|    2021-11-26|3.0783487174439297|a2ae446a-2959-41c...|     986886|             NULL|\n",
      "| 49167531725|    2021-11-26| 51.58228625503599|7080c274-17f7-4cc...|    1195503|             NULL|\n",
      "| 31101120643|    2021-11-26|25.228114942417797|8e301c0f-06ab-45c...|     986886|             NULL|\n",
      "+------------+--------------+------------------+--------------------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------------+--------------------------+----------------------+-------------------------+-------------------------------+\n",
      "|merchant_abn_missing_count|order_datetime_missing_count|dollar_value_missing_count|order_id_missing_count|consumer_id_missing_count|fraud_probability_missing_count|\n",
      "+--------------------------+----------------------------+--------------------------+----------------------+-------------------------+-------------------------------+\n",
      "|                         0|                           0|                         0|                     0|                        0|                       14191446|\n",
      "+--------------------------+----------------------------+--------------------------+----------------------+-------------------------+-------------------------------+\n",
      "\n",
      "After merchant_info join:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset count is  13614675\n",
      "+------------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+\n",
      "|merchant_abn|order_datetime|      dollar_value|            order_id|consumer_id|fraud_probability|                name|            category|revenue_level|take_rate|\n",
      "+------------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+\n",
      "| 79417999332|    2021-11-26|136.06570809815838|23acbb7b-cf98-458...|    1059280|             NULL|Phasellus At Company|gift, card, novel...|            b|     4.95|\n",
      "| 46451548968|    2021-11-26| 72.61581642788431|76bab304-fa2d-400...|    1195503|             NULL|Tempus Eu Ligula ...|health and beauty...|            a|     6.04|\n",
      "| 89518629617|    2021-11-26|3.0783487174439297|a2ae446a-2959-41c...|     986886|             NULL|Vulputate Velit E...|tent  and awning ...|            c|     3.09|\n",
      "| 49167531725|    2021-11-26| 51.58228625503599|7080c274-17f7-4cc...|    1195503|             NULL|     Felis Institute|digital goods: bo...|            a|     6.42|\n",
      "| 31101120643|    2021-11-26|25.228114942417797|8e301c0f-06ab-45c...|     986886|             NULL|Commodo Hendrerit...|cable, satellite,...|            a|     6.37|\n",
      "+------------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------------+--------------------------+----------------------+-------------------------+-------------------------------+------------------+----------------------+---------------------------+-----------------------+\n",
      "|merchant_abn_missing_count|order_datetime_missing_count|dollar_value_missing_count|order_id_missing_count|consumer_id_missing_count|fraud_probability_missing_count|name_missing_count|category_missing_count|revenue_level_missing_count|take_rate_missing_count|\n",
      "+--------------------------+----------------------------+--------------------------+----------------------+-------------------------+-------------------------------+------------------+----------------------+---------------------------+-----------------------+\n",
      "|                         0|                           0|                         0|                     0|                        0|                       13610672|                 0|                     0|                          0|                      0|\n",
      "+--------------------------+----------------------------+--------------------------+----------------------+-------------------------+-------------------------------+------------------+----------------------+---------------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check initial dataset size\n",
    "print(\"Original dataset:\")\n",
    "get_dataset_count(transactions)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Join transaction records with merchant fraud data\n",
    "transaction_records_with_fraud = transactions.join(\n",
    "    merchant_fp, \n",
    "    on=[\"merchant_abn\", \"order_datetime\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"After merchant_fp join:\")\n",
    "\n",
    "# See how the dataset size changes along the way\n",
    "get_dataset_count(transaction_records_with_fraud)\n",
    "\n",
    "# Preview\n",
    "transaction_records_with_fraud.show(5)\n",
    "\n",
    "# Check if the join led to any missing values.\n",
    "calculate_missing_values(transaction_records_with_fraud)\n",
    "\n",
    "# Join transaction records with merchant info\n",
    "transaction_records_final = transaction_records_with_fraud.join(\n",
    "    merchant_info, \n",
    "    on=\"merchant_abn\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"After merchant_info join:\")\n",
    "\n",
    "# See how the dataset size changes along the way\n",
    "get_dataset_count(transaction_records_final)\n",
    "\n",
    "# Preview\n",
    "transaction_records_final.show(5)\n",
    "\n",
    "# See how the dataset size changes along the way\n",
    "# Check if the join led to any missing values.\n",
    "calculate_missing_values(transaction_records_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+-----------------+-------------------+---------------------+\n",
      "|merchant_abn|order_datetime|      dollar_value|            order_id|consumer_id|fraud_probability|                name|            category|revenue_level|take_rate| avg_dollar_value|stddev_dollar_value|std_diff_dollar_value|\n",
      "+------------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+-----------------+-------------------+---------------------+\n",
      "| 31101120643|    2021-11-26|25.228114942417797|8e301c0f-06ab-45c...|     986886|             NULL|Commodo Hendrerit...|cable, satellite,...|            a|     6.37|78.37805888089956|  55.43483898784632|  -0.9587823273038548|\n",
      "| 79417999332|    2021-11-26|136.06570809815838|23acbb7b-cf98-458...|    1059280|             NULL|Phasellus At Company|gift, card, novel...|            b|     4.95| 91.9759216055508| 57.954340723860284|   0.7607676308956001|\n",
      "| 49167531725|    2021-11-26| 51.58228625503599|7080c274-17f7-4cc...|    1195503|             NULL|     Felis Institute|digital goods: bo...|            a|     6.42|52.07643449204165|  40.88797494937904| -0.01208541723128...|\n",
      "| 67978471888|    2021-11-26| 691.5028234458998|0380e9ad-b0e8-420...|     179208|             NULL|Magna Malesuada C...|artist supply and...|            a|     5.56| 648.789383595735| 318.21309330658926|  0.13422904572003783|\n",
      "| 46451548968|    2021-11-26| 72.61581642788431|76bab304-fa2d-400...|    1195503|             NULL|Tempus Eu Ligula ...|health and beauty...|            a|     6.04|65.46491492502317|  65.13640918856957|  0.10978347735072903|\n",
      "+------------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+-----------------+-------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Flag unusual transactions that deviate greatly from a merchant's usual dollar value\n",
    "\n",
    "# Calculate average and standard deviation of dollar_value per merchant\n",
    "transaction_stats = transaction_records_final.groupBy(\"merchant_abn\").agg(\n",
    "    F.avg(\"dollar_value\").alias(\"avg_dollar_value\"),\n",
    "    F.stddev(\"dollar_value\").alias(\"stddev_dollar_value\")\n",
    ")\n",
    "\n",
    "# Join the stats back to the original dataset\n",
    "transaction_records_final = transaction_records_final.join(transaction_stats, on=\"merchant_abn\", how=\"left\")\n",
    "\n",
    "# Calculate how many standard deviations away each transaction is\n",
    "# May need extra caution to interpret this feature as it can be POSITIVE OR NEGATIVE\n",
    "transaction_records_final = transaction_records_final.withColumn(\n",
    "    \"std_diff_dollar_value\", \n",
    "    F.when(\n",
    "        F.col(\"stddev_dollar_value\").isNotNull() & (F.col(\"stddev_dollar_value\") != 0), \n",
    "        (F.col(\"dollar_value\") - F.col(\"avg_dollar_value\")) / F.col(\"stddev_dollar_value\")\n",
    "    ).otherwise(0) \n",
    ")\n",
    "\n",
    "transaction_records_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this feature is to take into account how much a transaction's dollar value deviates from what is usual for that merchant, in order to flag unusual transaction amounts.\n",
    "\n",
    "\"stddev_dollar_value\" -> Accounts for magnitude of difference of dollar value against the average, while also adjusted for each merchant's typical variability. \n",
    "This is important because for example, while a $100 difference may be normal for one merchant but very unusual for another merchant.\n",
    "\n",
    "Consideration: how it could be positive or negative values for this feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+-----------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+\n",
      "|merchant_abn|order_month|order_datetime|      dollar_value|            order_id|consumer_id|fraud_probability|                name|            category|revenue_level|take_rate| avg_dollar_value|stddev_dollar_value|std_diff_dollar_value|monthly_order_volume|avg_monthly_order_volume|stddev_monthly_order_volume|std_diff_order_volume|\n",
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+-----------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+\n",
      "| 31101120643|    2021-11|    2021-11-26|25.228114942417797|8e301c0f-06ab-45c...|     986886|             NULL|Commodo Hendrerit...|cable, satellite,...|            a|     6.37|78.37805888089956|  55.43483898784632|  -0.9587823273038548|                 760|       542.4761904761905|         153.54661150530774|   1.4166630405666116|\n",
      "| 79417999332|    2021-11|    2021-11-26|136.06570809815838|23acbb7b-cf98-458...|    1059280|             NULL|Phasellus At Company|gift, card, novel...|            b|     4.95| 91.9759216055508| 57.954340723860284|   0.7607676308956001|                6987|       4725.047619047619|          1378.425350760442|    1.640968355453213|\n",
      "| 49167531725|    2021-11|    2021-11-26| 51.58228625503599|7080c274-17f7-4cc...|    1195503|             NULL|     Felis Institute|digital goods: bo...|            a|     6.42|52.07643449204165|  40.88797494937904| -0.01208541723128...|                 232|      143.52380952380952|         47.191756745875686|   1.8748229897994386|\n",
      "| 67978471888|    2021-11|    2021-11-26| 691.5028234458998|0380e9ad-b0e8-420...|     179208|             NULL|Magna Malesuada C...|artist supply and...|            a|     5.56| 648.789383595735| 318.21309330658926|  0.13422904572003783|                 861|       591.2857142857143|         170.47907286735895|   1.5820961551341646|\n",
      "| 46451548968|    2021-11|    2021-11-26| 72.61581642788431|76bab304-fa2d-400...|    1195503|             NULL|Tempus Eu Ligula ...|health and beauty...|            a|     6.04|65.46491492502317|  65.13640918856957|  0.10978347735072903|                 968|       661.4285714285714|         191.51255087554216|   1.6007902728561088|\n",
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+-----------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flag unusual monthly transaction volumes that deviate from a merchant's usual monthly volume\n",
    "\n",
    "# Extract month and year from order_datetime\n",
    "transaction_records_final = transaction_records_final.withColumn(\"order_month\", F.date_format(F.col(\"order_datetime\"), \"yyyy-MM\"))\n",
    "\n",
    "# Calculate number of transactions per merchant per month\n",
    "transaction_records_monthly = transaction_records_final.groupBy(\"merchant_abn\", \"order_month\").agg(\n",
    "    F.count(\"order_id\").alias(\"monthly_order_volume\")\n",
    ")\n",
    "\n",
    "# Calculate the average standard deviation of monthly transactions per merchant\n",
    "transaction_stats = transaction_records_monthly.groupBy(\"merchant_abn\").agg(\n",
    "    F.avg(\"monthly_order_volume\").alias(\"avg_monthly_order_volume\"),\n",
    "    F.stddev(\"monthly_order_volume\").alias(\"stddev_monthly_order_volume\")\n",
    ")\n",
    "\n",
    "# Join the monthly volume feature back with the original dataset\n",
    "transaction_records_final = transaction_records_final.join(transaction_records_monthly, on=[\"merchant_abn\", \"order_month\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Join the transaction statistics back to the original dataset \n",
    "transaction_records_final = transaction_records_final.join(transaction_stats, on=\"merchant_abn\", how=\"left\")\n",
    "\n",
    "# Calculate how many standard deviations away each monthly volume is\n",
    "transaction_records_final = transaction_records_final.withColumn(\n",
    "    \"std_diff_order_volume\", \n",
    "    F.when(F.col(\"stddev_monthly_order_volume\").isNotNull() & (F.col(\"stddev_monthly_order_volume\") != 0),\n",
    "           (F.col(\"monthly_order_volume\") - F.col(\"avg_monthly_order_volume\")) / F.col(\"stddev_monthly_order_volume\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "transaction_records_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+\n",
      "|revenue_level|revenue_level_ordinal|\n",
      "+-------------+---------------------+\n",
      "|            b|                    2|\n",
      "|            a|                    1|\n",
      "|            c|                    3|\n",
      "|            a|                    1|\n",
      "|            a|                    1|\n",
      "+-------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the ordinal mapping for revenue levels\n",
    "revenue_mapping = {\n",
    "    'a': 1,\n",
    "    'b': 2,\n",
    "    'c': 3,\n",
    "    'd': 4,\n",
    "    'e': 5\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding to the revenue_level column in transaction_records_final\n",
    "transaction_records_final = transaction_records_final.withColumn(\n",
    "    \"revenue_level_ordinal\", \n",
    "    F.when(F.col(\"revenue_level\") == 'a', 1)\n",
    "     .when(F.col(\"revenue_level\") == 'b', 2)\n",
    "     .when(F.col(\"revenue_level\") == 'c', 3)\n",
    "     .when(F.col(\"revenue_level\") == 'd', 4)\n",
    "     .when(F.col(\"revenue_level\") == 'e', 5)\n",
    ")\n",
    "\n",
    "transaction_records_final.select(\"revenue_level\", \"revenue_level_ordinal\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Weekday Features, One hot encoding on months, weekdays and using ordinal encoding on revenue levels, adding binary feature for year 2021 or 2022 and for weekend.\n",
    "\n",
    "Adding Binary feature of whether it is a holiday or not\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------+--------------------+-------------+---------+------------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+---------------------+----------+-------+----------+\n",
      "|merchant_abn|order_month|order_datetime|      dollar_value|            order_id|consumer_id|fraud_probability|          name|            category|revenue_level|take_rate|  avg_dollar_value|stddev_dollar_value|std_diff_dollar_value|monthly_order_volume|avg_monthly_order_volume|stddev_monthly_order_volume|std_diff_order_volume|revenue_level_ordinal|order_date|weekday|is_weekend|\n",
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------+--------------------+-------------+---------+------------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+---------------------+----------+-------+----------+\n",
      "| 73256306726|    2022-09|    2022-09-30| 644.2874839146772|b8cf8d51-3270-488...|     738492|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|    1.561549940625708|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30|  826.831892512803|422d6e14-f7f4-40f...|    1203726|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|    2.353680269056528|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30|442.41743273051725|498e3a1d-58ac-483...|     423902|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|   0.6855582234710432|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30|403.13863775884255|9e4a966d-96a3-468...|    1467006|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|   0.5151124398931016|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30|486.46221774724745|5d2970c8-ada4-441...|    1288418|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|   0.8766854691035099|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30|260.74234429007095|b54f5a76-da16-469...|    1160912|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178| -0.10279979077440903|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30| 470.7345559020866|597c79d0-7370-47d...|    1291633|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|   0.8084371012760219|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30| 582.3621870648965|06c04b28-cbd3-431...|    1451395|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|   1.2928322838730244|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30|417.29618393246756|dbbe36cb-9dfa-4d5...|    1112051|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|   0.5765474725320756|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30| 565.4281808762443|08e56e89-eed3-4e1...|    1442380|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|   1.2193491244222323|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-30|235.07613832473461|88e95c26-dd59-4c5...|     953841|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178| -0.21417532025257496|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-30|      6|         0|\n",
      "| 73256306726|    2022-09|    2022-09-24|242.57487119198328|1d69ea44-5be1-4fb...|     210156|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178| -0.18163543708351224|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-24|      7|         1|\n",
      "| 73841664453|    2022-09|    2022-09-24|119.53450589342728|bee5587e-9b0c-483...|    1244057|             NULL|Lacinia At LLP|digital  goods: b...|            a|     5.55| 86.87621527713947|  53.70003761718162|    0.608161410409117|                  57|                   47.95|          11.53701137985689|   0.7844319210606739|                    1|2022-09-24|      7|         1|\n",
      "| 73256306726|    2022-09|    2022-09-24|29.223394426108367|62c39c92-699e-499...|     572255|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|  -1.1074494713759218|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-24|      7|         1|\n",
      "| 73841664453|    2022-09|    2022-09-24| 34.44744944042494|7ec9cb9a-6248-4ab...|      32839|             NULL|Lacinia At LLP|digital  goods: b...|            a|     5.55| 86.87621527713947|  53.70003761718162|  -0.9763264266306521|                  57|                   47.95|          11.53701137985689|   0.7844319210606739|                    1|2022-09-24|      7|         1|\n",
      "| 73841664453|    2022-09|    2022-09-24|102.76830873896284|b61b267d-2a4a-42d...|    1125601|             NULL|Lacinia At LLP|digital  goods: b...|            a|     5.55| 86.87621527713947|  53.70003761718162|  0.29594194281790626|                  57|                   47.95|          11.53701137985689|   0.7844319210606739|                    1|2022-09-24|      7|         1|\n",
      "| 73256306726|    2022-09|    2022-09-24|211.17253546367388|a34c04e2-1ab7-4e4...|     225023|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178| -0.31790223761161107|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-24|      7|         1|\n",
      "| 73256306726|    2022-09|    2022-09-24| 74.85570375960462|1de49be0-9f47-473...|    1292549|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|  -0.9094333477543174|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-24|      7|         1|\n",
      "| 73256306726|    2022-09|    2022-09-24| 97.60205789806356|7404a0f9-77fc-494...|     610279|             NULL|        Id LLP|health and beauty...|            b|     4.81|284.43229320049755| 230.44744285923178|  -0.8107281772554047|                 283|      250.61904761904762|          75.55956338576618|   0.4285486962865678|                    2|2022-09-24|      7|         1|\n",
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------+--------------------+-------------+---------+------------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+---------------------+----------+-------+----------+\n",
      "only showing top 19 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert order_datetime to a DateType column if it's not already\n",
    "transaction_records_final = transaction_records_final.withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "\n",
    "# Extract the weekday (1 = Sunday, 7 = Saturday)\n",
    "transaction_records_final = transaction_records_final.withColumn(\"weekday\", F.dayofweek(\"order_date\"))\n",
    "\n",
    "# Add a column to flag weekends (Saturday = 7, Sunday = 1)\n",
    "transaction_records_final = transaction_records_final.withColumn(\n",
    "    \"is_weekend\", \n",
    "    F.when((F.col(\"weekday\") == 7) | (F.col(\"weekday\") == 1), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "transaction_records_final.show(19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Index the weekday column first (it must be indexed before encoding)\n",
    "indexer = StringIndexer(inputCol=\"weekday\", outputCol=\"weekday_index\")\n",
    "transaction_records_final = indexer.fit(transaction_records_final).transform(transaction_records_final)\n",
    "\n",
    "# One-hot encode the indexed weekday\n",
    "encoder = OneHotEncoder(inputCol=\"weekday_index\", outputCol=\"weekday_vec\")\n",
    "transaction_records_final = encoder.fit(transaction_records_final).transform(transaction_records_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+------------+-----------+--------------+\n",
      "|order_month|year|month|is_year_2021|month_index|     month_vec|\n",
      "+-----------+----+-----+------------+-----------+--------------+\n",
      "|    2021-11|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "|    2021-11|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "|    2021-11|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "|    2021-11|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "|    2021-11|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "+-----------+----+-----+------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract year and month from 'order_month' and create new columns\n",
    "transaction_records_final = transaction_records_final.withColumn(\n",
    "    \"year\", F.split(F.col(\"order_month\"), \"-\")[0].cast(\"integer\")\n",
    ").withColumn(\n",
    "    \"month\", F.split(F.col(\"order_month\"), \"-\")[1].cast(\"integer\")\n",
    ")\n",
    "\n",
    "# Binary encoding for year\n",
    "transaction_records_final = transaction_records_final.withColumn(\n",
    "    \"is_year_2021\", F.when(F.col(\"year\") == 2021, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Index the month column\n",
    "month_indexer = StringIndexer(inputCol=\"month\", outputCol=\"month_index\")\n",
    "transaction_records_final = month_indexer.fit(transaction_records_final).transform(transaction_records_final)\n",
    "\n",
    "# One-hot encode the indexed month column\n",
    "month_encoder = OneHotEncoder(inputCols=[\"month_index\"], outputCols=[\"month_vec\"])\n",
    "transaction_records_final = month_encoder.fit(transaction_records_final).transform(transaction_records_final)\n",
    "\n",
    "# Show the results\n",
    "transaction_records_final.select(\"order_month\", \"year\", \"month\", \"is_year_2021\", \"month_index\", \"month_vec\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 04:15:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/09/17 04:15:55 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 172.29.253.151:34055 in 10000 milliseconds\n",
      "24/09/17 04:15:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1219)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:48)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+------------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+---------------------+----------+-------+----------+-------------+-------------+----+-----+------------+-----------+--------------+\n",
      "|merchant_abn|order_month|order_datetime|      dollar_value|            order_id|consumer_id|fraud_probability|                name|            category|revenue_level|take_rate|  avg_dollar_value|stddev_dollar_value|std_diff_dollar_value|monthly_order_volume|avg_monthly_order_volume|stddev_monthly_order_volume|std_diff_order_volume|revenue_level_ordinal|order_date|weekday|is_weekend|weekday_index|  weekday_vec|year|month|is_year_2021|month_index|     month_vec|\n",
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+------------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+---------------------+----------+-------+----------+-------------+-------------+----+-----+------------+-----------+--------------+\n",
      "| 83412691377|    2021-11|    2021-11-26|56.590290983073245|7509b9b7-48fb-4f8...|    1443334|             NULL|Suspendisse Sagit...|watch, clock, and...|            c|     2.94| 34.89197800233158|  24.26931234270316|   0.8940637738038552|                1000|       680.3809523809524|         195.99017225118106|   1.6307911970679005|                    3|2021-11-26|      6|         0|          0.0|(6,[0],[1.0])|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "| 38700038932|    2021-11|    2021-11-26|  4449.30910811421|776df727-49c6-427...|    1395241|             NULL|Etiam Bibendum In...|tent and awning s...|            a|     6.31|1338.5004712138689|  774.3127760170715|    4.017509116796178|                 484|       339.6190476190476|          98.24076353045929|   1.4696643958410143|                    1|2021-11-26|      6|         0|          0.0|(6,[0],[1.0])|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "| 35344855546|    2021-11|    2021-11-26| 37.25988746024652|38f599ea-03e0-425...|     512071|             NULL|Quis Tristique Ac...|watch, clock, and...|            c|     2.92| 88.52644577048906|  67.44520882069787|  -0.7601215743364953|                 103|       72.47619047619048|         21.699352634627253|   1.4066691314606514|                    3|2021-11-26|      6|         0|          0.0|(6,[0],[1.0])|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "| 83412691377|    2021-11|    2021-11-26|34.932453882390476|7cf7e22c-9697-466...|      10967|             NULL|Suspendisse Sagit...|watch, clock, and...|            c|     2.94| 34.89197800233158|  24.26931234270316| 0.001667780260410587|                1000|       680.3809523809524|         195.99017225118106|   1.6307911970679005|                    3|2021-11-26|      6|         0|          0.0|(6,[0],[1.0])|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "| 15613631617|    2021-11|    2021-11-26| 261.4190350038583|978f56be-48b0-46d...|     777839|             NULL|     Ante Industries|motor vehicle sup...|            e|     0.35| 304.2187850604068|  197.1322343080884| -0.21711188029076373|                 120|                    85.0|          23.89979079406345|   1.4644479653225153|                    5|2021-11-26|      6|         0|          0.0|(6,[0],[1.0])|2021|   11|           1|        8.0|(11,[8],[1.0])|\n",
      "+------------+-----------+--------------+------------------+--------------------+-----------+-----------------+--------------------+--------------------+-------------+---------+------------------+-------------------+---------------------+--------------------+------------------------+---------------------------+---------------------+---------------------+----------+-------+----------+-------------+-------------+----+-----+------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_records_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.age 160:>               (0 + 0) / 10]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o555.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Standardize the numeric features\u001b[39;00m\n\u001b[1;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric_features_vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_numeric_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, withMean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, withStd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m scaler_model \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_with_numeric_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m transaction_records_scaled \u001b[38;5;241m=\u001b[39m scaler_model\u001b[38;5;241m.\u001b[39mtransform(data_with_numeric_vec)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o555.fit"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "# Assemble the features to scale\n",
    "numeric_features = [\"dollar_value\", \"monthly_order_volume\", \"take_rate\"]\n",
    "assembler = VectorAssembler(inputCols=numeric_features, outputCol=\"numeric_features_vec\")\n",
    "\n",
    "# Apply the assembler to combine the numeric features into a vector\n",
    "data_with_numeric_vec = assembler.transform(transaction_records_final)\n",
    "\n",
    "# Standardize the numeric features\n",
    "scaler = StandardScaler(inputCol=\"numeric_features_vec\", outputCol=\"scaled_numeric_features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(data_with_numeric_vec)\n",
    "transaction_records_scaled = scaler_model.transform(data_with_numeric_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transaction_records_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m relevant_columns \u001b[38;5;241m=\u001b[39m features \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Filter the dataset to keep only the relevant columns and remove rows with NULL in fraud_probability\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m transaction_records_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mtransaction_records_scaled\u001b[49m\u001b[38;5;241m.\u001b[39mselect(relevant_columns)\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraud_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull())\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# VectorAssembler to combine the features into a single vector\u001b[39;00m\n\u001b[1;32m     26\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39mfeatures, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transaction_records_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List of features to be used in the model\n",
    "features = [\n",
    "    \"scaled_numeric_features\",\n",
    "    \"monthly_order_volume\", \n",
    "    \"std_diff_order_volume\", \n",
    "    \"revenue_level_vec\",\n",
    "    \"revenue_level_ordinal\",\n",
    "    \"is_weekend\",\n",
    "    \"weekday_vec\",\n",
    "    \"is_year_2021\",\n",
    "    \"month_vec\"\n",
    "\n",
    "\n",
    "    \n",
    "]\n",
    "\n",
    "# Select only the necessary columns: the features and the target column\n",
    "relevant_columns = features + [\"fraud_probability\"]\n",
    "\n",
    "# Filter the dataset to keep only the relevant columns and remove rows with NULL in fraud_probability\n",
    "transaction_records_filtered = transaction_records_scaled.select(relevant_columns).filter(F.col(\"fraud_probability\").isNotNull())\n",
    "\n",
    "# VectorAssembler to combine the features into a single vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Prepare the data\n",
    "data = assembler.transform(transaction_records_filtered)\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Preview\n",
    "train_data.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, LinearRegression\n",
    "\n",
    "# Define model regressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"fraud_probability\", featuresCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"fraud_probability\", featuresCol=\"features\")\n",
    "\n",
    "lr = LinearRegression(labelCol=\"fraud_probability\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Parameter grid\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 7]) \\\n",
    "    .build()\n",
    "\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluator for regression models\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"fraud_probability\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"  \n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"fraud_probability\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\" \n",
    ")\n",
    "\n",
    "# Cross-validation \n",
    "dt_cv = CrossValidator(\n",
    "    estimator=dt,\n",
    "    estimatorParamMaps=dt_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=lr_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline \n",
    "dt_pipeline = Pipeline(stages=[dt_cv])\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[rf_cv])\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[lr_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.age 701:>               (0 + 0) / 10]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/shipingkfc/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o446.evaluate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# _ mins\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dt_model \u001b[38;5;241m=\u001b[39m \u001b[43mdt_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# _ mins\u001b[39;00m\n\u001b[1;32m      7\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m rf_pipeline\u001b[38;5;241m.\u001b[39mfit(train_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:118\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43meva\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, metric, model \u001b[38;5;28;01mif\u001b[39;00m collectSubModel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o446.evaluate"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "# _ mins\n",
    "dt_model = dt_pipeline.fit(train_data)\n",
    "\n",
    "# _ mins\n",
    "rf_model = rf_pipeline.fit(train_data)\n",
    "\n",
    "# _ mins\n",
    "lr_model = lr_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation/Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "\n",
    "dt_rmse = rmse_evaluator.evaluate(dt_predictions)\n",
    "dt_r2 = r2_evaluator.evaluate(dt_predictions)\n",
    "print(f\"Decision Tree RMSE: {dt_rmse}\")\n",
    "print(f\"Decision Tree R2: {dt_r2}\")          \n",
    "\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "print(f\"Random Forest R2: {rf_r2}\")           \n",
    "\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
    "print(f\"Linear Regression R2: {lr_r2}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt_model = dt_model.stages[-1].bestModel\n",
    "print(f\"Best Decision Tree maxDepth: {best_dt_model._java_obj.getMaxDepth()}\")\n",
    "print(f\"Best Decision Tree maxBins: {best_dt_model._java_obj.getMaxBins()}\")\n",
    "\n",
    "\n",
    "best_rf_model = rf_model.stages[-1].bestModel\n",
    "print(f\"Best Random Forest numTrees: {best_rf_model.getNumTrees}\")\n",
    "print(f\"Best Random Forest maxDepth: {best_rf_model.getMaxDepth()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the feature names from the VectorAssembler\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "best_rf_model = rf_model.stages[0].bestModel\n",
    "best_dt_model = dt_model.stages[0].bestModel\n",
    "\n",
    "dt_feature_importances = best_dt_model.featureImportances\n",
    "rf_feature_importances = best_rf_model.featureImportances\n",
    "\n",
    "rf_importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": rf_feature_importances.toArray()\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "dt_importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": dt_feature_importances.toArray()\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(rf_importances_df)\n",
    "print()\n",
    "print(dt_importances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients \n",
    "best_lr_model = lr_model.stages[0].bestModel\n",
    "coefficients = best_lr_model.coefficients\n",
    "\n",
    "# Get feature names from the VectorAssembler\n",
    "lr_importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": coefficients\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(lr_importances_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
