{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.ranking_model_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Ranking Model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading transactions\n",
    "transactions = spark.read.parquet(f\"../data/curated/transactions.parquet\")\n",
    "transactions = transactions.withColumns(\n",
    "    {\"period\": F.date_format(F.col(\"order_datetime\"), \"yyyy-MM\")})\n",
    "transactions = transactions.drop(\"merchant_fp\", \"consumer_fp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading predicted fraud probabilities\n",
    "merchant_fp = spark.read.parquet(f\"../data/curated/predicted_merchant_fp.parquet\")\n",
    "consumer_fp = spark.read.parquet(f\"../data/curated/predicted_consumer_fp.parquet\")\n",
    "\n",
    "# Join with transaction data\n",
    "transactions_full = transactions.join(consumer_fp, on = ['consumer_id', 'order_datetime', 'order_id'], how = 'inner')\n",
    "transactions_full = transactions_full.join(merchant_fp, on = ['merchant_abn', 'order_datetime', 'order_id'], how = 'inner')\n",
    "# transactions_full.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are many ways in ranking the top 100 merchants toi join the BNPL firm, we decided adopted the approach of an investor, consider each merchant as a \"projects\" and evaluating their value. To evluate the merchant's value, we will be using a modified version of **Discounted Cash Flow (DCF)** model where we substitute *cash flow* with *revenue*. The orignal DCF model has the below formula\n",
    "\n",
    "$$ \\text{DCF} = \\sum^{n}_{t=1}\\frac{CF_t}{(1+r)^t}$$\n",
    "\n",
    "where $t$ is the time period of cash flow and $r$ is the discount rate.\n",
    "\n",
    "The DCF model we based on is the one that use **Free Cash Flows (FCF)** from **Earning Before Interest and Tax (EBIT)**. EBIT are usually a percentage of sales revenue and in Corporate Financial Decision Making (FNCE20003), the formula for FCF is defined as\n",
    "\n",
    "$$ \\text{FCF} = \\text{EBIT}(1-t) + \\text{Depreciation} - \\text{Capital Expenditure} - \\Delta\\text{Working Capital}$$\n",
    "\n",
    "Since the BNPL charges merchant per transaction, this means that the firm is charging for a percentage of the sales revenue. Thus, the merchant doesn't pay the BNPL firm through EBIT or their FCF. This allows us to safely consider the percentage of revenue of the merchant as cash flows for the BNPL firm.\n",
    "\n",
    "We will calculate of the project's  DCF using revenues from September 2022, October 2022, and November 2022. The value of the DCF is then multipled by the take rate, which we will called **Expected Project Value (EPV)**. After that, we will assign weights or penalties to the DCF and pick the merchants with the highest DCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data into desried format\n",
    "agg_transactions = transactions_full.groupBy(\"merchant_abn\", \"period\", \"take_rate\").agg(\n",
    "    F.count(F.col(\"order_id\")).alias(\"num_orders\"),\n",
    "    F.round(F.sum(\"dollar_value\"),2).alias('revenue'),\n",
    "    F.round(F.mean(F.col(\"dollar_value\")), 2).alias(\"revenue_per_order\"),\n",
    "    F.round(F.mean(F.col(\"merchant_fp\")), 2).alias(\"avg_merchant_fp\"),\n",
    "    F.round(F.mean(F.col(\"consumer_fp\")), 2).alias(\"avg_consumer_fp\")\n",
    ")\n",
    "\n",
    "agg_transactions = agg_transactions.orderBy([\"merchant_abn\", \"period\"], ascending = [False, True])\n",
    "\n",
    "# Partition the data based on merchant ABN and comput lag variables for each specific partition\n",
    "merchant_partition = Window.partitionBy(\"merchant_abn\").orderBy(\"period\")\n",
    "agg_transactions = agg_transactions.withColumns({\n",
    "    \"revenue_lag_1\": F.lag(\"revenue\", 1, None).over(merchant_partition),\n",
    "    \"revenue_lag_2\": F.lag(\"revenue\", 2, None).over(merchant_partition),\n",
    "    \"revenue_lag_3\": F.lag(\"revenue\", 3, None).over(merchant_partition),\n",
    "    \"num_order_lag_1\": F.lag(\"num_orders\", 1, None).over(merchant_partition),\n",
    "    \"num_order_lag_2\": F.lag(\"num_orders\", 2, None).over(merchant_partition),\n",
    "    \"expected_profit\": F.round(F.col(\"revenue\") * F.col(\"take_rate\")/100,2)\n",
    "    })\n",
    "agg_transactions = agg_transactions.withColumns({\n",
    "    \"revenue_growth\":\n",
    "        F.when(F.col(\"revenue_lag_1\").isNotNull(), F.round((F.col(\"revenue\") - F.col(\"revenue_lag_1\"))/F.col(\"revenue_lag_1\"), 2)).otherwise(None),\n",
    "    \"revenue_growth_lag_1\": F.when(F.col(\"revenue_lag_2\").isNotNull(), \n",
    "        F.round((F.col(\"revenue_lag_1\") - F.col(\"revenue_lag_2\"))/F.col(\"revenue_lag_2\"), 2)).otherwise(None),\n",
    "    \"revenue_growth_lag_2\": F.when(F.col(\"revenue_lag_3\").isNotNull(), \n",
    "        F.round((F.col(\"revenue_lag_2\") - F.col(\"revenue_lag_3\"))/F.col(\"revenue_lag_3\"), 2)).otherwise(None)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing different merchants' values using the DCF model is only reliable when all merchants have revenue in the timeline of interest i.e from May 2021 to August 2022, all merchants must have sales in each month. We noticed that there are some merchants with no revenue in some particular month. Thus, we decide to asume that the missing revenue in some moths of some merchants are totally due to the merchant's inability to make any sales and not because of data entry errors. We adopted the perspective that we want merchants that have consistent sales. We will only consider merchants with 15 months of revenue records, prior to the date of the last transaction entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 415:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merchants with complete sales records: 3212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a dataframe that store the valid date range of the data\n",
    "months = pd.date_range(start=\"2021-06-01\", end=\"2022-08-31\", freq='MS').strftime('%Y-%m').tolist()\n",
    "months_df = spark.createDataFrame([(month,) for month in months], [\"period\"])\n",
    "\n",
    "# Join with the transactions dataframe\n",
    "transactions_correct_months = transactions.join(months_df, on= ['period'], how = 'right')\n",
    "\n",
    "# Get the list of merchants with complete sales records\n",
    "complete_merchants = transactions_correct_months.groupBy(\"merchant_abn\").agg(F.countDistinct(\"period\")\\\n",
    "                                                                             .alias(\"month_count\")).filter(F.col(\"month_count\") == len(months)).select(\"merchant_abn\")\n",
    "print(f\"Number of merchants with complete sales records: {complete_merchants.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the merchants with complete sales records from the aggregated sales \n",
    "# print(f\"Number of entries before removing merchants with missing sales records: {agg_transactions.count()}\")\n",
    "# agg_transactions = agg_transactions.join(complete_merchants, on='merchant_abn', how='inner')\n",
    "# agg_transactions = agg_transactions.filter(\"2021-06\" <= F.col(\"period\")).orderBy(['merchant_abn', 'period'],\n",
    "#                                                                                       ascending = [True, True])\n",
    "# print(f\"Number of entries after removing merchants with missing sales records: {agg_transactions.count()}\")\n",
    "\n",
    "# # Export the aggregated transactions to reduce the need to run this block again\n",
    "# agg_transactions.write.parquet(f\"../data/curated/agg_transactions.parquet\", mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_transactions = spark.read.parquet(f\"../data/curated/agg_transactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_transactions_sub = agg_transactions.select(*['merchant_abn', 'period', 'revenue', 'revenue_lag_1', 'revenue_lag_2', 'revenue_lag_3',\n",
    "'revenue_growth_lag_1', 'revenue_growth_lag_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since the last month of sales record is August 2022, this means that we have to forecast the revenues of the next 3 months. We will try 2 different approaches for forecasting revenue of 3 periods into the future. The first approach is to use a simple LSTM to predict the revenue. Contrast to the machine learning nature of approach 1, the second approach will be simply to compute the average monthly revenue growth rate of the 15 months period and then assume that revenues after August 2022 will grow by the same amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for merchant in complete_merchants.rdd.flatMap(lambda x:x).collect():\n",
    "#     # Predict sales revenue 3 period for all valid merchants\n",
    "#     partition = agg_transactions_sub.filter(F.col(\"merchant_abn\") == merchant)\n",
    "    \n",
    "#     forecasted_revenue = forecast_revenue(partition)\n",
    "\n",
    "#     print(forecasted_revenue)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the above code multiple times, we will see that the forecasted revenues will change with each rerun. This is because the weights between the units are randomly intialise each time and Stochastic Gradient Descent isn't guaranteed to always find the local maximum and minimum. Thus, in order to train a neural network that minimise the mean squared error, we would have to fine-tune our model. The process would be feasible if we're doing for 8-10 merchants. However, our data contains more than 3000 merchants which would be computationally expensive. Thus, we will stick with the second approach that was mentioned previously.\n",
    "\n",
    "Let's aggregate our data and find the monthly average growth for each merchant. From there, we can compute the DCF for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>period</th><th>take_rate</th><th>num_orders</th><th>revenue</th><th>revenue_per_order</th><th>avg_merchant_fp</th><th>avg_consumer_fp</th><th>revenue_lag_1</th><th>revenue_lag_2</th><th>revenue_lag_3</th><th>num_order_lag_1</th><th>num_order_lag_2</th><th>expected_profit</th><th>revenue_growth</th><th>revenue_growth_lag_1</th><th>revenue_growth_lag_2</th></tr>\n",
       "<tr><td>63937753588</td><td>2021-10</td><td>4.17</td><td>14</td><td>42480.29</td><td>3034.31</td><td>41.25</td><td>16.06</td><td>20691.97</td><td>41997.37</td><td>36180.46</td><td>8</td><td>12</td><td>1771.43</td><td>1.05</td><td>-0.51</td><td>0.16</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+-------+---------+----------+--------+-----------------+---------------+---------------+-------------+-------------+-------------+---------------+---------------+---------------+--------------+--------------------+--------------------+\n",
       "|merchant_abn| period|take_rate|num_orders| revenue|revenue_per_order|avg_merchant_fp|avg_consumer_fp|revenue_lag_1|revenue_lag_2|revenue_lag_3|num_order_lag_1|num_order_lag_2|expected_profit|revenue_growth|revenue_growth_lag_1|revenue_growth_lag_2|\n",
       "+------------+-------+---------+----------+--------+-----------------+---------------+---------------+-------------+-------------+-------------+---------------+---------------+---------------+--------------+--------------------+--------------------+\n",
       "| 63937753588|2021-10|     4.17|        14|42480.29|          3034.31|          41.25|          16.06|     20691.97|     41997.37|     36180.46|              8|             12|        1771.43|          1.05|               -0.51|                0.16|\n",
       "+------------+-------+---------+----------+--------+-----------------+---------------+---------------+-------------+-------------+-------------+---------------+---------------+---------------+--------------+--------------------+--------------------+"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_transactions.limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_avg_growth = agg_transactions.groupBy(\"merchant_abn\", ).agg(\n",
    "    F.mean(F.col('revenue_growth')).alias('avg_monthly_revenue_growth'),\n",
    "    F.mean(F.col(\"num_orders\")).alias(\"avg_num_orders\"),\n",
    "    F.mean(F.col(\"revenue_per_order\")).alias('avg_revenue_per_order'),\n",
    "    (F.stddev(F.col('revenue'))/F.mean(F.col('revenue'))).alias(\"coef_of_variation\"),\n",
    "    F.stddev(F.col('revenue_growth')).alias(\"std_reveune_growth\")\n",
    ")\n",
    "merchant_latest_revenue = agg_transactions.filter(F.col(\"period\") == '2022-08').select(\"merchant_abn\", \"revenue\", \"take_rate\")\n",
    "merchant_latest_revenue = merchant_latest_revenue.join(merchant_avg_growth, on ='merchant_abn', how = 'inner')\n",
    "merchant_latest_revenue = merchant_latest_revenue.withColumns({\n",
    "    \"forecasted_revenue_1\": F.col(\"revenue\") * (1 + F.col(\"avg_monthly_revenue_growth\")),\n",
    "    \"forecasted_revenue_2\": F.col(\"revenue\") * (1 + F.col(\"avg_monthly_revenue_growth\"))**2,\n",
    "    \"forecasted_revenue_3\": F.col(\"revenue\") * (1 + F.col(\"avg_monthly_revenue_growth\"))**3,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Victoria State Government [website](https://djsir.vic.gov.au/about-us/overview/the-economic-assessment-information-portal/i-am-looking-for-guidance-on-particular-economic-assessment-processes,-methods-and-variables#:~:text=Department%20of%20Treasury%20and%20Finance,on%20the%20category%20of%20investment.), there is no single discount rate. As a general guideline, the discount rate is between 4% and 7%. We acknowledge that each merchant may have their own discount rates but it's quite extensive to give each and everyone of them an individual discount rate. Thus, we will use the mid point of the recommended range and apply it to all merchants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>avg_monthly_revenue_growth</th><th>avg_num_orders</th><th>avg_revenue_per_order</th><th>coef_of_variation</th><th>std_reveune_growth</th><th>discounted_revenue_flow</th><th>expected_project_value</th></tr>\n",
       "<tr><td>73256306726</td><td>0.02066666666666667</td><td>269.1333333333333</td><td>284.46199999999993</td><td>0.20053707832195655</td><td>0.23998412645919096</td><td>191447.79571678603</td><td>9208.638973977408</td></tr>\n",
       "<tr><td>73841664453</td><td>0.06533333333333333</td><td>47.13333333333333</td><td>86.75933333333334</td><td>0.2788256236984031</td><td>0.26699027983020196</td><td>15736.800264718144</td><td>873.392414691857</td></tr>\n",
       "<tr><td>78916025936</td><td>3.8819999999999992</td><td>3.8</td><td>321.5033333333332</td><td>0.7532806819543889</td><td>12.823639554688496</td><td>291776.5269985978</td><td>1079.5731498948119</td></tr>\n",
       "<tr><td>83412691377</td><td>0.025333333333333333</td><td>727.1333333333333</td><td>34.932</td><td>0.16312759228803875</td><td>0.17307581603227556</td><td>77261.59325069639</td><td>2271.490841570474</td></tr>\n",
       "<tr><td>92202115241</td><td>0.09266666666666667</td><td>5.8</td><td>333.6026666666666</td><td>0.41587657752670437</td><td>0.6025833275711059</td><td>4417.673386814076</td><td>250.04031369367672</td></tr>\n",
       "<tr><td>96946925998</td><td>0.34</td><td>6.0</td><td>924.2446666666666</td><td>0.6041988901065665</td><td>1.3132565846561963</td><td>27450.551701676337</td><td>1644.2880469304125</td></tr>\n",
       "<tr><td>64185141673</td><td>1.1159999999999999</td><td>5.2</td><td>290.19066666666663</td><td>0.6354321269447655</td><td>2.478818266836034</td><td>15163.199259590589</td><td>874.9165972783769</td></tr>\n",
       "<tr><td>66610548417</td><td>0.092</td><td>10.8</td><td>936.6219999999998</td><td>0.2972128564236676</td><td>0.41068235900754246</td><td>39351.835035257325</td><td>2585.4155618164064</td></tr>\n",
       "<tr><td>71002398501</td><td>0.33999999999999997</td><td>6.466666666666667</td><td>326.126</td><td>0.4544716307485942</td><td>1.372630425757161</td><td>7819.599611378878</td><td>295.58086531012157</td></tr>\n",
       "<tr><td>72762528640</td><td>0.25133333333333335</td><td>3.6666666666666665</td><td>273.3106666666667</td><td>0.5389553454598275</td><td>0.9667387493650027</td><td>1296.750849223466</td><td>29.176894107527986</td></tr>\n",
       "<tr><td>87211363921</td><td>0.06000000000000001</td><td>35.13333333333333</td><td>55.967333333333336</td><td>0.3353069504218318</td><td>0.375347458078131</td><td>4872.837716257444</td><td>32.64801269892487</td></tr>\n",
       "<tr><td>68874243493</td><td>0.24133333333333332</td><td>14.266666666666667</td><td>3198.2326666666672</td><td>0.45992313698924436</td><td>0.9829682938257299</td><td>99133.61879431811</td><td>2498.1671936168163</td></tr>\n",
       "<tr><td>80421506936</td><td>0.04733333333333334</td><td>26.666666666666668</td><td>84.776</td><td>0.18529927722569514</td><td>0.215720277431766</td><td>7486.991294832171</td><td>381.8365560364407</td></tr>\n",
       "<tr><td>81583941068</td><td>0.022666666666666665</td><td>191.13333333333333</td><td>298.1940000000001</td><td>0.1870711541709492</td><td>0.20457156158548484</td><td>155520.77665641758</td><td>4727.831610355094</td></tr>\n",
       "<tr><td>86662713230</td><td>0.02533333333333333</td><td>1094.2666666666667</td><td>52.40200000000001</td><td>0.1853516272668784</td><td>0.1852745710200584</td><td>166307.1756759368</td><td>10660.289960827547</td></tr>\n",
       "<tr><td>90568944804</td><td>0.03666666666666667</td><td>541.6</td><td>901.3306666666667</td><td>0.18814575097036243</td><td>0.22843150479071922</td><td>1530233.3272726843</td><td>62739.56641818005</td></tr>\n",
       "<tr><td>98545158925</td><td>0.041999999999999996</td><td>257.73333333333335</td><td>40.972</td><td>0.18931001682279155</td><td>0.21294533168330854</td><td>28202.551190090388</td><td>600.7143403489252</td></tr>\n",
       "<tr><td>63937753588</td><td>0.16133333333333333</td><td>12.0</td><td>2977.2613333333334</td><td>0.3044827841360601</td><td>0.5310618294197641</td><td>135898.61419222382</td><td>5666.972211815733</td></tr>\n",
       "<tr><td>71305424518</td><td>0.05133333333333332</td><td>22.666666666666668</td><td>51.638000000000005</td><td>0.3090333290976863</td><td>0.35411593474185155</td><td>3016.85222147923</td><td>76.0246759812766</td></tr>\n",
       "<tr><td>71649111610</td><td>0.7346666666666664</td><td>8.266666666666667</td><td>289.3773333333333</td><td>0.6942064834299622</td><td>2.3842425172268826</td><td>28174.490597478878</td><td>1214.3205447513394</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+\n",
       "|merchant_abn|avg_monthly_revenue_growth|    avg_num_orders|avg_revenue_per_order|  coef_of_variation| std_reveune_growth|discounted_revenue_flow|expected_project_value|\n",
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+\n",
       "| 73256306726|       0.02066666666666667| 269.1333333333333|   284.46199999999993|0.20053707832195655|0.23998412645919096|     191447.79571678603|     9208.638973977408|\n",
       "| 73841664453|       0.06533333333333333| 47.13333333333333|    86.75933333333334| 0.2788256236984031|0.26699027983020196|     15736.800264718144|      873.392414691857|\n",
       "| 78916025936|        3.8819999999999992|               3.8|    321.5033333333332| 0.7532806819543889| 12.823639554688496|      291776.5269985978|    1079.5731498948119|\n",
       "| 83412691377|      0.025333333333333333| 727.1333333333333|               34.932|0.16312759228803875|0.17307581603227556|      77261.59325069639|     2271.490841570474|\n",
       "| 92202115241|       0.09266666666666667|               5.8|    333.6026666666666|0.41587657752670437| 0.6025833275711059|      4417.673386814076|    250.04031369367672|\n",
       "| 96946925998|                      0.34|               6.0|    924.2446666666666| 0.6041988901065665| 1.3132565846561963|     27450.551701676337|    1644.2880469304125|\n",
       "| 64185141673|        1.1159999999999999|               5.2|   290.19066666666663| 0.6354321269447655|  2.478818266836034|     15163.199259590589|     874.9165972783769|\n",
       "| 66610548417|                     0.092|              10.8|    936.6219999999998| 0.2972128564236676|0.41068235900754246|     39351.835035257325|    2585.4155618164064|\n",
       "| 71002398501|       0.33999999999999997| 6.466666666666667|              326.126| 0.4544716307485942|  1.372630425757161|      7819.599611378878|    295.58086531012157|\n",
       "| 72762528640|       0.25133333333333335|3.6666666666666665|    273.3106666666667| 0.5389553454598275| 0.9667387493650027|      1296.750849223466|    29.176894107527986|\n",
       "| 87211363921|       0.06000000000000001| 35.13333333333333|   55.967333333333336| 0.3353069504218318|  0.375347458078131|      4872.837716257444|     32.64801269892487|\n",
       "| 68874243493|       0.24133333333333332|14.266666666666667|   3198.2326666666672|0.45992313698924436| 0.9829682938257299|      99133.61879431811|    2498.1671936168163|\n",
       "| 80421506936|       0.04733333333333334|26.666666666666668|               84.776|0.18529927722569514|  0.215720277431766|      7486.991294832171|     381.8365560364407|\n",
       "| 81583941068|      0.022666666666666665|191.13333333333333|    298.1940000000001| 0.1870711541709492|0.20457156158548484|     155520.77665641758|     4727.831610355094|\n",
       "| 86662713230|       0.02533333333333333|1094.2666666666667|    52.40200000000001| 0.1853516272668784| 0.1852745710200584|      166307.1756759368|    10660.289960827547|\n",
       "| 90568944804|       0.03666666666666667|             541.6|    901.3306666666667|0.18814575097036243|0.22843150479071922|     1530233.3272726843|     62739.56641818005|\n",
       "| 98545158925|      0.041999999999999996|257.73333333333335|               40.972|0.18931001682279155|0.21294533168330854|     28202.551190090388|     600.7143403489252|\n",
       "| 63937753588|       0.16133333333333333|              12.0|   2977.2613333333334| 0.3044827841360601| 0.5310618294197641|     135898.61419222382|     5666.972211815733|\n",
       "| 71305424518|       0.05133333333333332|22.666666666666668|   51.638000000000005| 0.3090333290976863|0.35411593474185155|       3016.85222147923|      76.0246759812766|\n",
       "| 71649111610|        0.7346666666666664| 8.266666666666667|    289.3773333333333| 0.6942064834299622| 2.3842425172268826|     28174.490597478878|    1214.3205447513394|\n",
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rate = 1.055\n",
    "\n",
    "merchant_latest_revenue = merchant_latest_revenue.withColumns(\n",
    "    {\"discounted_revenue_flow\": F.col(\"forecasted_revenue_1\")/discount_rate + F.col(\"forecasted_revenue_2\")/discount_rate**2 + F.col(\"forecasted_revenue_3\")/discount_rate**3,\n",
    "     \"expected_project_value\": F.col(\"discounted_revenue_flow\") * F.col('take_rate')/100}\n",
    "    )\n",
    "merchant_latest_revenue = merchant_latest_revenue.drop(\"forecasted_revenue_1\", \"forecasted_revenue_2\", \"forecasted_revenue_3\", \"revenue\", \"take_rate\")\n",
    "merchant_latest_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be using the predicted fraud probability for both merchants and consumer as part of our ranking system. We will assign our weights of choice, $\\alpha$ and $\\beta$ to the merchants and consumers' fraud probability, respectively, and sum them. We think that the merchant's fraud probability is more important to the BNPL firm as a merchant with a higher fraud probability with likely to commit scams, thus damaging the BNPL firm's reputation and customer will not shop from the firm anymore. Whereas for customer, it's easier to assess the risk of a customer not paying for the items. Thus, we decided the weight is going to be $\\alpha = 0.65$ and $\\beta = 0.35$. The formula for the combined fraud probability is:\n",
    "\n",
    "$$\\text{Combined Fraud Probability (CBF)} = \\alpha \\times \\text{Merchant's FP} + \\beta\\times\\text{Consumer's FP}$$\n",
    "\n",
    "We will use the combined fraud probability to get the fraud-adjusted DCF\n",
    "\n",
    "$$ \\text{Fraud-adjusted EPV} = (1 - \\text{CBF}) \\times \\text{EPV}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fp = agg_transactions.withColumns({\n",
    "    \"total_merchant_fp\": F.col(\"avg_merchant_fp\") * F.col(\"num_orders\"),\n",
    "    \"total_consumer_fp\": F.col(\"avg_consumer_fp\") * F.col(\"num_orders\"),\n",
    "}).select(\"merchant_abn\", \"period\", \"num_orders\", \"total_merchant_fp\", \"total_consumer_fp\")\n",
    "\n",
    "avg_fp = avg_fp.groupBy(\"merchant_abn\").agg(\n",
    "    (F.sum(F.col(\"total_merchant_fp\"))/F.sum(F.col(\"num_orders\"))).alias(\"avg_merchant_fp\"),\n",
    "    (F.sum(F.col(\"total_consumer_fp\"))/F.sum(F.col(\"num_orders\"))).alias(\"avg_consumer_fp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.65\n",
    "beta = 0.35\n",
    "\n",
    "avg_fp = avg_fp.withColumn(\"combined_fp\", alpha * F.col(\"avg_merchant_fp\") + beta * F.col('avg_consumer_fp'))\n",
    "merchant_ranking_metrics = merchant_latest_revenue.join(avg_fp, on='merchant_abn', how='inner')\n",
    "merchant_ranking_metrics = merchant_ranking_metrics.drop(\"avg_merchant_fp\",\"avg_consumer_fp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_ranking_metrics = merchant_ranking_metrics.withColumn(\n",
    "    \"risk_adjusted_epv\",\n",
    "    F.col(\"expected_project_value\") * (1 - F.col(\"combined_fp\")/100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>avg_monthly_revenue_growth</th><th>avg_num_orders</th><th>avg_revenue_per_order</th><th>coef_of_variation</th><th>std_reveune_growth</th><th>discounted_revenue_flow</th><th>expected_project_value</th><th>combined_fp</th><th>risk_adjusted_epv</th></tr>\n",
       "<tr><td>70344541271</td><td>51.08466666666667</td><td>4.866666666666666</td><td>1081.5886666666665</td><td>0.5737462225685777</td><td>195.37035923989032</td><td>7.229664594670486E8</td><td>4.128138483556847E7</td><td>42.42405479452055</td><td>2.3768147512990005E7</td></tr>\n",
       "<tr><td>71616292306</td><td>27.928666666666665</td><td>2.6666666666666665</td><td>210.442</td><td>0.730322688378949</td><td>106.90505098762485</td><td>5417787.616379169</td><td>177161.6550555988</td><td>39.5622125</td><td>107072.58461398579</td></tr>\n",
       "<tr><td>95276443363</td><td>15.658666666666665</td><td>3.6</td><td>296.6186666666666</td><td>0.6820969976318357</td><td>51.655349033856346</td><td>6601250.027970397</td><td>251507.6260656721</td><td>39.01125925925927</td><td>153391.33400438444</td></tr>\n",
       "<tr><td>75547072158</td><td>12.812666666666667</td><td>3.8666666666666667</td><td>284.17533333333336</td><td>0.7476836101265073</td><td>48.9629811281174</td><td>253224.4641329908</td><td>11445.745778811182</td><td>39.041500000000006</td><td>6977.154940576614</td></tr>\n",
       "<tr><td>12171241826</td><td>10.885333333333334</td><td>8.2</td><td>228.37266666666665</td><td>0.568344026863831</td><td>40.80740511697543</td><td>5073911.329712124</td><td>209552.5379171107</td><td>38.0224756097561</td><td>129875.47529795238</td></tr>\n",
       "<tr><td>33223110337</td><td>10.853333333333333</td><td>2.8</td><td>214.40200000000002</td><td>0.9092808051054941</td><td>38.67401336199554</td><td>463179.7602395222</td><td>21908.402659329404</td><td>38.604011904761904</td><td>13450.880288578708</td></tr>\n",
       "<tr><td>70783350473</td><td>10.031333333333333</td><td>3.066666666666667</td><td>576.6173333333334</td><td>0.5781118668358438</td><td>36.41120272401783</td><td>2314209.098268511</td><td>42812.86831796745</td><td>38.95503260869566</td><td>26135.101503985294</td></tr>\n",
       "<tr><td>52129470223</td><td>9.220000000000002</td><td>4.866666666666666</td><td>309.45399999999995</td><td>0.6017263523053834</td><td>33.47001707285407</td><td>3312653.5794925285</td><td>109317.56812325344</td><td>39.02289726027398</td><td>66658.68582708623</td></tr>\n",
       "<tr><td>33790986203</td><td>7.944</td><td>5.2</td><td>327.7420000000001</td><td>0.6609718021645656</td><td>27.62230615178351</td><td>2327647.7691004816</td><td>140589.9252536691</td><td>41.86257051282051</td><td>81735.3686604302</td></tr>\n",
       "<tr><td>51420872378</td><td>8.786666666666665</td><td>2.8</td><td>258.158</td><td>0.9624601796783046</td><td>23.669936829738464</td><td>30096.247715517915</td><td>990.1665498405393</td><td>38.90971428571428</td><td>604.8955743448707</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+--------------------------+------------------+---------------------+------------------+------------------+-----------------------+----------------------+------------------+--------------------+\n",
       "|merchant_abn|avg_monthly_revenue_growth|    avg_num_orders|avg_revenue_per_order| coef_of_variation|std_reveune_growth|discounted_revenue_flow|expected_project_value|       combined_fp|   risk_adjusted_epv|\n",
       "+------------+--------------------------+------------------+---------------------+------------------+------------------+-----------------------+----------------------+------------------+--------------------+\n",
       "| 70344541271|         51.08466666666667| 4.866666666666666|   1081.5886666666665|0.5737462225685777|195.37035923989032|    7.229664594670486E8|   4.128138483556847E7| 42.42405479452055|2.3768147512990005E7|\n",
       "| 71616292306|        27.928666666666665|2.6666666666666665|              210.442| 0.730322688378949|106.90505098762485|      5417787.616379169|     177161.6550555988|        39.5622125|  107072.58461398579|\n",
       "| 95276443363|        15.658666666666665|               3.6|    296.6186666666666|0.6820969976318357|51.655349033856346|      6601250.027970397|     251507.6260656721| 39.01125925925927|  153391.33400438444|\n",
       "| 75547072158|        12.812666666666667|3.8666666666666667|   284.17533333333336|0.7476836101265073|  48.9629811281174|      253224.4641329908|    11445.745778811182|39.041500000000006|   6977.154940576614|\n",
       "| 12171241826|        10.885333333333334|               8.2|   228.37266666666665| 0.568344026863831| 40.80740511697543|      5073911.329712124|     209552.5379171107|  38.0224756097561|  129875.47529795238|\n",
       "| 33223110337|        10.853333333333333|               2.8|   214.40200000000002|0.9092808051054941| 38.67401336199554|      463179.7602395222|    21908.402659329404|38.604011904761904|  13450.880288578708|\n",
       "| 70783350473|        10.031333333333333| 3.066666666666667|    576.6173333333334|0.5781118668358438| 36.41120272401783|      2314209.098268511|     42812.86831796745| 38.95503260869566|  26135.101503985294|\n",
       "| 52129470223|         9.220000000000002| 4.866666666666666|   309.45399999999995|0.6017263523053834| 33.47001707285407|     3312653.5794925285|    109317.56812325344| 39.02289726027398|   66658.68582708623|\n",
       "| 33790986203|                     7.944|               5.2|    327.7420000000001|0.6609718021645656| 27.62230615178351|     2327647.7691004816|     140589.9252536691| 41.86257051282051|    81735.3686604302|\n",
       "| 51420872378|         8.786666666666665|               2.8|              258.158|0.9624601796783046|23.669936829738464|     30096.247715517915|     990.1665498405393| 38.90971428571428|   604.8955743448707|\n",
       "+------------+--------------------------+------------------+---------------------+------------------+------------------+-----------------------+----------------------+------------------+--------------------+"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_ranking_metrics.orderBy(\"std_reveune_growth\", ascending = False).limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that the merchant 1 has a risk-adjusted EPV of approximately $23 millions. This is due to their average monthly revenue growth rate is 5108% which is a massive growth. This is due to fact that this merchant has highly fluctuating monthly revenue growth rate which may affect the average monthly growth rate. Thus, it's important that we penalised merchant with high revenue growth standard deviation. We will use a modified Winsorizor method to remove any standard deviation of revenue growth that are outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the intial approach for removing  merchant with unstable revenue growth rate. Double click the cell to view\n",
    "<!-- The formula for calculating the weight is\n",
    "\n",
    "$$W_{\\text{Revenue growth}} = \\frac{1}{\\log (1+s^{\\text{r.g}}_i)} - \\frac{1}{2}$$\n",
    "\n",
    "where $s^{\\text{rev growth}}_i$ is the standard deviation of the merchant's revenue growth rate accross the 15 months period. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lower_bound = merchant_ranking_metrics.select(F.percentile(F.col(\"std_reveune_growth\"), 0.01)).first()[0]\n",
    "upper_bound = merchant_ranking_metrics.select(F.percentile(F.col(\"std_reveune_growth\"), 0.99)).first()[0]\n",
    "\n",
    "merchant_ranking_metrics = merchant_ranking_metrics.filter((lower_bound <= F.col(\"std_reveune_growth\")) & (F.col(\"std_reveune_growth\") <= upper_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "growth_df = merchant_latest_revenue.select(\"merchant_abn\", \"avg_monthly_revenue_growth\").toPandas()\n",
    "\n",
    "# Scatter plot: Merchants vs Growth Rate\n",
    "df_sorted = growth_df.sort_values(by='avg_monthly_revenue_growth', ascending=False)\n",
    "top_5 = df_sorted.head(5)\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.figure(facecolor='#FDFDFD')\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(growth_df['merchant_abn'], growth_df['avg_monthly_revenue_growth'] * 100, color='#4C91D4', label='All Merchants') # Plot all merchants\n",
    "plt.scatter(top_5['merchant_abn'], top_5['avg_monthly_revenue_growth']* 100, color='#FF6F61', label='Top 5 Merchants') # Highlight the top 5 merchants with red color\n",
    "plt.axhline(30, color='#AD505E', linestyle='dashed', linewidth=2)\n",
    "plt.text(10, 30, \"30%\", color='#AD505E',  fontweight= 'bold')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "for i, row in top_5.iterrows():\n",
    "    plt.annotate(f\"{round(row['avg_monthly_revenue_growth'] *100,2)}\", (row['merchant_abn'], row['avg_monthly_revenue_growth']*100), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xticks([])\n",
    "plt.savefig(f\"../plots/growth_rate\", transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>avg_monthly_revenue_growth</th><th>avg_num_orders</th><th>avg_revenue_per_order</th><th>coef_of_variation</th><th>std_reveune_growth</th><th>discounted_revenue_flow</th><th>expected_project_value</th><th>combined_fp</th><th>risk_adjusted_epv</th></tr>\n",
       "<tr><td>97884414539</td><td>1.6406666666666667</td><td>3.0</td><td>9981.536666666665</td><td>0.7347099651497621</td><td>4.043104300033871</td><td>1895396.73499406</td><td>129266.05732659489</td><td>44.049511111111116</td><td>72324.99104162121</td></tr>\n",
       "<tr><td>48534649627</td><td>0.025333333333333336</td><td>3335.4666666666667</td><td>143.09733333333332</td><td>0.17851904439502903</td><td>0.1612392484418581</td><td>1400750.4061942906</td><td>93009.82697130089</td><td>30.673439088983052</td><td>64480.51434849038</td></tr>\n",
       "<tr><td>86578477987</td><td>0.026000000000000002</td><td>13805.266666666666</td><td>35.028</td><td>0.17180320536602955</td><td>0.1635236985883086</td><td>1426866.798267028</td><td>91747.5351285699</td><td>30.291424461195973</td><td>63955.89983008991</td></tr>\n",
       "<tr><td>32361057556</td><td>0.025333333333333333</td><td>4341.933333333333</td><td>109.82733333333334</td><td>0.1775619949607971</td><td>0.17058163804494092</td><td>1380419.1374607098</td><td>91245.70498615292</td><td>30.4234067389335</td><td>63485.65302640829</td></tr>\n",
       "<tr><td>45629217853</td><td>0.027333333333333334</td><td>11255.333333333334</td><td>37.83866666666667</td><td>0.18717135892602974</td><td>0.17826411651842677</td><td>1265784.8255556792</td><td>88351.78082378641</td><td>30.81994237694723</td><td>61121.81288488873</td></tr>\n",
       "<tr><td>79827781481</td><td>0.028666666666666667</td><td>241.2</td><td>2036.1873333333338</td><td>0.1758887478875903</td><td>0.18814381530652352</td><td>1417110.0615200887</td><td>96646.90619567005</td><td>38.2490462962963</td><td>59680.386300950144</td></tr>\n",
       "<tr><td>21439773999</td><td>0.024666666666666667</td><td>6118.266666666666</td><td>78.28</td><td>0.17513345558089158</td><td>0.16422400497346243</td><td>1387461.9684095625</td><td>84635.1800729833</td><td>30.246255208446836</td><td>59036.20751198023</td></tr>\n",
       "<tr><td>96680767841</td><td>0.029333333333333333</td><td>1582.3333333333333</td><td>315.58133333333336</td><td>0.1762583893366516</td><td>0.18997994881666053</td><td>1468326.4819790302</td><td>86778.09508496069</td><td>32.35259734569201</td><td>58703.12739786161</td></tr>\n",
       "<tr><td>38700038932</td><td>0.02266666666666666</td><td>365.26666666666665</td><td>1337.1320000000003</td><td>0.17855375493793263</td><td>0.17878425412717286</td><td>1418536.365320908</td><td>89509.64465174929</td><td>34.81062803431283</td><td>58350.77519719366</td></tr>\n",
       "<tr><td>64403598239</td><td>0.025333333333333333</td><td>5752.2</td><td>78.12666666666668</td><td>0.17188020953378486</td><td>0.1548209411207793</td><td>1323501.879621348</td><td>83512.96860410707</td><td>30.271728173568377</td><td>58232.14975859427</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+------------------+\n",
       "|merchant_abn|avg_monthly_revenue_growth|    avg_num_orders|avg_revenue_per_order|  coef_of_variation| std_reveune_growth|discounted_revenue_flow|expected_project_value|       combined_fp| risk_adjusted_epv|\n",
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+------------------+\n",
       "| 97884414539|        1.6406666666666667|               3.0|    9981.536666666665| 0.7347099651497621|  4.043104300033871|       1895396.73499406|    129266.05732659489|44.049511111111116| 72324.99104162121|\n",
       "| 48534649627|      0.025333333333333336|3335.4666666666667|   143.09733333333332|0.17851904439502903| 0.1612392484418581|     1400750.4061942906|     93009.82697130089|30.673439088983052| 64480.51434849038|\n",
       "| 86578477987|      0.026000000000000002|13805.266666666666|               35.028|0.17180320536602955| 0.1635236985883086|      1426866.798267028|      91747.5351285699|30.291424461195973| 63955.89983008991|\n",
       "| 32361057556|      0.025333333333333333| 4341.933333333333|   109.82733333333334| 0.1775619949607971|0.17058163804494092|     1380419.1374607098|     91245.70498615292|  30.4234067389335| 63485.65302640829|\n",
       "| 45629217853|      0.027333333333333334|11255.333333333334|    37.83866666666667|0.18717135892602974|0.17826411651842677|     1265784.8255556792|     88351.78082378641| 30.81994237694723| 61121.81288488873|\n",
       "| 79827781481|      0.028666666666666667|             241.2|   2036.1873333333338| 0.1758887478875903|0.18814381530652352|     1417110.0615200887|     96646.90619567005|  38.2490462962963|59680.386300950144|\n",
       "| 21439773999|      0.024666666666666667| 6118.266666666666|                78.28|0.17513345558089158|0.16422400497346243|     1387461.9684095625|      84635.1800729833|30.246255208446836| 59036.20751198023|\n",
       "| 96680767841|      0.029333333333333333|1582.3333333333333|   315.58133333333336| 0.1762583893366516|0.18997994881666053|     1468326.4819790302|     86778.09508496069| 32.35259734569201| 58703.12739786161|\n",
       "| 38700038932|       0.02266666666666666|365.26666666666665|   1337.1320000000003|0.17855375493793263|0.17878425412717286|      1418536.365320908|     89509.64465174929| 34.81062803431283| 58350.77519719366|\n",
       "| 64403598239|      0.025333333333333333|            5752.2|    78.12666666666668|0.17188020953378486| 0.1548209411207793|      1323501.879621348|     83512.96860410707|30.271728173568377| 58232.14975859427|\n",
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+------------------+"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_ranking_metrics.orderBy(\"risk_adjusted_epv\", ascending = False).limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some merchant with a very few amount of orders per month. From the BNPL perspective, we would want merchants with a decent amount of order volume per month as more volume would likely result in more revenue for BNPL firm. We also found that a low average monthly order volume also lead to unstable growth rate, thus, led to unrealistic forecasted revenues. Thus, we need to create a weight that penalises merchant with low order volume. The weight is compute using a Sigmoid function\n",
    "\n",
    "$$ W_{\\text{num orders}} = \\frac{1}{1 + e^{-(\\bar{x_i} - \\bar{x_{.}})}}$$\n",
    "\n",
    "where $\\bar{x_i}$ is the average number of order of merchant $i$ and $\\bar{x_.}$ is the average number of order of all merchants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_order_vol = merchant_latest_revenue.select(\"merchant_abn\", \"avg_num_orders\").toPandas()\n",
    "\n",
    "log_mean_order_volume = np.log(np.mean(avg_order_vol['avg_num_orders']))\n",
    "plt.figure(facecolor='#FDFDFD')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.axvline(np.exp(log_mean_order_volume), color='#AD505E', linestyle='dashed', linewidth=2)\n",
    "_, bins, _ = plt.hist(avg_order_vol['avg_num_orders'], bins=20, color=\"#50AD9F\", edgecolor=\"black\", log=True, alpha=0.5)\n",
    "plt.text(np.exp(log_mean_order_volume + 1.4) , 50, f'{np.exp(log_mean_order_volume):.2f}', color='#AD505E',  fontweight= 'bold',\n",
    "         horizontalalignment='center', verticalalignment='bottom')\n",
    "plt.xticks(bins, rotation=45)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.savefig(f\"../plots/order_volume\", transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212.6747613117476"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(avg_order_vol['avg_num_orders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.359764056546743"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mean_order_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212.6747613117476"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute average monthly order volume of all merchants\n",
    "mean_num_orders = agg_transactions.agg(F.mean(\"num_orders\")).collect()[0][0]\n",
    "mean_num_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>avg_monthly_revenue_growth</th><th>avg_num_orders</th><th>avg_revenue_per_order</th><th>coef_of_variation</th><th>std_reveune_growth</th><th>discounted_revenue_flow</th><th>expected_project_value</th><th>combined_fp</th><th>risk_adjusted_epv</th></tr>\n",
       "<tr><td>48534649627</td><td>0.025333333333333336</td><td>3335.4666666666667</td><td>143.09733333333332</td><td>0.17851904439502903</td><td>0.1612392484418581</td><td>1400750.4061942906</td><td>93009.82697130089</td><td>30.673439088983052</td><td>64480.51434849038</td></tr>\n",
       "<tr><td>86578477987</td><td>0.026000000000000002</td><td>13805.266666666666</td><td>35.028</td><td>0.17180320536602955</td><td>0.1635236985883086</td><td>1426866.798267028</td><td>91747.5351285699</td><td>30.291424461195973</td><td>63955.89983008991</td></tr>\n",
       "<tr><td>32361057556</td><td>0.025333333333333333</td><td>4341.933333333333</td><td>109.82733333333334</td><td>0.1775619949607971</td><td>0.17058163804494092</td><td>1380419.1374607098</td><td>91245.70498615292</td><td>30.4234067389335</td><td>63485.65302640829</td></tr>\n",
       "<tr><td>45629217853</td><td>0.027333333333333334</td><td>11255.333333333334</td><td>37.83866666666667</td><td>0.18717135892602974</td><td>0.17826411651842677</td><td>1265784.8255556792</td><td>88351.78082378641</td><td>30.81994237694723</td><td>61121.81288488873</td></tr>\n",
       "<tr><td>79827781481</td><td>0.028666666666666667</td><td>241.2</td><td>2036.1873333333338</td><td>0.1758887478875903</td><td>0.18814381530652352</td><td>1417110.0615200887</td><td>96646.90619567005</td><td>38.2490462962963</td><td>59680.38630092573</td></tr>\n",
       "<tr><td>21439773999</td><td>0.024666666666666667</td><td>6118.266666666666</td><td>78.28</td><td>0.17513345558089158</td><td>0.16422400497346243</td><td>1387461.9684095625</td><td>84635.1800729833</td><td>30.246255208446836</td><td>59036.20751198023</td></tr>\n",
       "<tr><td>96680767841</td><td>0.029333333333333333</td><td>1582.3333333333333</td><td>315.58133333333336</td><td>0.1762583893366516</td><td>0.18997994881666053</td><td>1468326.4819790302</td><td>86778.09508496069</td><td>32.35259734569201</td><td>58703.12739786161</td></tr>\n",
       "<tr><td>38700038932</td><td>0.02266666666666666</td><td>365.26666666666665</td><td>1337.1320000000003</td><td>0.17855375493793263</td><td>0.17878425412717286</td><td>1418536.365320908</td><td>89509.64465174929</td><td>34.81062803431283</td><td>58350.77519719366</td></tr>\n",
       "<tr><td>64403598239</td><td>0.025333333333333333</td><td>5752.2</td><td>78.12666666666668</td><td>0.17188020953378486</td><td>0.1548209411207793</td><td>1323501.879621348</td><td>83512.96860410707</td><td>30.271728173568377</td><td>58232.14975859427</td></tr>\n",
       "<tr><td>89726005175</td><td>0.026</td><td>10937.533333333333</td><td>41.28066666666666</td><td>0.17785400363988527</td><td>0.1696551123392901</td><td>1346359.9274887978</td><td>80916.23164207675</td><td>30.303525237256423</td><td>56395.76096538315</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+-----------------+\n",
       "|merchant_abn|avg_monthly_revenue_growth|    avg_num_orders|avg_revenue_per_order|  coef_of_variation| std_reveune_growth|discounted_revenue_flow|expected_project_value|       combined_fp|risk_adjusted_epv|\n",
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+-----------------+\n",
       "| 48534649627|      0.025333333333333336|3335.4666666666667|   143.09733333333332|0.17851904439502903| 0.1612392484418581|     1400750.4061942906|     93009.82697130089|30.673439088983052|64480.51434849038|\n",
       "| 86578477987|      0.026000000000000002|13805.266666666666|               35.028|0.17180320536602955| 0.1635236985883086|      1426866.798267028|      91747.5351285699|30.291424461195973|63955.89983008991|\n",
       "| 32361057556|      0.025333333333333333| 4341.933333333333|   109.82733333333334| 0.1775619949607971|0.17058163804494092|     1380419.1374607098|     91245.70498615292|  30.4234067389335|63485.65302640829|\n",
       "| 45629217853|      0.027333333333333334|11255.333333333334|    37.83866666666667|0.18717135892602974|0.17826411651842677|     1265784.8255556792|     88351.78082378641| 30.81994237694723|61121.81288488873|\n",
       "| 79827781481|      0.028666666666666667|             241.2|   2036.1873333333338| 0.1758887478875903|0.18814381530652352|     1417110.0615200887|     96646.90619567005|  38.2490462962963|59680.38630092573|\n",
       "| 21439773999|      0.024666666666666667| 6118.266666666666|                78.28|0.17513345558089158|0.16422400497346243|     1387461.9684095625|      84635.1800729833|30.246255208446836|59036.20751198023|\n",
       "| 96680767841|      0.029333333333333333|1582.3333333333333|   315.58133333333336| 0.1762583893366516|0.18997994881666053|     1468326.4819790302|     86778.09508496069| 32.35259734569201|58703.12739786161|\n",
       "| 38700038932|       0.02266666666666666|365.26666666666665|   1337.1320000000003|0.17855375493793263|0.17878425412717286|      1418536.365320908|     89509.64465174929| 34.81062803431283|58350.77519719366|\n",
       "| 64403598239|      0.025333333333333333|            5752.2|    78.12666666666668|0.17188020953378486| 0.1548209411207793|      1323501.879621348|     83512.96860410707|30.271728173568377|58232.14975859427|\n",
       "| 89726005175|                     0.026|10937.533333333333|    41.28066666666666|0.17785400363988527| 0.1696551123392901|     1346359.9274887978|     80916.23164207675|30.303525237256423|56395.76096538315|\n",
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+-----------------+"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_ranking_metrics = merchant_ranking_metrics.withColumn(\"risk_adjusted_epv\", F.col(\"risk_adjusted_epv\") * (1/(1 + F.exp(-F.col(\"avg_num_orders\"))*np.exp(mean_num_orders))))\n",
    "merchant_ranking_metrics.orderBy(\"risk_adjusted_epv\", ascending = False).limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **coefficient of variation** is a ratio between the standard deviation and the mean, measuring the relative stability which help us compare merchants with different average revenue. Thus, we will create a weight that favors merchant with higher stability. The weight is calculate as\n",
    "\n",
    "$$W_{\\text{CV}} = \\frac{1}{1 + CV}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>avg_monthly_revenue_growth</th><th>avg_num_orders</th><th>avg_revenue_per_order</th><th>coef_of_variation</th><th>std_reveune_growth</th><th>discounted_revenue_flow</th><th>expected_project_value</th><th>combined_fp</th><th>risk_adjusted_epv</th></tr>\n",
       "<tr><td>48534649627</td><td>0.025333333333333336</td><td>3335.4666666666667</td><td>143.09733333333332</td><td>0.17851904439502903</td><td>0.1612392484418581</td><td>1400750.4061942906</td><td>93009.82697130089</td><td>30.673439088983052</td><td>54713.17129337545</td></tr>\n",
       "<tr><td>86578477987</td><td>0.026000000000000002</td><td>13805.266666666666</td><td>35.028</td><td>0.17180320536602955</td><td>0.1635236985883086</td><td>1426866.798267028</td><td>91747.5351285699</td><td>30.291424461195973</td><td>54579.04496012397</td></tr>\n",
       "<tr><td>32361057556</td><td>0.025333333333333333</td><td>4341.933333333333</td><td>109.82733333333334</td><td>0.1775619949607971</td><td>0.17058163804494092</td><td>1380419.1374607098</td><td>91245.70498615292</td><td>30.4234067389335</td><td>53912.79040771167</td></tr>\n",
       "<tr><td>45629217853</td><td>0.027333333333333334</td><td>11255.333333333334</td><td>37.83866666666667</td><td>0.18717135892602974</td><td>0.17826411651842677</td><td>1265784.8255556792</td><td>88351.78082378641</td><td>30.81994237694723</td><td>51485.24888621164</td></tr>\n",
       "<tr><td>79827781481</td><td>0.028666666666666667</td><td>241.2</td><td>2036.1873333333338</td><td>0.1758887478875903</td><td>0.18814381530652352</td><td>1417110.0615200887</td><td>96646.90619567005</td><td>38.2490462962963</td><td>50753.42918974075</td></tr>\n",
       "<tr><td>21439773999</td><td>0.024666666666666667</td><td>6118.266666666666</td><td>78.28</td><td>0.17513345558089158</td><td>0.16422400497346243</td><td>1387461.9684095625</td><td>84635.1800729833</td><td>30.246255208446836</td><td>50237.87488272766</td></tr>\n",
       "<tr><td>96680767841</td><td>0.029333333333333333</td><td>1582.3333333333333</td><td>315.58133333333336</td><td>0.1762583893366516</td><td>0.18997994881666053</td><td>1468326.4819790302</td><td>86778.09508496069</td><td>32.35259734569201</td><td>49906.659905709246</td></tr>\n",
       "<tr><td>64403598239</td><td>0.025333333333333333</td><td>5752.2</td><td>78.12666666666668</td><td>0.17188020953378486</td><td>0.1548209411207793</td><td>1323501.879621348</td><td>83512.96860410707</td><td>30.271728173568377</td><td>49691.21355992612</td></tr>\n",
       "<tr><td>38700038932</td><td>0.02266666666666666</td><td>365.26666666666665</td><td>1337.1320000000003</td><td>0.17855375493793263</td><td>0.17878425412717286</td><td>1418536.365320908</td><td>89509.64465174929</td><td>34.81062803431283</td><td>49510.49110209373</td></tr>\n",
       "<tr><td>89726005175</td><td>0.026</td><td>10937.533333333333</td><td>41.28066666666666</td><td>0.17785400363988527</td><td>0.1696551123392901</td><td>1346359.9274887978</td><td>80916.23164207675</td><td>30.303525237256423</td><td>47880.09446935283</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+------------------+\n",
       "|merchant_abn|avg_monthly_revenue_growth|    avg_num_orders|avg_revenue_per_order|  coef_of_variation| std_reveune_growth|discounted_revenue_flow|expected_project_value|       combined_fp| risk_adjusted_epv|\n",
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+------------------+\n",
       "| 48534649627|      0.025333333333333336|3335.4666666666667|   143.09733333333332|0.17851904439502903| 0.1612392484418581|     1400750.4061942906|     93009.82697130089|30.673439088983052| 54713.17129337545|\n",
       "| 86578477987|      0.026000000000000002|13805.266666666666|               35.028|0.17180320536602955| 0.1635236985883086|      1426866.798267028|      91747.5351285699|30.291424461195973| 54579.04496012397|\n",
       "| 32361057556|      0.025333333333333333| 4341.933333333333|   109.82733333333334| 0.1775619949607971|0.17058163804494092|     1380419.1374607098|     91245.70498615292|  30.4234067389335| 53912.79040771167|\n",
       "| 45629217853|      0.027333333333333334|11255.333333333334|    37.83866666666667|0.18717135892602974|0.17826411651842677|     1265784.8255556792|     88351.78082378641| 30.81994237694723| 51485.24888621164|\n",
       "| 79827781481|      0.028666666666666667|             241.2|   2036.1873333333338| 0.1758887478875903|0.18814381530652352|     1417110.0615200887|     96646.90619567005|  38.2490462962963| 50753.42918974075|\n",
       "| 21439773999|      0.024666666666666667| 6118.266666666666|                78.28|0.17513345558089158|0.16422400497346243|     1387461.9684095625|      84635.1800729833|30.246255208446836| 50237.87488272766|\n",
       "| 96680767841|      0.029333333333333333|1582.3333333333333|   315.58133333333336| 0.1762583893366516|0.18997994881666053|     1468326.4819790302|     86778.09508496069| 32.35259734569201|49906.659905709246|\n",
       "| 64403598239|      0.025333333333333333|            5752.2|    78.12666666666668|0.17188020953378486| 0.1548209411207793|      1323501.879621348|     83512.96860410707|30.271728173568377| 49691.21355992612|\n",
       "| 38700038932|       0.02266666666666666|365.26666666666665|   1337.1320000000003|0.17855375493793263|0.17878425412717286|      1418536.365320908|     89509.64465174929| 34.81062803431283| 49510.49110209373|\n",
       "| 89726005175|                     0.026|10937.533333333333|    41.28066666666666|0.17785400363988527| 0.1696551123392901|     1346359.9274887978|     80916.23164207675|30.303525237256423| 47880.09446935283|\n",
       "+------------+--------------------------+------------------+---------------------+-------------------+-------------------+-----------------------+----------------------+------------------+------------------+"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_ranking_metrics = merchant_ranking_metrics.withColumn(\"risk_adjusted_epv\", F.col(\"risk_adjusted_epv\") * (1/(1 + F.col(\"coef_of_variation\") )))\n",
    "merchant_ranking_metrics.orderBy(\"risk_adjusted_epv\", ascending = False).limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found the adjusted EPV that accounts for different factor, we can now merge the merchants to their respective segments and will select top 20 merchants from each segnment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+--------------------------------------+-------------------------------------------------+\n",
      "|merchant_abn|risk_adjusted_epv     |name                                  |segments                                         |\n",
      "+------------+----------------------+--------------------------------------+-------------------------------------------------+\n",
      "|73256306726 |5106.3445582384       |Id LLP                                |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|73841664453 |5.079036435468569E-70 |Lacinia At LLP                        |Books, Media, Arts, Crafts, and Hobbies          |\n",
      "|83412691377 |1405.0444371658846    |Suspendisse Sagittis Nullam Associates|Fashion, Personal Accessories, Health, and Beauty|\n",
      "|92202115241 |1.4102977797408417E-88|Fames Ac Turpis Limited               |Books, Media, Arts, Crafts, and Hobbies          |\n",
      "|96946925998 |1.0739640028656378E-87|Nisi Cum Corporation                  |Books, Media, Arts, Crafts, and Hobbies          |\n",
      "|64185141673 |2.3941758416049687E-88|Maecenas Corp.                        |Books, Media, Arts, Crafts, and Hobbies          |\n",
      "|66610548417 |2.4996015505912874E-85|Nulla Inc.                            |Vehicles, Repairs, and Miscellaneous Services    |\n",
      "|71002398501 |3.3941808635930086E-88|Ipsum Phasellus Corp.                 |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|72762528640 |1.9403629120087297E-90|Sit Amet Risus Associates             |Vehicles, Repairs, and Miscellaneous Services    |\n",
      "|87211363921 |1.0379429906229265E-76|Mauris Non PC                         |Books, Media, Arts, Crafts, and Hobbies          |\n",
      "+------------+----------------------+--------------------------------------+-------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading in the segmented merchants\n",
    "segments = spark.read.parquet(f\"../data/curated/segmented_merchants_info.parquet/\")\n",
    "segments = segments.select(\"name\", \"merchant_abn\", \"segments\")\n",
    "\n",
    "complete_ranking = merchant_ranking_metrics.select(\"merchant_abn\", \"risk_adjusted_epv\")\n",
    "\n",
    "# Merge ranking with segments\n",
    "complete_ranking = complete_ranking.join(segments, on = 'merchant_abn', how='inner')\n",
    "complete_ranking.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvgAAAFUCAYAAABRFEjrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3gU5fr/8ffuZtOBNHrvICCEgKKgIEWaIUEBwSOickThxwELIIKAohCpHo2AckQUUNADaGiJghSlhaoQipCQQkIIkCWkZ7O78/sjh/2CpGxImU1yv66Ly2T3mZnPbNbde5555hmNoigKQgghhBBCiEpBq3YAIYQQQgghROmRAl8IIYQQQohKRAp8IYQQQgghKhEp8IUQQgghhKhEpMAXQgghhBCiEpECXwghhBBCiEpECnwhhBBCCCEqESnwhRBCCCGEqESkwBdCCCGEEKISkQJfCCGEEEKISkQKfCGEEEIIISoRKfCFEEIIIYSoRKTAF0IIIYQQohKRAl8IIYQQQohKRAp8IYQQQgghKhEp8IUQQgghhKhEpMAXQgghhBCiEpECXwghhBBCiEpECnyhiuvXr/PGG2/Qt29fBg0axCuvvEJ0dLQqWT7//PMy30Z4eDh+fn4EBARY/x08eBAAX1/f+1rnrl27iIyMLLWMSUlJTJo0qdTWV1Lh4eG8+uqr+T537Ngxhg0bxoABAxgwYADff/+99TmDwcDw4cMJDAzk2LFjhIaGMnDgQEaPHs3p06f58MMPS5ztjz/+YPjw4QQEBDBw4ECCg4NLvM6/Cw4OZtWqVQB88skn1veLEEIIURQHtQOIqkdRFCZOnEhgYCAff/wxAOfOnSM5OZmmTZuWe54vvviC1157rVjLmM1mdDpdsZbp0qULX3zxRbGWKcyuXbvo1asXLVq0uOc5k8mEg0Px/veuXbs2n376aWnFKzPXr19nypQpLFu2jHbt2mEwGPjnP/9J7dq16dWrF4cOHaJZs2YsWLAAgLFjxzJnzhy6desGQIcOHUqc4e233+aTTz6hTZs2mM3mMj84nTx5cpmuXwghROUiBb4od4cPH8bBwYFRo0ZZH2vbti2QV/wvXLiQ33//HY1Gw/jx4xk0aBDh4eEEBwfj7e3N+fPn6devH61atWLNmjXk5OSwbNkyGjVqxPTp03F0dCQyMpLk5GSmT5/OE088webNm4mIiGD27NkAvPrqq7z88sv8/vvvZGdnExAQQIsWLViyZAkhISGsXbuW3NxcOnbsyJw5c9DpdPj6+vLiiy+yf/9+3n77bfbu3cvu3bvR6XT06NGDt99+u8SvzZdffkloaChGo5F+/fpZe9R/+uknVq1ahUajoXXr1owaNYrdu3dz5MgRVqxYQXBwMDNnzsTX15cTJ07Qu3dv+vfvz4wZMzAYDHh5eREUFES9evWYPn067u7uREREcP36daZOncqAAQOIj4/ntddeY9u2bZjNZhYvXsz+/fsBGDFiBKNHj2bx4sU273N8fDzTpk0jKysLgFmzZtG5c2fCw8P57LPP8PT05MKFC7Rr147Fixej0Wj47bffmD9/Pp6enrRr1y7f9X777bcMHTrU+ryXlxdTp04lODiY2rVrs2jRIuvftF+/fpw4cYI5c+bQu3dvevXqxVdffcUXX3xBRkYGH374IREREQBMnDiR/v37s3//foKDgzEajTRs2JCgoCDc3NzuymAwGKhZsyYAOp3OepAVHByMq6srY8eOBeCpp56yniH65z//SceOHTl79ixNmzZlwYIFuLi40Lt3bwYOHEh4eDgAS5YsoXHjxndtb/r06fTq1YsBAwYQERHBRx99RGZmJp6engQFBVGrVi3WrFnDhg0brHluHzwLIYSoeqTAF+Xu4sWLBRZvv/zyC+fPnyckJISbN28ybNgwunTpAsD58+fZsWMHHh4e9OnTh+HDh7Nx40a++eYb1q5dy8yZMwFISEhg3bp1xMXF8cILL/Doo48WmGXKlCl8++23hISEABAVFUVoaCjr169Hr9fz3nvvsXXrVgIDA8nMzKRly5ZMnjyZlJQUZs6cSVhYGBqNhtTU1CL3+9ixYwQEBFh/Dw4OplGjRtbf9+/fT2xsLBs3bkRRFMaPH8/Ro0fx8PBgxYoVrF+/Hi8vL1JSUvDw8LAWrAMGDLCuIzU1lXXr1gHw2muvERgYyNChQ9m4cSMffvghy5cvB+DatWt89913XLp0ifHjx9+1DoDvv/+e+Ph4fvzxRxwcHEhJSSElJYWdO3favM/e3t6sXr0aJycnYmJiePPNN9m8eTMAZ8+eZfv27dSqVYtRo0Zx/PhxOnTowKxZs/jmm29o3Lgxr7/+er7rjYyMJDAw8K7H2rdvT2RkJG3btmXSpEl3HcyFh4czbdo0OnToYC2iAZYvX467uztbt24F4NatWxgMBlasWMHq1atxdXVl5cqVrF69mokTJ961vTFjxjBgwAAeeughHnvsMYYOHYqTk1Ohr0d0dDTz5s3Dz8+Pd955h++++856IODu7s7GjRv56aefmD9/foFnenJzc61/Ry8vL3bs2MHHH39MUFAQK1euZPfu3Tg6Otr0fhRCCFF5SYEv7Mrx48cZPHgwOp0OHx8funbtyunTp3F3d6dDhw7UqlULgEaNGtG9e3cAWrVqdVfhNnDgQLRaLU2aNKFhw4ZcunTJ5u0fOnSIiIgIhg0bBkB2djbe3t5AXk9t//79gbyCzMnJiZkzZ9KrVy969epV5LqLGqJz4MABDhw4YC1eMzMziYmJITs7mwEDBuDl5QWAh4dHgesYNGiQ9eeTJ09ax4YHBASwaNEi63N9+/ZFq9XSokULbty4ke/rMHLkSOswHw8PD0wmU7H22WQyMXfuXM6fP49WqyUmJsb63IMPPkidOnUAaNOmDQkJCbi5udGgQQOaNGkCwJAhQ/jhhx/uWa+iKGg0mnsez++xwhw6dIilS5daf69RowZ79uwhMjLSenYpNzeXTp063bPsxIkTGTJkCPv372fbtm1s376dtWvXFrq9unXr4ufnZ923tWvX3tXTDzB48GCCgoIKXEd0dDQXLlzgpZdeAsBisVjPJLRu3ZopU6bQp08f+vbta+vLIIQQohKSAl+Uu5YtW/Lzzz/n+5yiKAUu5+joaP1Zq9Vaf9dqtZjNZutzfy/0NBoNOp0Oi8VifSwnJ6fA7Q8dOpS33nrrnuecnJys4+4dHBzYuHEjhw4dYvv27axbt441a9YUmN0WiqIwbtw4Ro4cedfjxVmvi4tLgc/d+brc+VoWlOXvr2Nx9/nrr7/Gx8eHkJAQLBYLDz74YL7b1+l01r+fLUV6ixYtiIiIoE+fPtbHIiIiaN68eZHL3im/fVQUhe7du99V+BekUaNGPPfcc4wYMYJHHnmEmzdvFvo+y+99WVyKotCyZcu7Liq+beXKlRw9epTdu3ezfPlytm/fXuzrMIQQQlQOMouOKHfdunXDaDTe1Tt76tQpjhw5QteuXQkNDcVsNmMwGDh27NhdhaEtwsLCsFgsxMXFcfnyZZo2bUr9+vU5f/48FouFxMRETp06ZW3v4OBAbm4uAI888gg///wzycnJAKSkpJCQkHDPNjIyMkhLS6Nnz57MmDGD8+fPA7Bz506WLFlS7NcEoEePHmzatImMjAwgb1ab5ORkHnnkEcLCwrh586Y1E4Cbm5u1bX58fX3Zvn07AFu3brX2Htuie/fubNiwAZPJZN1mcfc5LS2NmjVrotVqCQkJuesgLD/NmjUjPj6euLg4AGv2v/vHP/7Bjz/+yLlz5wC4efMmixcv5p///KfN+3d7H28PZ4K8ITqdOnXixIkTxMbGApCVlZXvBbR79+61HozGxsai1WqpXr069evX5+zZswCcOXOG+Ph46zJXrlzh5MmT1n278+8RGhoKwI4dOwqdValp06YYDAbrenJzc7l48aL1fd2tWzemTp1KWloamZmZxXo9hBBCVB7SvSPKnUaj4bPPPmP+/PmsXLkSJycn6tevz4wZM+jatSsnT54kICAAjUbD1KlTqVmzZrGG2TRt2pTnn3+e5ORk3n//fZycnPDz86N+/fr4+/vTsmXLu64BGDFiBEOGDOGBBx5gyZIlvP7667z88stYLBb0ej2zZ8+mfv36d20jIyODCRMmWHto33nnHQDi4uJwd3fPN9ffx+D/fex7jx49iIqKsvbgu7q6smjRIlq2bMlrr73G6NGj0Wq1PPDAA3z00UcMGjSIWbNmsXbt2nxnv3n33XeZMWMGq1atsl5ka6vhw4cTExPDkCFDcHBwYMSIETz55JPF2ufnnnuOf/3rX4SFhfHwww/j6upa6DadnJyYO3cu48aNw9PTEz8/Py5evHhPu1q1arFo0SLeffddMjIyUBSFMWPG0Lt3b5v3D/Je/7lz5/LUU0+h1WqZOHEiTz75JEFBQbz55psYjUYAXn/99XtmdwoJCSEoKAhnZ2d0Oh2LFy+2DuEKCQkhICCADh06WIcbATRv3pwff/yR2bNn06RJk7suMjcajQwfPhyLxVLo2QNHR0c+/fRTPvzwQ9LS0jCbzYwZM4YmTZowdepU0tPTURSFF198kerVqxfr9RBCCFF5aJTCxkQIUcHcOduIGqZMmcKMGTOs4+Wrgqq4z8V15wxFf9e7d282btwor58QQohSIz34QpSixYsXqx2h3FXFfRZCCCHsmfTgCyGEEEIIUYnIRbZCCCGEEEJUIlLgCyGEEEIIUYnIGHwhhBAlpigKmUYLOSYLOo0GB50GnVaDgzbvZyGEEOVHCnwhhBD3MFsUrqXmkpCSQ9ItI6nZZjJyLKTnmMmw/rOQbsz7OctowVLIFV06LXnFvvZ/hb9Og5ujDk83B7zdHfBy0+Pl5oC3mwPe7nk/e7np5eBACCHug1xkK4QQVZSiKNxIN3ElJYcrKUaupBhJuJnDlVtGkm7lYiqsYi8HGqC6iw5vNz0NvZ1o5uNM05rONPNxpoar9E8JIURBpMAXQogqIuFmDn9dzeKvq5lcSMoizpCD0VQxvwK83Bxo6uNMs5rO1v/W9XBEq5EefyGEkAJfCCEqIbNFIfJaFhEJmUQkZPDX1SzSss1qxypTznotzWs649vIHd9GbrSo7SIFvxCiSpICXwghKomYG9kcjU7jdEIG5xOzyMq1qB1JVdWcdXRs6Pa/gt+dmtX0akcSQohyIQW+EEJUYBeSsjgYmcqhqFSupBjVjmPXGng60rlxXrHfvr4bznqZKVoIUTlJgS+EEBWIRVE4eyWTQ1GpHIxM40Z6rtqRKiRHnYauTavRu40HnRu7y2w9QohKRQp8IYSwc2aLwp+XMzgYlUr4pTRSMk1qR6pUarjoeKxVDZ5o40Gr2i5qxxFCiBKTAl8IIexUcnouP0fc5OczNzFkSFFfHhp4OvJEGw96talBrWqOascRQoj7IgW+EELYmdPxGWw/ZeDwpVTMVfs6WdVogPb1Xend1oMeLWvIeH0hRIUiBb4QQtiBTKOZPedS2HH6JnGGHLXjiDtUc9YxoL0nT3X0wstNZuIRQtg/KfCFEEJFscnZbD9lYO/5W1V+Wkt756DV8Hir6gR29qGpj7PacYQQokBS4AshhAoir2Xx7eFrHItJVzuKKKaBHTyZ8EQ9tWMIIUSBHNQOIIQQVUlscjbfHr7G4ag0pHel4tFqYKivj9oxhBCiUFLgCyFEObiSksP68Ov8duEWFqnsK6xHmlenrofMriOEsG9S4AshRBm6npbLhiPX+PVcisyIUwk84ye990II+ycFvhBClIGbGbn8cPQGP5+5Sa5ZuuwrgwcbuNHShhthKYoZjUZXDomEECJ/UuALIUQpMlsUtv6ZzHeHr8usOJWMrb33plP/RslMwqHV82i9HyzjVEIIcS8p8IUQopREXsvis1+vEHU9W+0oopQ183Gmc2P3Itsp2QbMcWFgMWK8Fo7GqwMObV5GV7NzOaQUQog8UuALIUQJZRnNrDt8jW1/GuQC2kpqqJ+3Te1MlzaBxWj9XTGcJvfgG5hrdsHhgVfRerQqq4hCCGEl8+ALIUQJHI5K5Yt9idxIN6kdRZSRWtX1rHyhJTqtptB2iimTnF+GQ25B9zbQoK3/BA5txqJ1b1D6QYUQ4n+kB18IIe7DjbRcvtiXyOFLaWpHEWUs0Ne7yOIewByztZDiHkDBkrAb45Xf0DUejEPrMWicbTszIIQQxSE9+EIIUQyKorDtlIG1B6/JRbRVQHVnHateaoWzXltoO8ViImfnSMi+bvvKdS44tHkRXbNhaLTS3yaEKD3yiSKEEDZKzTLx8c4EjsUU1ksrKpPBD3oVWdwDWOJ3Fa+4BzBnYTqzAnNcGPqOb8qMO0KIUiM9+EIIYYOzVzJZFHZZxtpXIU4OGr56qRXVXQrvC1MUBeOel1DSokuwNQ26hv1xaDcejZNHCdYjhBDSgy+EEIVSFIWNx2/w7eFrcifaKqbfA55FFvcAlqRDJSzuARTMl8MwXz2IwwOvoGvsj0ZT9Lh/IYTIjxT4QghRgFtZJj7+JYHjsTIkp6rRaSGws41TY178rvQ2nJuK6c8lWBJ+Re87A41r7dJbtxCiyih6YKEQQlRBZxIymPxdlBT3VVT3FjWoXd2xyHYWQwSK4XSpb99y4w9y9rycd9MsIYQoJunBF0KIOyiKwn+P5Q3JkZtWVV3P2Hpjq4vryy6EKZ3ck0GYrx5E3+ktNI41ym5bQohKRQp8IYT4n+xcC4vD4gmPlrntqzLfRm40q+lSZDtLWiyWqwfKPI8lcR85htPofd9GV7tbmW9PCFHxyRAdIYQAbmWaeHdzjBT3gmf8fGxqZ47cAJTTaZ4cA7mH3yb31CcoFpnJSQhROCnwhRBV3pWUHKb+N5q/krLUjiJU1qKWMx0buhfZTsm6gTl+Zzkkups5ejPGA5NRsm6U+7aFEBWHFPhCiCrtwtVMpv03msRbRrWjCDtga++96dJ/wZJbxmnypxgiyNn3CpYbf6iyfSGE/ZMCXwhRZR2JTmPG5hhuZZnVjiLsQN0ajjzaonqR7ZTcdMwxW8shUSFyDBgPvokp8nt1cwgh7JIU+EKIKikswsC8bXHkmGSqHJFnaGdvtDbcXMocswVMGeWQqAiKGdOZ5RiPzkExZaqdRghhR6TAF0JUOesOJbFsd6JMgymsPFx09GnrUWQ7xWzEFLWxHBLZznJlL8b9k1Gyk9WOIoSwE1LgCyGqDEVRWLb7Ct8flQsUxd2e6uSNo0PRX4nm+F8gx/4KaeXWBYy/T8CSFqN2FCGEHZACXwhRZfznt6uERdxUO4awMy56LYM7eBXZTlEsmO14zLuSeRXj7xOx3PhT7ShCCJVJgS+EqBLWHExi658GtWMIO/RkO0/cnXVFtrMk7kdJjyuHRCWQm4bx0BTMCbvVTiKEUJEU+EKISu/7o9f57zEZliPu5aDVEODrbVNbU+T6Mk5TSixGco/NxRS5Qe0kQgiVSIEvhKjUQk4ms+7QNbVjCDv1eKvq1KymL7Kd5cafKDfPlkOi0qJgOrMC0/mv1Q4ihFCBFPhCiEorLMLAl79fVTuGsFMa4Glbb2xVUXrv/8b012pyz61SO4YQopxJgS+EqJT2nE9hxZ5EtWMIO+bXxJ3G3s5FtrOkXsKSdLgcEpUN84U15J5dqXYMIUQ5kgJfCFHpHIhM5d87E2See1GoZ2zuvd8AVOw3k/nit+SeWaF2DCFEOZECXwhRqUQkZLA4LF6Ke1Go1nVcaF/frch2StY1LPG/lkOismeO3EDu6c/UjiGEKAdS4AshKo2rt4wEbb+MSap7UQSbe++jfgDFVMZpyo/50n/JPfsftWMIIcqYFPhCiEoh02jmg61xpGab1Y4i7Fx9T0ceblatyHaKMQ1zzLZySFS+zBfXYbq0We0YQogyJAW+EKLCsygKi8LiiTPkqB1FVABDfX3QajRFtjNH/wTmrHJIVP5Mp4PlZlhCVGJS4AshKryrt4xcTKqchZgoXV5uDvRuU6PIdoo5B1P0pnJIpBYLuSfmY75+XO0gQogyIAW+EKLCq+fhxNJnm9HMp+gpD0XV5t/RC71D0V995rgwyLlZDolUZMkl98i7WFIuqJ1ECFHKpMAXQlQKtao7smB4Ux5vVV3tKMJOuTpqGdjBq8h2imLBHPV9OSSyA6ZMjIffxpIh94wQojKRAl8IUWk467VMHdCQMY/WQlv0EGtRxQxo74mbk67IdpYr+1AyEsohkZ3IMZB7ZAaKKVPtJEKIUiIFvhCi0hnWpSaz/Bvh5iQfcSKPg1bDkE7eNrU1Ra4v4zT2R0m9RO7xeSiKTDErRGUg335CiEqpS5NqLH22GQ09ndSOIuzAE21q4O2uL7Kd+foJlJS/yiGR/bFc3Y/p/FdqxxBClAIp8IUQFcqFfZHkZBhtalvPw4nFI5rycNOi5zwXlZcGGNrZthtbmS9+V7Zh7Jz5wlrMifvVjiGEKCEp8IUQFcalQzGEf3uc0Pk7Sblyy6ZlXJ10zHyqIc8+VBMZll81PdSsGg29ij6TY7l1Ecv1o+WQyJ4p5J6YjyX9stpBhBAlIAW+EKJCMFy+yeF1xwBIu55OaNAu4k7G27SsRqPh+W61eHtQQ1z08rFX1Qzzs6333nSx6o29z5cpg9wj76KY5cZxQlRU8k0nhLB7ORk57FtxAHOu2fqYKcfEvs8P8OeWCJsvDOzeojqLRjSlTvWix2KLyuGBeq60qetaZDtLZiKWK3vLPE9FoaTFYIr4TO0YQoj7JAW+EMLuHfgqnPQbGfc+ocCpbWfYu/wAudm5Nq2rsbczS0c2o1NDt1JOKezRMzb23psjfwDFXHTDKsQcs0XG4wtRQUmBL4Swaxd+iyLhdOE34Yn/M4HQoF2kXkuzaZ3VnB14L6AxATZOmygqpkZeTnRt4l5kO8V4C3PcjnJIVPHk/rEQJTtZ7RhCiGKSAl8IYbfSrqdz/L9/2NT2VmIqofN3cuWMbXfk1Gk1/PPxOrzRrz6OOrn8tjJ62s8Hjabov6350o9gzi6HRBWQ8Ra5J2R+fCEqGinwhRB2SbEoHFwdjinHZPMyxsxcdn/6O2fCztm8TO+2Hnw0rCk+7g73E1PYKR93B3q2qlFkO8WUjSl6czkkqrgs149jjvpB7RhCiGKQAl8IYZfO7vyLa5E3ir2coiic2HyK3788hMlo28FBy9ouLB3ZnLY2XIwpKoaATt442HBmxhy3A4y2TblalZnO/QfLrSi1YwghbCQFvhDC7txMSOGPkNMlWkfMkTh+XribjOR8Ls7Nh6erA/Oebkz/9p4l2q5Qn5uT1qa/o6KYMUd9Xw6JKgFLLrl/LEJRLGonEULYQAp8IYRdsZgsHPgqHIup5IWEIe4m2+fvJOnCNZva63VaJvaux/hedXHQyrj8impQBy9cHHVFtrMk7EHJvFoOiSoHJeUc5ugf1Y4hhLCBRpErZ4QQduTkj6eICLV9DL0ttDotXZ7tROteLW1e5kxCBh/tuExKlkydeCetBnyq6anv4Ug9DydqVdPj5qTD3UmLm5Puf//y+o5MZgWzBUwWBbNFIT3HjCHDhCEjl+R0E4YME9fSjMQbjJgspfNV5KjT8OVLrfB0Lfqaipy9Y1FuRZbKdqsMB1ecen+DxqWW2kmEEIWQAl8IYTeuX0rm54W/opRSsfd3LXo046HnOqNzKLp3F+B6Wi7zt8cRea1qzrDioNXQvJYzreu40KaOK018nKlTXY/eoXRP/prMCpcNOVy6kc2l61lcup7NhatZGM3Ffx8MaO/J/+tdr8h25mtHyT005X7iVnnaOj1wfHie2jGEEIWQAl8IYRcUi8KO+TsxxN0s0+3UbO5Nz9e641LDxab2OSYLwb9eYd9fVeNCzAaejjzSvDodG7rRuo4rznp1RnLmmCycScjkZFw6J2LTiTPkFLmMVgMrRregnodTkW2NB97AcuNEaUStkvRdP0BX73G1YwghCiAFvhDCLlz8PYrDa4+Vy7ZcPVzoOaE7Pk1sv9HV5hM3+OZAEmV0ckFVzWo682jz6jzaojoNvYoujtWQnJ7LgchU9pxPKfCMSvcW1Zk+qGGR67Kk/IVx37jSjli1OPvg1HsNGr3cEVoIeyQFvhBCdcasXELe3U52WtG9tKVFp9fx8PN+NH+kqc3LnIhNZ1FYPOk5FX9cfjVnHX0f8GBAe0+berztyWVDDrvPpbD3r1vcSM+1Pr702Wa0rF30mRnj0TlYruwtu4BVhK7lP9A/IAdKQtgjKfCFEKo7vvEPzv7ylyrbbtOnJX7DO6HV2jYUJTHFyIfb4mwaMmKPWtRyZvCDXjzeqgaOpTyWvrxZFIVj0en8dPIGGo2GeU83KXqZjASMv44GpeIfpKlO64hTn3VoXGurnUQI8TdS4AshVJV6LY2t74WVyrSY96tOm9o8/uojOLnZ1pOdZTSz9JcEDl9KK+NkpadTQzf+0a0WbSrpzbxyTBacbDhgyf1zCeaYLeWQqGrQNuiHo9+7ascQQvyNFPhCCFXt+ex34k9dUTsG7j5u9JrQA88GHja1VxSFDUeusz78Ovb8IdqunivPP1KL9vVlrLSSc5OcX0aAxah2lEpEg2PPL9B6tFY7iBDiDlLgCyFUc+XMVX79ZJ/aMawcnBx49MWHaOxX9IWatx2KSuXjXxLIyrWvO3w283FmTPfadG7srnYUu5F77kvMF9aqHaPS0fp0wrH7J2rHEELcQQp8IYQqLGYL2+b+zK3EVLWj3E0D7Qc+QKeA9mg0tt3NNi45mw+3XSbxlvo9w856Lf/oVhP/jt7o5G68VoopM6/3PrfiDKuqSPQPzUdXt7vaMYQQ/1Oxr7ASQlRYF3+Psr/iHkCBiB1n2btsP8as3KLbA428nVn6bDM6N1K3t/yhptVY9nxzAn19pLj/G3PsNinuy5Dp3EoUxb7OYglRlUmBL4Qod2aTmYjQc2rHKFT8qSuEBu0kNcm2otDdWcfsIY0Y2tn2ufVLS3VnHdMHNWSWfyNqVXMs9+1XBFrvTmjrdAfkwKcsKGkxWBJ/UzuGEOJ/ZIiOEKLcledNrUpK76LnsX8+Qv0OdW1eZu9fKQT/egWjqew/Xh+o58q0ASaOIhsAACAASURBVA3wdteX+bYqA8utSEx/rcGSaD/XflQWmhotcOq1Su0YQgikwBdClDOLxcKW2aGkXUtXO4rNNBoNnQI70H5gW5uXibyWxbxtl++6EVOpZgKGdfHhH91qyXCc+2C+fgLTqY9R0uPUjlKp6LstQFe7m9oxhKjypMAXQpSr6COx7P/ysNox7kvjLg15dMxDODg52NQ+JdPERzsuc+ZKZqnmqOasY0r/BjJDTgkpllzMkRswXVgL5op54zJ7o/Fqj9Njy9SOIUSVJwW+EKLcKIrCtrk/k5JwS+0o982zoQe9JvTA3du2eeVNZoWVvyUSevpmqWy/TnU97wc2pp6HbTflEkWzZCZiOvUJlqRDakepFPTd/43Ox1ftGEJUaVLgCyHKzeU/E9i7bL/aMUrMyd2Jx199lDqta9m8zM8RBj7fexWT5f4/clvWdmG2fyM8XG07gyCKx5y4n9w/l0COQe0oFZrWxw/H7kvVjiFElSaz6Aghyo29z5xjq5z0HHZ9vJfzuy/YvEz/9l7Mf6YJnvdZnHdp4s78p5tIcV+GdHV74NTrSzReHdSOUqFZbhzHcitS7RhCVGlS4AshysXV80ncuJSsdoxSo1gUjm44ycFvjmDONdu0TNu6rnw8shkta7sUa1s9W9fg3aca4ayXj+yypnH2xrH7v9E1G6Z2lArNHP2T2hGEqNLk20IIUS4qS+/930UdiOaXJXvITMmyqb23u56PnmlC7zY1bGr/SPNqvNGvvsyUU440Wgf0Hf6F3m826Ip3MCbymON3oeRmqB1DiCpLCnwhRJlLTUoj8VyS2jHKzI1LyeyYv5PrNp6hcHTQ8saTDfjnY3UorG73a+zO1AENpLhXia5BHxwfX4HGraHaUSoecxbmy2FqpxCiypICXwhR5iIPXFI7QpnLSsnil8W7i7WvAb7ezA1sTDVn3T3PPdjAjRmDG6LXyce0mrTVm+LY8ws03g+qHaXCMUeHqB1BiCpLvjmEEGXKYrFw6VCM2jHKhcVk4dA3Rzmy4QQWs8WmZTo2dGfps81o4v1/014283Hm3aca4uggH9H2QKN3w7HbQrQ+ndSOUqEo6bGYr59QO4YQVZJ8ewghylTC6USybmWrHaNc/bX7Irv+vY/sNNtunlSnhiOLRjTj0ebV8XR1YJZ/I1wc7+3VF+rROLigf3gBWh8/taNUKOYYudhWCDXIPPhCiDK1d/l+Lv+RoHYMVbh5u9FrQne8Gnra1F5RFG6km6hZTV/GycT9Usw55IbPxHL9qNpRKgatHqf+P6JxrKZ2EiGqFOnBF0KUmazUbOJPXVE7hmoykjMIW/ArMcfibGqv0WikuLdzGp0T+ofnoa31sNpRKgZLLubEfWqnEKLKkQJfCFFmLh2OQSnBnVsrA7PRzO8rD3Fi86kq/1pUFhqdE/qHPkRbs6vaUSoES/yvakcQosqRAl8IUWaiDkSrHcFunAk7x57PfseYaVQ7iigFGp0j+q7vo6nWRO0ods9y4w+U7MpzkzshKgIp8IUQZeJ61A1uJaaqHcOuJEQkEhq0S16XSkKjd0P/cBA42nbTsqrLgjlhj9ohhKhSpMAXQpSJS4dj1I5gl1KT0ggN2lWlr02oTLRu9XDs+gFoZNajwpgTZJiOEOVJCnwhRJmQArZgudm5/LbyINlpVWv60MpK69MRhwdeVTuGXVNunsWSkah2DCGqDCnwhRClznD5Jpk3s9SOYdcefs4P52rOascQpcShxbNo6/ZUO4Zds1z9Xe0IQlQZUuALIUqd9N4XrkWPZjR/tKnaMUQp0/tOB5c6asewW5akcLUjCFFlSIEvhCh1CVLgF8jdx40uIzqpHUOUAY3eFX3HN9SOYbcsyadQTHJmT4jyIAW+EKJUZaVmkxxzU+0Ydkmj0dD9pYfRO8vNrCorXe1uaOs9oXYM+2QxYrlxUu0UQlQJUuALIUrVlYhEFEVu6JSftv1aU6tlTbVjiDKm7/Av0LurHcMulXSYTtu2bQkICGDIkCEMHTqUEydO3Nd6wsPDefXVkl8Y3bt3b5577rm7HgsICOCpp54q1nqmT59OWFgYADNnziQyMrJEuYYMGcKbb75ZaJtdu3aVaDvR0dG88sor9OvXj4EDBzJ58mRu3LhRrHWsWbOGgQMH8tZbbxEeHn7ff8/S4Ovra9PjmzdvZu7cuYWu686/p63Ljhw50saktpECXwhRqmT8ff5q1K1Op4D2ascQ5UDj7C2z6hTAcq1kBb6zszMhISFs2bKFN998k6VLl5ZSsvuXkZFBYmLeDEFRUVElXt+8efNo0aLFfS8fFRWFoigcPXqUzMzMfNuYTKYSFfg5OTm8+uqrjBo1ip07dxIaGsqoUaMwGAz3bKcw3333HStXrmTJkiUcOXKEkyer7hmeDRs2lOr6HEp1bUKIKs1ispB49qraMexS15Gd0ellrvSqQtfYH/PlX1AMp9WOYleUzEQs6ZfRujcs8brS09OpXr163noVhYULF/L777+j0WgYP348gwYNKvDxO506dYrZs2cTHBxMYmIi8+bNA/KG1K1btw5398LPxgwcOJAdO3YwduxYtm3bxuDBg9myZQsAZrOZxYsXc+TIEYxGI//4xz8YOXIkiqLwwQcfcPjwYRo0aHDXWc/Ro0czbdo0OnTowJw5czh9+jQ5OTn079+fSZMmFfm6bN26lSFDhnDp0iV2795tPZswevRofH19OXHiBN27d2f37t0cOXKEFStWEBwczN69e9mwYQM6nY4WLVrw8ccfF7qNTp060bt3b+tj3bp1A/J6qffu3YvRaCQzM5MVK1YwYcIEUlNTMZlMTJ48mb59+zJ79mzi4+OZMGECzzzzDBs2bECr1bJlyxZmzZrF9evXWbZsGVqtlmrVqvHtt98Wut8TJkzg6tWr5OTk8MILL/Dss88CeT3wL7zwAnv27MHZ2Znly5fj4+PD5cuXmTJlCiaTiccee6zI1zU/CQkJzJgxA4PBgJeXF0FBQdSrVw+AgwcPsmbNGpKTk5k+fTpPPJE3dC8xMZGxY8cSHx+Pv78/EydOtOa8fYDz5ZdfEhoaitFopF+/fkyaNInMzExef/11rl69isViYcKECfe8l+8kBb4QotQkXbhGbnbhPTZVUYOO9anbtrbaMUQ50mg06DtNwbjnZVDMasexK5Zrx+67wM/OziYgIICcnByuX7/ON998A8Avv/zC+fPnCQkJ4ebNmwwbNowuXbpw8uTJfB+/7cSJE3z44YcsX76cevXqMW/ePGbPno2fnx8ZGRk4OTkVmal///688847jB07lj179rB48WJrgb9x40aqVavGpk2bMBqNjBw5ku7du3Pu3Dmio6PZunUrN27cYPDgwTzzzDP3rPuNN97Aw8MDs9nMiy++yPnz52nTpk2heUJDQ/nqq6+Ijo5m3bp1dw0XSk1NZd26dQDExsbSq1cvBgwYAMDKlSvZvXs3jo6OpKYWfrftixcv0q5duwKf/+OPP9iyZQseHh6YTCaWLVuGu7s7BoOBZ599lj59+jB37lz279/PN998g5eXF2lpabi6ujJ27FgA/P39WbVqFbVr1y4yD8D8+fPx8PAgOzubYcOG8eSTT+Lp6UlmZiYdO3bkjTfeYOHChfzwww9MmDCBefPmMWrUKAIDAws9eLj9nrvt1q1b1gObDz74gMDAQIYOHcrGjRut7yXIK/7XrVtHXFwcL7zwAo8++igAp0+fZuvWrbi4uDBs2DB69uxJhw4drOvfv38/sbGxbNy4EUVRGD9+PEePHsVgMFCrVi1WrlwJQFpaWqGvhwzREUKUmqt/XVM7gt3ROmjpMlxmzamKtNWaoK3fR+0YdsdiOHXfy94eohMWFsaXX37J22+/jaIoHD9+nMGDB6PT6fDx8aFr166cPn26wMchbyjL7NmzWbFihbXXtXPnznz00UesWbOGtLQ0HByK7getUaMG1atXZ/v27TRv3hxn5/+7v8WBAwcICQkhICCA4cOHk5KSQmxsLEePHrXmql27trX3++9CQ0MZOnQogYGBXLx4scghQKdOncLT05P69evzyCOPcPbsWW7dumV9vrAe39atWzNlyhRCQkLQ6Up2trF79+54eHgAeWdXli5dir+/Py+99BJJSUk2jdX39fVl+vTp/PDDD5jNRR8kr127liFDhjBixAgSExOJjY0FQK/XW3vP27dvT0JCAgAnT55k8ODBAHcV8H93+z13+9+dZ1FOnjxpPYAKCAjg+PHj1ucGDhyIVqulSZMmNGzYkEuXLgHw6KOP4unpibOzM/369btrGch7zxw4cMB64HDp0iViYmJo1aoVBw8eZNGiRRw7doxq1aoV+npID74QotRcv5SsdgS707ZPK6rVkgsuqyqHVs9jjN8FWNSOYjcsyaUzbMnX15ebN29iMBgKvLC/sAv+a9asSU5ODufOnaN27bwzbOPGjaNnz57s27ePESNGsHr1apo3b15klkGDBjF37lyCgoLu2f677757zxCQffv2odFoCl3n5cuX+eqrr9i4cSM1atRg+vTp5OTkFLrM9u3biY6OtvYwp6en88svvzB8+HAAXFxcClx25cqVHD16lN27d7N8+XK2b99e4AFOixYtOHr0aIHrunM7W7duxWAwsHnzZvR6Pb179y5yPwDmzp3Ln3/+yd69ewkMDOSnn37C09Mz37bh4eEcPHiQ77//HhcXF0aPHm3dhl6vt77WWq32roOFov4GxXXn+v6+7tu/F/T4bYqiMG7cuHwvut28eTP79u1jyZIldO/e3Tq8Jz/Sgy+EKBWKRcEQayi6YRWid9bTfmBbtWMIFWmrNUZb7/7G91Za2ddRMpNKvJqoqCjMZjMeHh507dqV0NBQzGYzBoOBY8eO8eCDDxb4OED16tVZuXIlS5cuJTw87+LfuLg4Wrduzbhx42jfvj3R0dEA1mEsBenbty9jx46lR48edz3eo0cP1q9fT25uLpA380xmZiZdu3Zlx44dmM1mrl27Zt3+nTIyMnBxcaFatWrcuHGD3377zfrckiVL2Llz513tLRYLYWFhbNmyhd27d1sL9W3btuWb2c3NjYyMDOuyiYmJdOvWjalTp5KWlkZmZianTp1i2rRp9yzr7+/PyZMn2bt3r/Wx3377jb/++uuetmlpaXh7e6PX6zl8+LC1B72wPJD3t+jYsSOTJ0/G09OTq1evkpSUxJgxY/LdRo0aNXBxcSEqKoo//vgj323cydfXl+3btwNYh1QV153r2Lp1K35+ftbnwsLCsFgsxMXFcfnyZZo2zbu54YEDB0hJSSE7O5tdu3bRuXPnu9bZo0cPNm3aZH0tkpKSSE5OJikpCRcXFwICAhg7dixnz54tNJv04AshSsWtq6ky/v5vWvVqjqOro9oxhMocWo3GeGWf2jHsiuXmGXSuxb8u5c7x0IqisGDBAnQ6Hf369ePkyZMEBASg0WiYOnUqNWvWLPDx28MlfHx8+Pzzz3nllVeYP38+W7ZsITw8HK1WS4sWLXj88ccLPUNwm7u7O+PGjbvn8eHDh5OQkMDTTz+Noih4enqyfPly+vXrx+HDh/H396dJkyZ07dr1nmXbtGnDAw88wODBg2nYsOFdheCFCxfuusAV4OjRo9SuXdt6NgKga9euTJkyhWvX7h0+OWjQIGbNmsXatWtZunQpM2fOJD09HUVRePHFF6levTpXrly5a8jRbc7Oznz++efMnz+f+fPn4+DgQOvWrZk5c+Y9bf39/Rk/fjxPP/00bdu2pVmzZvm+hk888QSTJk3i119/ZdasWXz99dfExsaiKArdunWjTZs2RERE5HtW4fHHH2fDhg34+/vTtGlTOnUqeljkzJkzmTJlCmvWrKF///5Fts/Pu+++y4wZM1i1apX1ItvbmjZtyvPPP09ycjLvv/++9XoOPz8/pk2bRmxsLP7+/neNv4e8Aj8qKsrag+/q6sqiRYuIjY1l4cKFaLVaHBwceO+99wrNplFkwmohRCmI3H+JQ2sKPmVb1ej0OoYGPYVL9Xu/HEXVYzw8HUvSIbVj2A1d8+Ho2xc8vMCe7Nmzh8uXL/PCCy+oHcVq7NixrFq1qsy3s2DBAgICAoq8sLe8rFu3jrp169Knj1zbUhQp8IUQpeLw2qNc/P2S2jHsRuteLXjoOb+iG4oqwWI4g/H3CWrHsBsarw44PfaZ2jGEqLRkDL4QolTciJbx97dptBoe6G8fPV7CPmi92qHx6lB0wypCuXUBRaYPFaLMSIEvhCix3BwTKVduFd2wiqj3QB3cvd3UjiHsjK5BP7Uj2A9zDkqG3PVaiLIiBb4QosQMsQYUi4z2u61596ZqRxB2SFevJ2jkbsa3KWkxakcQotKSAl8IUWIyPOf/OFVzomHH+mrHEHZI4+SBtmaXohtWEUparNoRhKi0pMAXQpRYSkKK2hHsRrOHG6N1kI9WkT9dA5n94zaL9OALUWbkW0gIUWJp19PVjmA3mj3SRO0Iwo5p6zwGOie1Y9gF6cEXouxIgS+EKLH06xlFN6oCXD1d8WqY/63UhQDQ6F3R1u6mdgy7oKTHFXkDKSHE/ZECXwhRIiajiazUbLVj2IUGD9ZVO4KoAHT1eqkdwT6Ys1Eyr6qdQohKSQp8IUSJpN+Q3vvbGjxYT+0IogLQ+viqHcFuKJmJakcQolKSAl8IUSJp12T8PYDOUUedNrXVjiEqAI2TJxr3RmrHsA/Z19VOIESlJAW+EKJE0m9IgQ9Qu2VNdHqZ41zYRuvdUe0IdkHJuqF2BCEqJSnwhRAlkiZDdADwaeqtdgRRgWi92qsdwS4o0oMvRJmQAl8IUSLpMkUmAD5NvdSOICoQjUdrtSPYBSU7We0IQlRKUuALIUpELrLN491EevCF7TTVGoPORe0YqlOyZYiOEGWhyAK/bdu2BAQEWP/Fx8cXeyO+vvnPGLB+/Xp++umnYq+vKLczP/XUU0yaNImsrKxS30ZJFPR6AOzcuZPWrVsTFRVVjonuz6+//srKlStLdZ29e/fGYDBYfw8PD+fVV1+9r3XFx8fz1FNPAXDu3Dn27dtnfS44OJhVq1aVLGwxjR49mtOnT+f7eP/+/QkICGDgwIF8//33pb7tv+9/acq8mVkm661I3H3ccK4mNy8SttNotGhqtFA7huqULBmiI0RZcCiqgbOzMyEhIWWy8VGjRpXJeu/M/NZbb7FhwwZeeuklm5Y1m83odOpdKLdt2zb8/PzYsWMH//rXv1TLURSTyUSfPn3o06di3Hb93LlzRERE0LNnT7Wj5Gvx4sV06NCBlJQU+vXrx9ChQ3F0dCy19ZfV/lvMFnKzTaW6zopIbm4l7ofGrR6K4d6D/iolx1B0GyFEsRVZ4OcnPj6eadOmWXvGZ82aRefOnbl27RpvvPEG6enpmM1m3nvvPbp06QLAxx9/zJ49e3B2dmb58uX4+PgQHByMq6srY8eO5dy5c8yZM4esrCwaNWrE/PnzqVGjBqNHj+bBBx8kPDyctLQ05s2bZ12nLbp06cJff/0FQEhICGvXriU3N5eOHTsyZ84cdDodvr6+vPjii+zfv5+3336bvXv3snv3bnQ6HT169ODtt98mISGBGTNmYDAY8PLyIigoiHr16jF9+nTc3d2JiIjg+vXrTJ06lQEDBpCRkcGECRNITU3FZDIxefJk+vbtW2jWjIwMTpw4wZo1axg/fvxdBf5//vMftmzZgkaj4fHHH2fKlCnExsYyZ84cDAYDOp2OTz75hEaNGvHll18SGhqK0WikX79+TJo0iczMTF5//XWuXr2KxWJhwoQJDBo0iMWLFxdrX2vUqMHZs2dp164drVq1IiIigtmzZ2MwGJgzZw5XrlwBYMaMGfj5+XHkyBHmzZsHgEajYd26dbi7u9v+ZrtDZmYmH3zwARcuXMBsNjNx4kT69u1b4PvxNqPRyKeffkp2djbHjx+3nhGIjIxk9OjRXLlyhTFjxvDCCy8Uuv3PPvuMPXv2kJOTg6+vL3PnzkWj0RT4Hs3Ozuadd94hMjKS5s2bk51d9M2gMjMzcXFxsR5k7t+/n+DgYIxGIw0bNiQoKAg3N7dCs0ybNo0OHTpgMBgYNmwYYWFh9+z/v//9bzZs2ICXlxcWi4X+/fvz/fff4+VVvHHkxkxjsdpXVu417+89Lao2jbOP2hHUp5hRzDlodHIGTIjSVGSBn52dTUBAAAANGjRg2bJleHt7s3r1apycnIiJieHNN99k8+bNbNu2jR49ejB+/HjMZrO14MrMzKRjx4688cYbLFy4kB9++IEJEybctZ1p06Yxa9YsHnroIT755BM+++wzZs6cCeT1qm/cuJF9+/bx2Wef8fXXX9u0cyaTid9++43HHnuMqKgoQkNDWb9+PXq9nvfee4+tW7cSGBhIZmYmLVu2ZPLkyaSkpDBz5kzCwsLQaDSkpqYC8MEHHxAYGMjQoUPZuHEjH374IcuXLwfg2rVrfPfdd1y6dInx48czYMAAnJycWLZsGe7u7hgMBp599ln69OmDRqMpMO+uXbt47LHHaNq0KR4eHpw5c4Z27dqxb98+fv31V3744QdcXFxISUkBYMqUKYwbN45+/fqRk5ODxWJh//79xMbGsnHjRhRFYfz48Rw9ehSDwUCtWrWsQ2rS0tJISUlh586dxdrXmJgYvv76a3Q6HZs3b7ZmnzdvHmPGjKFLly5cuXKFsWPHEhoayldffcXs2bPx8/MjIyMDJ6eiP8THjBmDVps3eiwzM5NmzZoB8Pnnn9OtWzeCgoJITU1l+PDhPProowW+H29zdHRk0qRJ1oMRyBuiEx0dzZo1a0hPT2fgwIGMGjUKvV5fYK7nn3+eiRMnAjB16lT27NlD7969gfzfo+vXr8fZ2ZmtW7dy/vx5nn766QLXPWXKFBwdHYmNjWXGjBnodDoMBgMrVqxg9erVuLq6snLlSlavXs3EiRMLzfJ3+e3/pUuX2LJlCy+++CIHDx6kTZs2xS7uAXIypMAHqFbTTe0IogKSAv9/TFkgBb4Qpeq+huiYTCbmzp3L+fPn0Wq1xMTEANChQwdmzJiByWSib9++tG3bFgC9Xs8TTzwBQPv27Tlw4MBd60tLSyMtLY2HHnoIgKFDhzJ58mTr8/369QOgXbt2JCQkFLlTdx6UdOnShWHDhvHDDz8QERHBsGHDrG28vfMuitPpdPTv3x8Ad3d3nJycmDlzJr169aJXr14AnDx5kuDgYAACAgJYtGiRdXt9+/ZFq9XSokULbtzIu2BIURSWLl3K0aNH0Wq1JCUlcePGDWrWrFlg7u3btzNmzBgABg0axLZt22jXrh2HDh3i6aefxsUl74IsDw8P0tPTSUpKsr42twvnAwcOcODAAQIDA4G8AjkmJoYuXbqwYMECFi1axBNPPEGXLl0wmUzF3tcBAwbkO4Tp4MGDREZGWn9PT08nPT2dzp0789FHH+Hv78+TTz6Jm1vRhdA333xjLTbDw8P56quvgLze7N27d1t/z8nJITExkVq1auX7fixKz549cXR0xMvLCy8vL5KTk6lTp06B7cPDw/nyyy/Jzs4mJSWFli1bWovq/N6jR48eZfTo0QC0adOG1q0LnjXj9hAdg8HAyJEjeeyxx7hw4QKRkZHWoWy5ubl06tSpyCy2eOaZZ5gwYQIvvvgimzZtKvTgozDSg59HevDF/ZACP49iykLj5KF2DCEqlfsaovP111/j4+NDSEgIFouFBx98EICuXbuybt069u3bx7Rp0xg7diyBgYHo9Xprz7VWq8VsNhdre7fHItu6bH4HJYqiMHToUN5666172js5OVmLVgcHBzZu3MihQ4fYvn0769atY82aNfcsc2dPfH5jpbdu3YrBYGDz5s3o9Xp69+5NTk5OgZlv3rzJ4cOHuXjxIhqNBrPZjEajYdq0aSiKUmjP/9/3c9y4cYwcOfKe5zZv3sy+fftYsmQJ3bt3Z+LEicXe19sHGX9nsVj4/vvvcXZ2vuvxcePG0bNnT/bt28eIESNYvXo1zZs3t2lf8vPpp59ae/RvCw4Ozvf9WJQ7/246nQ6TqeCx5Dk5Obz//vts2rSJunXrEhwcfNffs6D3qK1/t9u8vLx44IEH+PPPP3F2dqZ79+4sXbrU5iw6nQ5FUYC8oUkFqVu3Lt7e3hw6dIg///yTxYsXFyvnbblZufe1XGVTzUcKfFF8GpeCO3yqFJNcqC9EabuvaTLT0tKoWbMmWq2WkJAQa0GTkJCAt7c3I0aM4JlnnuHMmTM2ra9atWpUr16dY8eOAXlj5bt27VroMklJSdbebls88sgj/PzzzyQn5825m5KSku/ZgIyMDNLS0ujZsyczZszg/PnzQN7MN9u3bwfyinc/P79Ct5eWloa3tzd6vZ7Dhw8Xeebh559/JjAwkD179rB792727dtHgwYNOH78ON27d2fTpk3WIU8pKSm4u7tTp04ddu3aBeQVc1lZWfTo0YNNmzaRkZFhfZ2Sk5NJSkrCxcWFgIAAxo4dy9mzZ0ttXwF69OjBunXrrL+fO3cOgLi4OFq3bs24ceNo37490dHRQN6ZgOK6vY3bBezZs2eBgt+Pd3Jzc7O+JkUZM2YMSUlJdz12u4D29PQkIyODn3/+ucj1dO3ala1btwJw4cIF67UghcnKyuLcuXM0atSITp06ceLECWJjY63PRUdHF5qlfv36REREABAWFmZ9PL/9Hz58OFOnTmXgwIH3fWG5XGCbx9VLpjsUxSc9+P9jtq+Z7oSoDO6rB/+5557jX//6F2FhYTz88MO4uroCcOTIEVatWoWDgwOurq4sWLDA5nUuWLDAepHt7YsJC3Pt2jUcHGyP36JFC15//XVefvllLBYLer2e2bNnU79+/bva3b449nYR9c477wDw7rvvMmPGDFatWmW98LQw/v7+jB8/nqeffpq2bdve0+v8d9u3b+eVV1654BhbAgAAIABJREFU67Enn3ySrVu38v7773P+/HmeeeYZ9Ho9PXv25M0332ThwoXMnj2bTz75BL1ezyeffEKPHj2Iioqy9uC7urqyaNEiYmNjWbhwIVqtFgcHB957771S21eAmTNnMnfuXPz9/TGbzXTp0oW5c+fyzTffEB4ebh3C9Pjjj2MwGKxFenFMmDCB+fPnM2TIEBRFoX79+nzxxRcFvh/v9PDDD7Ny5UoCAgIKnXbTYrEQFxdHjRo17nq8evXqDB8+HH9/f+rXr0+HDh2KzDtq1Cjeeecd/P39adu2baFnFqZMmYKzszNGo5GhQ4fSvn3eXS6DgoJ48803rb3xr7/+Ok2bNi0wy8svv8zrr7/Oli1bePjhhwvc/0GDBtG7d2/eeeed+x6eA2A2Fu9sXGXk4OSAzkG9mbdEBebsBRodKFX7/yPFJAW+EKVNo9xPpWUH1q1bR926dSvMNI3i/+zZs4fLly8XOWuNGi5cuMCmTZusBzuV2enTpwkKCuK7776773Vc2BdJ+LfHSzFVxePq6cozC/zVjiEqqOwdT0FumtoxVKXv+gG6eo+rHUOISuW+evDtwfPPP692BHGfbl9wbY9atWpVJYr7lStXsn79+rsuoL4fJunBR+9cYT9GhT2omH1spauKn8EQoizIN5MQVdC4ceMYN25ciddjzpUvZgdHGZ4jSkCK27xhSkKIUnVfF9kKIQSAVlu8WYIqI51eihNREtKDj0ZKESFKm/xfJYS4b1opbrGYLWpHEBWZIu8fijmdsBCiaFLgCyHum1YnHyFmkxRoogSkwJchOkKUAfl2FkLcN62DfIRYpMAXJSJDdPj/7d13WFRn2gbw+0yjV6kCioKCDURF7BrsBRXURJNYPk2MSUyMrrpYYo8mauImljRLYkzZRCyxJjG26Bo09l6IBRBEwUJnyvn+YJ0VZXSAGc4w3L/ryrULzDnnnhHOPPOe530POIJPZGp8dyaicpOzwGeBT+UmiiJH8AH24BOZAf+qiKjcOILPlYSoAgoywRF8QJAppY5AZHX47kxE5cYefKAgu7Bcd2YmEvPTpY5gGZSOUicgsjp8dyaicpMrODlOp9WhILtQ6hhUBYl5aVJHsAiC0knqCERWhwU+EZUbW3SK5d/LlzoCVUFiHkfwAQAqFvhEpsZ3ZyIqN7mSpxAAyGOBT+XAAh+AIIegsJc6BZHV4bszEZWbrZOt1BEsQt7dPKkjUBXEAh8A23OIzIIFPhGVm52rndQRLMK9tAdSR6AqiAU++++JzIUFPhGVm9JGAaUtl7i7m3xP6ghUxYjaQhb4APvvicyEBT4RVYi9G0fx76WywKeyEe9eAESN1DEkJ9h6Sh2ByCqxwCeiCrFzYYFflKdGTmau1DGoCtHdPSt1BIsg2PtIHYHIKrHAJ6IK4Qh+sawbd6WOQFWILosFPsACn8hcWOATUYVwBL/YrUsZUkegKkIUddBlnpQ6hkVggU9kHizwiahC7LmSDgAg7fwtqSNQFSHeuwios6WOYRFY4BOZBwt8IqoQFvjF7t98gPz7vOEVPZvu9jGpI1gMFvhE5sECn4gqhGvh/0/aBY7i07NpMxKljmAZVC68iy2RmbDAJ6IKcfHhOtYPpZ1jgU9PJ+alQ8w8JXUMiyA4+EkdgchqscAnogpR2avgUIOjcACQfCIVWrVW6hhkwbTJvwIQpY5hEWQuwVJHILJaLPCJqMLc/F2ljmAR1PlqpJ5OkzoGWTBt8i9SR7AYgnOQ1BGIrBYLfCKqMBb4/3P18HWpI5CF0mWdgZibInUMi8ERfCLzYYFPRBXGAv9/Uk+nQV2gljoGWSDtjZ1SR7AgAgTnulKHILJaLPCJqMJY4P+PVq3FtSM3pI5BFkbUFkJ7c4/UMSyG4FCTK+gQmRELfCKqMCdPRyhsFFLHsBgXfr8sdQSyMLqb+wB1jtQxLAb774nMiwU+EVWYIBPg6ucidQyLce/mfaSdS5c6BlkIUdRCc+kbqWNYFPbfE5kXC3wiMgm26ZR0ftclqSOQhdCl7IKYw7atRwnujaWOQGTVWOATkUmwwC8p9Wwa7qc/kDoGSUwUtdBcXCt1DMsiyCFzayh1CiKrxgKfiEzCK9hD6giWRQRObzsndQqSmDb5Vy6N+RjBORiCwk7qGERWjQU+EZmEq58LbJ1tpY5hUa4dvoGs5LtSxyCJiDoNtBy9f4LMI0zqCERWjwU+EZmEIAjwCfWSOoZFEUURxzeeljoGSUSb/AvEvJtSx7A4Mo8IqSMQWT0W+ERkMr6h3lJHsDg3z6Qh/WKG1DGokonqbGgurJY6huUR5JDVCJc6BZHVY4FPRCbj24AFfmmOJZyEqBOljkGVSHNmOVBwR+oYFkdwqQdB6Sh1DCKrxwKfiEzGoYYDnLz45v24zGtZuLT/itQxqJJobyVCe2OH1DEsksw7SuoIRNUCC3wiMimO4pfu9NZz0Gq0UscgMxPVuVCfXCx1DIsl92krdQSiaoEFPhGZlA/78J/gHeKFbpOjIVfIpY5CZqY5+ymQzzkXpbL1hMw1ROoURNWCQuoARGRdfEK9IQgCRJE95yp7JZoNCEe99kFSR6FKoL19FNrrW6SOYbHkPm2kjkBUbbDAJyKTsnFQwb22GzKvZUkdRVK1IvwROaQZ7F15Q5/qQCy6D/XxhVLHsGgytucQVRoW+ERkcrWbB1TbAt/OxRYthzRHrWb+UkehSiKKWqiPzAby06WOYrkU9pB5NpM6BVG1wQKfiEwuMLIWjm04CVSzLp3gdnXQfGBTqOxVUkehSqQ58yl0d45KHcOiyTwjIciUUscgqjZY4BORyTm428Mr2BMZl29LHaVSOHk6ImpoizLd6CtHnQ9HJdt3qjpRp4Hu/mWpY1g8ud9zUkcgqla4ig4RmUWdlrWkjmB2gkxAw26h6DOzu9HFfYG2CIvP/IjOv0xCWl6mmROSuQkyBVRtPoS8Vi+po1gupSP774kqGQt8IjKL2s0DIJNb7ynGzd8VPad0QfOB4VCojLsYmnj7Avr9/i5WXt6B++pcTDu+xswpqTIIMgWUEf+EotHr4Nvqk+R+0RDkbFsjqkxs0SEis7BxtIFvA2+knkmTOopJyZVyNOnTEI26hRr9ASZbnYeFZ37E+mv7IT4yMeE/GWfxw9U9GFyH7QvWQBE8GIJjANR/zQW0+VLHsRjygO5SRyCqdjjUQERmExhlXW06XvU80WdGdzTp2dDo4n7XzWPovWsafrq2r0Rx/9DC0//GxfvJpo5KEpH7tIWq/XLAjjd8AwDBwR8y98ZSxyCqdljgE5HZBIT7Qa6q+ndvVdoqEfVSc3Sb+BycvZ2M2uZOwX2MS1yOsYlLkVFwz+Dj8rSFeP3Qx7hTcN9UcUliMpcg2HT4DIJbI6mjSI6j90TSYIFPRGajtFXCP6ym1DEqxD+8JvrO7oH6HYMhCIJR2yRc/wO9d03DLzf/MurxN/Mz8eafS1GoVVckKlkQwdYdqrb/gsy/i9RRJCRAHtBN6hBE1RILfCIyq6A2daSOUC62TjZoP7o1nnuzPezd7I3aJjk3A/93YBGmHVuN++rcMh3v5N0kTDu2ujxRqRKJOh1yL18z6rGCXAVV83ehCB0FwLgPh9ZE5hUJwd5H6hhE1RIn2RKRWdVs5ANnHyc8SM+WOorR6rYORIvnm8LGwcaox2tFHb6+8iuWnt+IfG1RuY+7NeVP1HXyxRuhfcu9DzKv5C//jYzNu1BrzIvw6tvZqG0UIcOKJ98eXwBoC82c0HLIgwZJHYGo2hJEUaxm95okosp2ad8VJH5r+Xf6dPRwQNTLLVCzofGjjhfvJ2P68TU4ffeqSTIIELCk5evo4Rdpkv2R6dza9BuSP/tO/7Vnn2jUev1FCHLj5pno7l5A0eFpQMEdc0W0GIJjbdh0Xit1DKJqiwU+EZmdplCDhPgtKMot/+i2OQmCgNDO9RDerwmUNsZd2CzSqrH8ws9YfXkH1KLWpHls5SqsbPMPtPCob9L9Uvll7voPrn60EtCVfMt0btYIdae9AYWDcW1cYv5tFCVOhXj/kjliWgxF2AQo6vSTOgZRtcUCn4gqxfGNp3Bmx3mpYzzB1c8FrYdFwqNODaO3OXrnEqYfX4OrOelmy2UrV+Hjlm+go0+42Y5Bxrm9fS+uL10LGHi7tA3wRfDsd2Bb08uo/YmaAqiPvQdd2n5TxrQcSmfYdPsJgsJW6iRE1RYLfCKqFHn38rFxylbotDqpowAAZAoZmvRqiMY9GkCmMG69gRx1Pj48+xN+uLq31DXtTU0pyDG/+SjEBLQ2+7GodLc2/orkz79/5uMUzo4Imv4mnMJCjdqvKIrQnF8J7eV1FY1oceTBL0LZ6DWpYxBVayzwiajSHFj1J64mXpc6BjyDPNB6WCRcfJ2N3mZP2gnMPrkW6fl3zZjsSQIETA9/CS/VNW5CJ5nOze+34ObXG4x+vKCQo/Zbw+DRvYPR22iTf4X6xCJAZ5nta2UmyGHT9QcIdsZdzSAi82CBT0SVJvN6Fra/95tkx1faKhARG4b6nYxf0z6r8AHmnfoO21MSzZzu6d5q0B9vhrKnubKkfJWA9B+2lmtb7wE94D9qEASZcVeGdFlnUHR4OlBYuR8ezUFeqyeUEfFSxyCq9ljgE1Gl+mXh78i4UvmriPg19kXUyy3g4G7cZEgA2HTjIN4//QPuFeWYMZnxhgZ1wdQmLxr94YTKTldYhOtLv0bmrv9UaD8uUU1RN/41yO2M60MX89JR9Gc8xGzTrMYkCUEBVed1kDn4Sp2EqNpjgU9ElerG8RTs+/RgpR3PxtEGkS9EoE5UbaO3Sc27g5nHv8aBjDNmTFY+Hb3DML/5KNSwMb69iIxTkJqOpLnLkX8txST7s6sTgODZ42DjZdwEblGdB/XROdDdOmSS41c2ee3eUDadLHUMIgILfCKSwPb5vyHzWpbZj1MnqjZaPB8BWyfjblilE3VYl7QL/zq3AXkWfEMiDxtnzG82Ch18wqSOYjXuHjyKax+ugjYv36T7Vbg5I3jG23BsEGTU40VRB83ZFdAm/WTSHGYnKGDT5VveuZbIQrDAJ6JKl37hFn77aK/Z9u/gbo+ol1vAr7HxrQKXH6Ri+rE1OHk3yWy5TEmAgKFBXTCx0SCo5Eqp41RZolaLlNU/4VbCL2Y7hqBUIHDCSNR4zvjVkDTXtkBz6l+AqDFbLlOSB/aFMvwfUscgov9igU9Ekvj94324eda068gLgoD6zwUjon8TKG2NK3qLdBp8fnErvri0DWpd1SimHhXiHIDFka+hnrOf1FGqnPzkNFz/1xrknL1cKcfzHRKDmsNijZ5Dob19DOojMwH1AzMnqyCZqnj0nivnEFkMFvhEJIms5LvYNu9XmGo5eRdfZ7QeFgnPIA+jtzmeeQXvHl+DK9k3TRNCIjYyJSY3eQEv1onmBFwj6AqLcPP7LbiVsBOiunI/1Lm1j0Sdia9AZqMy6vG6nBSoE6dAzLlh5mTlJ68TB2XYOKljENEjWOATkWT+WHkI1w5XrHCRKWRo3KMBGvdqALlCbtQ2uZoCLDmbgO/+/h26SrhhVWVp4lYHU5oMQbMa9aSOYrHuJZ7EjU+/RVH6bcky2NcLRPCst6Gq4WbU40V1NtRHZkJ3+6iZk5WD0rl49F7FSd9EloQFPhFJJvtODn6esQM6TfnubutRxx2th7eEa00Xo7f549ZpzDz+NW7mZ5brmFVBD79ITGw0CP4OnlJHsRhFtzNx49PvcO8/x6SOAgBQergheObbcKgXaNTjRZ0GmtOfQHtts3mDlZGiyTgo6sZJHYOIHsMCn4gkdfiHY7i4u2w90AobBZr2b4LQ5+pBkBnXknK3MBvzT3+HLcl/lidmlaOUKTCwdnuMCYmBt51xI8XWSJ11H+k/bcft7XuhK7Ssu8XKbFSoM+lVuLVrYfQ2mr8ToDmzHBC1ZkxmHMGpDlTPrYIgGHfljIgqDwt8IpJUQXYBNk3bBnWBcb3QNRv5IOrlFnCs4WD0MbYkH8KCU98jqyi7vDGrLBuZEi/U6YRR9XpWq0K/MCMTtxJ24vaOfRCL1FLHMUwQ4Dc8Dr6D+xi9ifZWItR/zQY0uWYM9mzKNksg92wmaQYiKh0LfCKS3KmtZ3Hy56ffVMrGQYUWz0egbutAo/eblpeJWSe+wb5bJyuYsOqTCzK09WqEAbU7INq3KZQyhdSRzCL38jXcWr8Tdw/8BVEr/Si3sWp0boPa40ZApjJu9Sfdg2vFk2/zpJkgLvNtD1XLeZIcm4iejQU+EUlOq9Ziy+ydyM7IKfXngZG10OKFCNg52xq1P1EU8d3V3fjo7HrkagpMGdUquKucEBPQGgMDO1jF8ppFt7OQtf8wsvYmIu/yNanjlJtjw3oImjEWSlfjJqyKhfdQdORdiJmnzJzsMTIVVNFrIXMw/j4TRFS5WOATkUVIu3ALux67+ZW9mx2iXmoB/7CaRu/n7+w0TD+2BseyKmdt86ou3K0u4mq3Ryef8CrVwqO+n427fxxB1t7E4nXsreStTOXtgXqzx8Eu0N+ox4s6NTQnFkObvNPMyf5HEfoKFCFDK+14RFR2LPCJyGIcXP0n/v7zOiAA9TsEISIuHCo741oW1DoNVl7ajk8vbkFRFbxhlSWo5eCFSI8Q/X9+9sbfU8DcdEVq5F66ipxzl5F94jyyT16oUi04ZSGzt0Xd+DFwbRlu9Daay99Bc+5LAOVbkcpYgkt9qDp8CsFKW7yIrAULfCKyGAXZhdi74gCaxYXBq57xSzyeyvob04+vwaUHKWZMV/3UtK+Blh6hiKxRH3WcfOFr5w4vOzfIBZnZj12UeRe555OQc/4Kcs5eQV7S9Uq/KZWkZDL4v/I8fOK6G72JNu0A1EfnAdp882QSFFB1/AIylyDz7J+ITIYFPhFVWfmaQvzr3AZ8k/SbVd2wypIpBDm8bF3ha+8OX7sa8LV3h5+9BxwVdpALMgiCALkgw3M+TaGQlVw+UdTpUHAjDTq1GqJaA11hEYpuZ6HodiaKMjJRmJGJoowsqO9kWdySllLx6NkRtd58GTKFcSPmuvuXUZQ4FcjPMHkWecgIKEP/z+T7JSLTY4FPRFXSwYyzmHn8a6TkSXdHUjIssfcyuKhKLmWqzcvH8bg3JEpUdTmFhyJo+ptQODka9XixIBNFh6dDvHvOZBkE5yCoOn7B1hyiKsL811mJiEzoXlEO4o+uxKiDi1ncWzCdWEovuIxvOeWRffICzr8zDwXJaUY9XrCtAVXbf0HmF22aAIIcyoh4FvdEVQjPtkRUZexIOYw+u6Zh042DUkehZyitZUpggV9uham3cH78PDw4btyovCC3garFTChC/g+AcXd7NkRRfxhkrvUrtA8iqlw82xKRxbuVfxdvHPoY4498ijuFD6SOQ0bQljqCX7FCs7rT5uTh8vSPkLFtj9HbKEJHQNliBiC3KdcxZR7NIA8ZVq5tiUg6vN5GRBZLFEX8+9peLD7zE3I0ZloZhMyitOldHMGvOFGrxY2la1Fw4yYCRg+BIH/2ayr3i4Zg71s8+bYwy/iD2bhD2Xw6hEpYNYmITIt/tURkse6rc/HxuQ0s7qug0kbwWeCbTsbmXbg881/Q5hr3tyFzawCbjp9DcAk28ggyKJtNg2Bbo/whiUgyPNsSkcVyVTlievjLUsegctAZWqCNbTom8+Cv0zg/fh4K042bbC7YeUHVbhlkPu2e+Vh5/Zcg92pR0YhEJBEW+ERk0Xr7R6GLbzOpY1AZlbqKDsB2DxMruHET58fNQfaZS0Y9XlDYQdlyHuTBLxp+TI1wKLjePVGVxjMtEVm8mU2HwVVl3BrgZBlKnWQLcATfDDT3c3BpyiLc+e2AUY8XBAHKRq9BGTEFkClL/tCmBlTNZ0AQ5KVvTERVAgt8IrJ4nrYuWNBslNQxqAxEA3cWZh++eYhqDa59uAopq38qdYJzaeS1ekDV5iNA5VL8DZkKqpbzINh5mDEpEVUGnmmJqEp4zrcphgd1kzoGGcnwCD7fdswp/cftSJq7DNqCQqMeL6sRBlWHzyE41YGy6UTI3BuaOSERVQaeaYmoyvhH40Fo5BoodQwyguEefLbomNu9/xzDhX/MR9Ft45bElDn4QtXpS8gDups5GRFVFhb4RFRlqGQKLIl8HY4KO6mj0DMYXkWHbzuVIT/pBs6/PQc5F/826vHC4734RFSl8UxLRFVKLUcvzI4YLnUMegYdDIzgc5JtpVHfvY+Lk95H1r5EqaMQUSVjgU9EVU5v/ygMrN1e6hj0FIZadDiCX7nEIjX+XvAZUr/ZJHUUIqpEPNMSUZU0Pfxl9uNbMEMtOuzBl0bat5uRtOBT6IrUUkchokrAAp+IqiRbuQorWr0Nb1s3qaNQKQyuoiPn245U7u47jIuT3oc6677UUYjIzHimJaIqy9vODStavw07uUrqKPQYQ2uxcwRfWrkX/8b5cXOQl3RD6ihEZEYs8ImoSmvkGoj3m78KASwcLYnWwCRb9uBLr+h2Fi7+cyE0uXlSRyEiM+GZloiqvO5+LTCuYZzUMegRBnvwuYqORQgYMwQKB3upYxCRmbDAJyKrMCakD/oFtJE6Bv0XV9GxXL5DYuDRpa3UMYjIjHimJSKrMTdiBFrUCJE6BuFpI/h825FSjS5tUXNYrNQxiMjMeKYlIquhkivxWetxaOJWR+oo1Z7BEXxOspWMa5tmCBw/khOdiaoBFvhEZFUclXZY2eYfCHEOkDpKtaYDR/AtiVNEQ9SNHwOBy5QSVQv8Syciq+OicsDqdhNRx9FH6ijVluEefI4eVzaH0CAEz3wbMpVS6ihEVElY4BORVaph44w17SbBz95D6ijVkqEbXXEEv3LZBfqj3tzxkNvaSB2FiCoRz7REZLV87NzxVbtJ8LJ1lTpKtWNoki178CuPba2aqD9/IhRODlJHIaJKxgKfiKxagIMX1rRlkV/ZDLXosAe8ctgH1ULIongo3V2kjkJEEuCZloisXpBzTXzXYRpqO3hJHaXaMDTJliP45ucQWhf1P/gnlC5OUkchIomwwCeiasHfwQPfdpiKUBeurlMZDI7gswffrBybhBS35TjyLrVE1RnPtERUbXjYuuCb9vFoXqOe1FGsnsEefBb4ZuPcrBHqz5sAub2d1FGISGI80xJRteKktMeqthPR0Ttc6ihWzfAqOmzRMQf3jlEInjUOMhuV1FGIyAKwwCeiasdWrsLyVm+hb0BrqaNYLdHgnWz5tmNqvoP7oE78a1znnoj0FFIHICKSgkImxwfNX0WAgxdWXPgZoqFJoVQuWgMtOhzBNx1BIUftt4fDo1t7qaMQkYXhUAoRVVuCIOCtBv3xccs3YC/njYBMSQdDd7Ll244pyB3tUW/eBBb3RFQqnmmJqNrr5tcC33ecBn97T6mjWA1Dk2y5ik7FqXw8EfrRNDg3bSh1FCKyUDzTEhEBCHEJwE+dZiDKI1TqKFbB0DKZXAe/YlxahqPh0pmwq1VT6ihEZMFY4BMR/ZebjSNWtZ2IF+tGSx2lyuOdbE1MJqDm8DgEzx4HhZOD1GmIyMJxki0R0SMUMjlmhA9FU7cgzD75DXI1BVJHqpIM3cmWLTplp3BxQt1/vgbnZo2kjkJEVQTPtEREpehbqw02Rc9GuFuQ1FGqJEPr4LNFp2wcGgSh4bJZLO6JqExY4BMRGRDg4IVvO0zBGyF9Ief67WUicpJtxchk8B0Sg5BF8VB5ukudhoiqGLboEBE9hUImx9sNY9HWqxEmHf0CN/MypY5UJRgcwec6+M9k4+eDOpNegWMorx4RUflwKIWIyAjNPepjc/Qc9PKPkjpKlWBwki1H8A0TBHjGRKPh8lks7omoQjiCT0RkJCelPT6KHIOefpF479S3SM+/K3Uki2Voki178Eun9HBD4IRRcGGvPRGZAAt8IqIy6lqzOVp7NsQn5zfi279/N9yOUo0Zek04gv8YmQCvPtGoOTwOCgd7qdMQkZVggU9EVA6OSjtMDXsRfQPaYOaJr3H23jWpI1kUQ5Ns2YP/P/b166D2W8PgUC9Q6ihEZGVY4BMRVUBjt0D82OldfJu0Cx+f38h18/+LI/iGyR3t4fd/A+HZsyNfDyIyCxb4REQVJBdkGBbcDd38WuCjs+uxJflPiIZ60KsJjuCXQhBQo3Mb+L/yPJSuzlKnISIrxgKfiMhEfOzcsbDFaAwP7oZFZ37En7fPSx1JMhzBL8m5eWP4jxwE+6BaUkchomqABT4RkYk1cg3EV+0mY1/6SSw5l4AL95OljlTpuIpOMft6gfAfOQjOEQ2ljkJE1QgLfCIiM+noE44O3mHYnnoYn5zbiOu5t6SOVGmq+zr4Nr6e8Bs+AG4dW0KoZh9qiEh6LPCJiMxIEAT09o9C95otsCX5EFZf3onL2alSxzI7XTXtwbep6QWf53ujRpc2kCn4FktE0uDZh4ioEihkcsTWbof+tdpi361TWHlpB/7KvCh1LLOpbiP4doH+8HmhF9w7REGQW+dzJKKqgwU+EVElEgQBnXzC0cknHCezkrDy8g78fvOY4Z71Kqq69OA7hYXCZ1BPuESGSR2FiEiPBT4RkUTC3YOwNGosruWk46srv2JL8iGrWUffmlfRkdnZwr1TFLz6RHNVHCKySCzwiYgkFujog1lNh2Fy4xewI/UwNlz/A0czL0sdq0JEAwU+qnD7il2gPzz7PIca0a0ht7eTOg4RkUFV90xLZEBERESJrzds2IA5c+ZIlAbYvHkzYmJi0Lt3b/Tt2xfTpk3DgwcPKrTPx5+jJTD0Om/YsAGtWrVCv3790K9fP0yePNlkx5w2bRquXLli8OdDhw7F6dOnn/j+6dOnMW/ePJPlMBV7hQ0G1G6PbztMxfYu8/FKvZ7wtHGROla5aA1Msq1qK8rIbG1Qo3MbhH40FY0+mwuvPtEs7onI4nHVbS4LAAAW3UlEQVQEn8iM9u/fj6+//horV66Et7c3tFotNm7ciDt37sDZ2bg7WWo0Giiq+GocvXr1wowZM8q83bOe+3vvvVeuPE2aNEGTJk3KtW1lqevki4mNn8c7DQdg/61T2HjjIA7cOo18bZHU0YxiaJItqkCLjqBUwCUyDO4dW8IlqinktjZSRyIiKpOqXTUQlVFqaiqmTp2KrKwsuLu7Y8GCBahZsybi4+NhY2ODv//+Gzdv3sSCBQuwceNGnDhxAuHh4Xj//fcBAAcOHMDSpUtRVFSEgIAALFiwAA4ODgaP99lnn2Hy5Mnw9vYGAMjlcgwcOFD/82XLlmHPnj0oLCxEREQE5syZA0EQMHToUERERODYsWOIjo5G165dMXHiRGg0GrRv377EMVauXIkdO3agqKgIXbt2xdtvv42UlBS8+uqraN68OY4fPw5vb2+sWLECtra2BrOeOnUK8+fPR0FBAWxtbTF//nzUrVsXGzZswO7du5Gfn4/k5GR06dJFPwqfkJCAL774Ap6enggMDIRKpTL632Lo0KGYPHkymjRpgqysLAwcOBC7d+/Ghg0bsHfvXhQVFSEvLw9vvvkmli1bBjc3N1y6dAmNGjXC4sWL9a/T5MmT0bBhQ0ybNg1nzpyBIAgYMGAARowYAQDYuXMnZs+ejezsbLz33nto0aIFEhMTsXr1anz++edYunQpbt68iZSUFNy8eRPDhw/HsGHDAADLly/Hli1b4OvrCzc3NzRq1AijRo3C2rVr8cMPP0AulyM4OBhLliwx+nmXlUImR7RvBKJ9I1CgLcLBW2fwW9ox7E0/iXtFOWY7bkXpUMV68GUyODdtAPdOUXBt2xwKB3upExERlRsLfLI6BQUF6Nevn/7r+/fvIzo6GgAwd+5c9O/fH7GxsVi/fj3mzZuHFStWAAAePHiAtWvX4vfff8eYMWPw/fffo169ehg4cCDOnz8Pb29vfPrpp1izZg3s7e3xxRdfYM2aNRg7dqzBLFeuXEGjRo0M/vzll1/Wbz9p0iTs2bNHn/XBgwdYt24dAGDMmDEYMmQI+vfvj2+//Va//YEDB3D9+nWsX78eoiji9ddfx5EjR+Dr64vr16/jo48+wrx58zBu3Dj88ssvJV6Xx9WtWxfr1q2DQqHAf/7zHyxZsgRLly4FAJw/fx6bNm2CSqVCjx49MHToUMjlcixduhQbNmyAo6Mjhg0bhoYNS79b5/bt23H06FEAwLBhwzBgwACDOQDgxIkT+Pnnn+Hq6orExEScO3cO27Ztg5eXF4YMGYKjR4+iRYsW+sefP38et27dwtatW/Wv3UNarRbr16/Hvn37sGzZMnz11VdPHO/q1atYu3YtcnJy0LNnTwwZMgQXLlzAr7/+ik2bNkGj0SAuLk7/b/nFF19g9+7dUKlUFW63KgtbuQqdazZD55rNoBV1OHLnInbdPIbdacdxMz+z0nIYw1CLjiWtg69wcYJz88ZwiQyDS/PGUDg7Sh2JiMgkWOCT1bG1tcXmzZv1X2/YsAFnzpwBABw/flxftPbr1w+LFi3SP+65556DIAgICQmBh4cHQkJCAADBwcFITU1Feno6rly5giFDhgAA1Go1mjZtanSuixcvYvLkycjNzcWECRPQq1cvJCYmYuXKlSgoKMC9e/dQr149fYHfq1cv/baP5168eDEA4ODBgzh48CD69+8PAMjLy8O1a9fg6+sLf39/NGjQAADQqFEjpKY+/eZK2dnZ+Oc//4nr169DEASo1Wr9z1q3bg0nJycAQFBQEFJTU3Hv3j20bNkS7u7u+rzXrl0rdd9lbdFp27YtXF1d9V+HhYXBx8cHABAaGorU1NQSBX5AQACSk5Mxd+5cdOzYEe3atdP/rGvXrs98DTp27AiVSgV3d3e4u7sjMzMTR48eRefOnfVXPZ577jn940NCQjBx4kR07twZXbp0Mfp5mZJckKGVZwO08myA6eEv4dKDFBy+fQFH7lzEkTsXkVWULUmuhwxNshUECUfwZQIc6tWBS2QTOEeGwaFeYKVcURg6dChGjx5d4urbV199hWvXrmHWrFmlbpOSkoIxY8boP7Q+6uOPP0ZkZCTatGlT6raPXqEylaFDhyIjIwM2NjZQKpWYN2+e/vxiKr///juSkpIwevRok+63PKKjo7F+/Xr9+e3R7/v4+OC7777Tf69fv37QarXYunUrTp8+jc2bN2P69OkmybF06VLY29tj1KhRiI+PR6dOndCjRw+T7Ntc7ty5g2nTpiEtLQ0ajQZ+fn748ssvTbLvwYMH44cffjDJvqwdC3yq1h6d8PewvUQQhBKtJjKZDBqNBjKZDG3btsVHH31k9P6Dg4Nx9uxZtGrVCiEhIdi8eTPmzJmDgoICFBYWYvbs2UhISICvry+WLl2KwsJC/bZ2diUn8pU2OVEURYwePRqDBw8u8f2UlJQSz0Eul5fYd2k+/vhjREVFYfny5UhJSdG3qQB4Yl9ardZgJmPJ5XKI/x3lLSoq2Vf++HM3dPyHXFxcsHnzZhw4cADfffcdduzYgQULFpTYViaTPbGdof1rNJqnZv/iiy9w5MgR7N69GytWrMC2bdsknydR39kf9Z398XJQ8QeOKw9S9cX+kTsXcbvwfqXmMTyCX3kFvszOFg4hdeHYMBiODYPh0CBIktab3r17Y/v27SUK/O3bt5d7wvm4ceNMFa1MFi9ejCZNmiAhIQELFy7EmjVrTLr/zp07o3Pnzk9839LmIeXm5iItLQ2+vr5ISkoq8bOqML/H3D755BO0adMGw4cPBwBcuHDB6G1FUYQoipAZOE+wuDeehTZDEplHREQEtm3bBgDYsmULmjdvbvS2TZs2xbFjx3D9+nUAQH5+Pq5evQoA+PDDD/Hbb789sc1rr72GhQsXIj09Xf+9goLidc4fFtxubm7Izc3FL7/8YlTun3/+Wf/9du3aISEhAbm5uQCAW7duITPz6a0a69at07f+PCo7O1s/V2Djxo1P3QdQPKp++PBh3L17F2q1Gjt37nzmNo/y8/PTX1kp67aPy8rKgiiK6N69O8aNG4dz585VaH8A0KxZM/38iNzcXOzduxcAoNPpkJaWhlatWmHSpEnIzs5GXl5ehY9nasHOfhhSNxoftXwdf/T6F3Z1W4TlUW/h7Qax6FazBWo7eEMG87XLGBzBN1OLjsxGBft6gajRpS0CXn8JDZbORMT65Qh5fxL8hsXCpUUTyfrqu3fvjj179ug/yKakpCAjI0N//lm5ciUGDBiAmJgYfPLJJ/rttFotpk+fjt69e2PkyJH6c0d8fLz+b+bUqVMYPHgw+vbti4EDByInp+S8jLy8PEyZMgUDBgxA//79sWvXLgDA5cuXMXDgQPTr1w8xMTEGr76VpmnTprh165b+6wMHDuCFF15AbGws3n77bf35KDo6GosWLcLAgQMxcOBA/blz9+7dGDRoEPr3748RI0bgzp07AEquxBUfH48FCxZg6NChWLx4MQ4fPqxfiat///5PPM/HzZw5E3Fxcejdu3eJ1zQ6OhqffPIJYmNjERMToy/Q7969i5EjR6J///6YMWOGfvChND179sT27dsBAFu3bkXv3r31P0tMTMRrr70GAAYzf/nll4iJiUHfvn31V2Nv3LiBUaNGIS4uDi+++OITHxwet2zZMgwYMAB9+vTBu+++q887dOhQ/WvevXt3/PXXXwCKf5c++OAD/e/Zw0JZFEV88MEH6NOnD2JiYvTP69HnAQBz5szBhg0bABR/0OvVqxdiYmLwwQcfPJEtIyNDf8UVKL7q+lBpv+spKSno2bMnZs2ahdjYWKxYsQILFy7Ub7NhwwbMnTsXQMkV5MryOu7YsQN9+vRB37598dJLLz31tbUWlvORmKgSTJ8+HVOnTsWqVav0k2yN9fDxEyZM0L9Rv/POO6hTpw4uXbqkb615VMeOHZGVlYVXX30VWq0Wzs7OqFevHtq1awdnZ2cMGjQIMTEx8PPze+qoz7Rp0zBx4kSsXbsW3bt313+/Xbt2SEpK0o/g29vbY9GiRQZHPwDg77//RrNmzZ74/iuvvIL4+HisWbMGrVq1eubr4eXlhbFjx2Lw4MHw9PREw4YNodMZWDmlFCNHjsQ777yDn3/+GVFRUUZvV5qMjAxMmTJFf/wJEyZUaH9A8QeY6Oho9O3bF35+fmjcuDGcnJyg1WoxadIk5OTkQBRFjBgxwugVkaTk7+ABfwcPdK75v3/7PE0hLj9IwcX7KUjKvomb+ZlIy8tEen4WMguzIVbg7rqGbnRV0RF8hbMjVF41YOPrBbtAf9gF+sEu0A82vl4WO4HXzc0NYWFh2L9/P7p06YLt27ejZ8+eEAShQvNoioqKMH78eCxZsgRhYWHIycl5YiL9Z599hlatWmHBggV48OABBg0ahDZt2uCHH37AsGHD0LdvXxQVFZXpb/ePP/7Qt6ZlZWU9dW6So6Mj1q9fj02bNmH+/Pn4/PPP0bx5c/z4448QBAE//fQTVq5cifj4+CeOc+3aNXz11VeQy+UYM2YMZsyYgebNmyM3Nxc2Nk9f2Wj8+PFwdXWFVqvFiBEjcOHCBX2h6ebmho0bN+Lbb7/F6tWr8d5772H58uVo1qwZxo4di7179+Lf//63wX13794dU6ZMwahRo7Bnzx4sXry4xMDLQ6tXr34i8759+/D777/jxx9/hJ2dHe7duwcAePfddzF79mwEBgbi5MmTmD17NtauXWsww9Pmb5U272j9+vVwcnJCQkICioqKMHjwYLRt2xbnzp3DhQsXsHnzZty9excDBw4s0f74uHv37uG3337Dzp07IQhCqXOQXnrpJYwfPx7r1q1DmzZtEBcXB29v76f+rl+9ehULFizArFmzkJWVhRdeeEF/hWv79u0YM2ZMiWOU9XVcsWIFVq1aBW9v70qdNyUlFvhkdY4fP17i67i4OMTFxQEA/P39Sz1pPlwl5+FjHu17ffRnrVu3RkJCwhPbazQag2vTx8bGIjY2ttSfjR8/HuPHj3/i+998802JrwMCAkq84Tzaozp8+HD9pdBHPfocRo0apf//qamppb6ZRkRElLiK8M477wAo+foBKNHXO2DAgGdOmH18+4eCgoKwZcsW/dcPX4fHHx8VFVXiA8CjvfyPvk6lXXV49Ofu7u7YvXv3E/t86623Smzz6Os2cuRIvPXWW8jPz8dLL72EkSNHQqlU4vvvv3/qc64q7BU2CHcPQrh70BM/K9KqkZafhbT8TNzMy0JGwT3kqPOQoylAjjofuZoC5GjykavOR46mAHmaghJFvYOy9BWbHm/rEuRyyB3tIHd0gNzBHgpHe8gd7SF3sIfSxQkqbw+oPN31/1tVl6x82KbTpUsXbNu2DfPnzwdQsXk0V69ehaenJ8LCwgAUF9OPO3DgAHbv3o3Vq1cDKL5ymJaWhqZNm+Kzzz5Deno6unXrhsDAwGc+h4kTJyI/Px86nU4/mnvy5Mmnzk3q06eP/vk/HFBJT0/H+PHjcfv2bRQVFcHf37/U4/Xo0QNyuRxA8RW1999/HzExMejWrdtTVy8Dikdsf/zxR2g0Gty+fRtJSUn6Ar9bt24AgMaNG+uvvB45cgTLli0DAHTq1AkuLobvP+Hi4gJnZ2ds27YNQUFBBlcnKy3zoUOHEBcXp29DdHV1RW5uLo4fP16i9erxtsXHPW3+Vmnzjg4ePIiLFy/qz/HZ2dm4fv06jh49it69e0Mul8PDwwORkZE4ffp0qb9LQPHvmI2NDaZNm4ZOnTqhU6dOTzymffv22LVrF/744w/s378fsbGx2Lp161N/12vWrKn/vXF3d0dAQABOnDiB2rVr4+rVq09cbS/r6xgREYH4+Hj07NlT//pYOxb4RCawatUqqSMYzZQT76zdjBkzcOXKFRQWFiI2NvapKyJZG5VcidqO3qjt6G3S/Xr06IAandtAUMiL//tvAWftunTpgvfffx9nz55FQUGB/nepIvNoRFE0ah7MJ598grp165b4XlBQEMLDw7F3716MGjUK8+bNQ+vWrZ+6n8WLFyM0NBQffvgh5syZg2XLlkEUxTLPTZo3bx5GjBiBzp07IzExUV9YP+7RuTijR49Gx44dsW/fPjz//PNYs2YNgoKe/GAKAMnJyVi9ejXWr18PFxcXxMfHl3jtlEolgKfPy3mWXr16Yc6cOU+9Clxa5tL+zURRhLOzc4nFIZ7mWfO3Spt3JIoipk+f/sQyy/v27Sv1GHK5vMRVnYf7VygUWL9+PQ4dOoRt27Zh3bp1pQ6aubq6IiYmBjExMXjttddw5MiRp/6u29uXbJ/r2bMnduzYgbp166Jr166lvmZleR3nzJmDkydPYu/evejfvz82bdoENze3Up+7tbDM65lERBbgww8/xObNm7Fz584S/ahUfjKVEnIHO8hsVNWmuAcABwcHtGzZElOnTtWPagPlm0fzUN26dZGRkYFTp04BAHJycp6YIN6uXTusW7dO36P9cH5KcnIyAgICMGzYMERHR+PixYsAiq8IPtpf/zilUol33nkHJ06cQFJS0lPnJgHFI+lAcZvFw6ucj8732bRpk1HP9caNGwgJCcHo0aPRuHFj/TFKW1EmNzcXdnZ2cHJywp07d7B///5n7j8yMlJ/RXHfvn24f//pk9K7dOmCUaNGlVixy5jMbdu2RUJCAvLz8wEUt7w4OjrC399f/1qJovjUiallmb/1ULt27fD999/rV0e7evUq8vLyEBkZiR07dkCr1SIrKwt//fUXwsLC4Ofnh6SkJBQVFSE7OxuHDh0CUPzaZmdno2PHjpg6dWqpOQ8dOqR/fjk5Obhx4wZ8fX3L9LverVs37Nq1C1u3bi2xotxDZX0db9y4gfDwcIwbNw5ubm4l5sVZK47gExERVYI+ffpg7NixJUa7yzOP5iGVSoUlS5Zg3rx5+hvUPb6yzRtvvIH58+ejb9++EEURfn5++Pzzz7F9+3b8/PPPUCgU8PDwwJtvvgmdTocbN248tT0FKF6KeOTIkVi1ahXmz59vcG4SUNwiMWjQIOh0Ov3zHjt2LMaNGwdvb2+Eh4cjJSXlmc/166+/RmJiImQyGYKDg9GhQwf95PrHhYaGomHDhujduzcCAgJKnXP0uDfffBP/+Mc/EBsbi8jISNSsWfOpj3d0dHzmcp6lZVapVLhw4QIGDBgApVKJjh07YsKECVi0aBFmzZqFTz/9FBqNBr169SoxOfVRZZm/9dCgQYOQmpqKuLg4iKIINzc3rFixAl27dsXx48fRr18/CIKASZMmwdPTE0Dxh6eYmBgEBgbq73GSm5uLN954Q/8hY8qUKU8c6+zZs5g7d65+pbRBgwbp28iM/V13cXFBcHAwrly5ot/2UR06dCjT67hw4UJcv34doiiiVatWBl9bayKIT5sqTkRERNXCpUuXkJCQUGrRVh6G1pI3lT179iA5ObnEkr5EVIwFPhEREZmcuQt8IjKMBT4RERERkRXhJFsiIiIiIivCAp+IiIiIyIqwwCciIiIisiIs8ImIiIiIrAgLfCIiIiIiK8ICn4iIiIjIirDAJyIiIiKyIizwiYiIiIisCAt8IiIiIiIrwgKfiIiIiMiKsMAnIiIiIrIiLPCJiIiIiKwIC3wiIiIiIivCAp+IiIiIyIqwwCciIiIisiIs8ImIiIiIrAgLfCIiIiIiK8ICn4iIiIjIirDAJyIiIiKyIizwiYiIiIisCAt8IiIiIiIrwgKfiIiIiMiKsMAnIiIiIrIiLPCJiIiIiKwIC3wiIiIiIivCAp+IiIiIyIqwwCciIiIisiIs8ImIiIiIrMj/A5amMNUb2vIXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouped_segments = complete_ranking.groupBy(\"segments\").agg(F.count(\"merchant_abn\").alias('num_merchants')).orderBy(\"num_merchants\", ascending = False)\n",
    "grouped_segments_pd = grouped_segments.toPandas()\n",
    "colours = [\"#f7b03e\", \"#4c91d4\", \"#a265a8\", \"#27ae60\" ,\"#cb4760\"]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.pie(grouped_segments_pd['num_merchants'], colors=colours, wedgeprops=dict(width=0.5), startangle=-40, explode= [0.05]*5, labels=grouped_segments_pd['segments'])\n",
    "plt.savefig(f\"../plots/donut_chart_segments\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_value_segments = complete_ranking.groupBy(\"segments\").agg(F.sum(\"risk_adjusted_epv\").alias(\"total_epv\")).orderBy(\"total_epv\", ascending=False)\n",
    "total_value_segments_pd = total_value_segments.toPandas()\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.bar(total_value_segments_pd['segments'], total_value_segments_pd['total_epv'], color = colours, edgecolor = 'black')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.xticks([])\n",
    "plt.yticks(size=14)\n",
    "plt.savefig(f'../plots/total_revenue_segments', transparent = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segments</th>\n",
       "      <th>total_epv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Books, Media, Arts, Crafts, and Hobbies</td>\n",
       "      <td>1.419670e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computers, Electronics, and Office Supplies</td>\n",
       "      <td>9.237914e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vehicles, Repairs, and Miscellaneous Services</td>\n",
       "      <td>8.031290e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fashion, Personal Accessories, Health, and Beauty</td>\n",
       "      <td>7.351826e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home, Garden, and Furnishings</td>\n",
       "      <td>6.485205e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            segments     total_epv\n",
       "0            Books, Media, Arts, Crafts, and Hobbies  1.419670e+06\n",
       "1        Computers, Electronics, and Office Supplies  9.237914e+05\n",
       "2      Vehicles, Repairs, and Miscellaneous Services  8.031290e+05\n",
       "3  Fashion, Personal Accessories, Health, and Beauty  7.351826e+05\n",
       "4                      Home, Garden, and Furnishings  6.485205e+05"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:09 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:13:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:14:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:14:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:14:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:14:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:14:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:14:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:15:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:15:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:15:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:15:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:15:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:15:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:16:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:16:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:16:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.0.220:51431\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "24/10/07 21:16:27 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 51458)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/skyeha/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/skyeha/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/skyeha/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/skyeha/.local/lib/python3.8/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_value_segments_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [\"Computers, Electronics, and Office Supplies\", \"Home, Garden, and Furnishings\", \"Books, Media, Arts, Crafts, and Hobbies\", \"Fashion, Personal Accessories, Health, and Beauty\"\n",
    "            , \"Vehicles, Repairs, and Miscellaneous Services\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----------------------------+-------------------------------------------+\n",
      "|merchant_abn|risk_adjusted_epv |name                         |segments                                   |\n",
      "+------------+------------------+-----------------------------+-------------------------------------------+\n",
      "|21439773999 |50237.87488272766 |Mauris Non Institute         |Computers, Electronics, and Office Supplies|\n",
      "|82368304209 |47340.66112303206 |Nec Incorporated             |Computers, Electronics, and Office Supplies|\n",
      "|35909341340 |40682.22831221837 |Arcu Sed Eu Incorporated     |Computers, Electronics, and Office Supplies|\n",
      "|45433476494 |36275.68139853707 |Adipiscing Elit Foundation   |Computers, Electronics, and Office Supplies|\n",
      "|58454491168 |35154.509483312075|Diam At Foundation           |Computers, Electronics, and Office Supplies|\n",
      "|94690988633 |34267.85544270964 |Eu Placerat LLC              |Computers, Electronics, and Office Supplies|\n",
      "|67400260923 |28740.143895221187|Eleifend PC                  |Computers, Electronics, and Office Supplies|\n",
      "|80518954462 |28358.23050269475 |Neque Sed Dictum Incorporated|Computers, Electronics, and Office Supplies|\n",
      "|57757792876 |26740.83898664496 |Pretium Et LLC               |Computers, Electronics, and Office Supplies|\n",
      "|34096466752 |26526.128306872553|Nullam Enim Ltd              |Computers, Electronics, and Office Supplies|\n",
      "+------------+------------------+-----------------------------+-------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seg1_ranking = complete_ranking.filter(F.col('segments') == segments[0])\n",
    "seg1_ranking = seg1_ranking.orderBy(\"risk_adjusted_epv\", ascending=False)\n",
    "seg1_ranking.write.parquet(f\"../data/curated/seg1_ranking.parquet\", mode='overwrite')\n",
    "seg1_ranking.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------------------+-----------------------------+\n",
      "|merchant_abn|risk_adjusted_epv |name                          |segments                     |\n",
      "+------------+------------------+------------------------------+-----------------------------+\n",
      "|79827781481 |50753.42918974075 |Amet Risus Inc.               |Home, Garden, and Furnishings|\n",
      "|76767266140 |39744.37216363429 |Phasellus At Limited          |Home, Garden, and Furnishings|\n",
      "|43186523025 |37023.390424610785|Lorem Ipsum Sodales Industries|Home, Garden, and Furnishings|\n",
      "|49212265466 |32280.258727887383|Auctor Company                |Home, Garden, and Furnishings|\n",
      "|21772962346 |29226.73924604561 |Purus Gravida Sagittis Ltd    |Home, Garden, and Furnishings|\n",
      "|38090089066 |26982.619960897013|Interdum Feugiat Sed Inc.     |Home, Garden, and Furnishings|\n",
      "|42355028515 |26876.88514332147 |Eu Inc.                       |Home, Garden, and Furnishings|\n",
      "|76314317957 |26000.036184292283|Semper Corp.                  |Home, Garden, and Furnishings|\n",
      "|24852446429 |23186.20228093389 |Erat Vitae LLP                |Home, Garden, and Furnishings|\n",
      "|90543168331 |20781.929508536887|Phasellus Dapibus Incorporated|Home, Garden, and Furnishings|\n",
      "+------------+------------------+------------------------------+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "seg2_ranking = complete_ranking.filter(F.col('segments') == segments[1])\n",
    "seg2_ranking = seg2_ranking.orderBy(\"risk_adjusted_epv\", ascending=False)\n",
    "seg2_ranking.write.parquet(f\"../data/curated/seg2_ranking.parquet\", mode='overwrite')\n",
    "seg2_ranking.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----------------------------+---------------------------------------+\n",
      "|merchant_abn|risk_adjusted_epv |name                         |segments                               |\n",
      "+------------+------------------+-----------------------------+---------------------------------------+\n",
      "|32361057556 |53912.79040771167 |Orci In Consequat Corporation|Books, Media, Arts, Crafts, and Hobbies|\n",
      "|45629217853 |51485.24888621164 |Lacus Consulting             |Books, Media, Arts, Crafts, and Hobbies|\n",
      "|64403598239 |49691.21355992612 |Lobortis Ultrices Company    |Books, Media, Arts, Crafts, and Hobbies|\n",
      "|94493496784 |44891.043232800184|Dictum Phasellus In Institute|Books, Media, Arts, Crafts, and Hobbies|\n",
      "|63123845164 |44701.43692027049 |Odio Phasellus Institute     |Books, Media, Arts, Crafts, and Hobbies|\n",
      "|72472909171 |40469.11957641558 |Nullam Consulting            |Books, Media, Arts, Crafts, and Hobbies|\n",
      "|79417999332 |40427.97266394555 |Phasellus At Company         |Books, Media, Arts, Crafts, and Hobbies|\n",
      "|40515428545 |40112.4793529307  |Elit Sed Consequat Associates|Books, Media, Arts, Crafts, and Hobbies|\n",
      "|27326652377 |38748.743017667766|Tellus Aenean Corporation    |Books, Media, Arts, Crafts, and Hobbies|\n",
      "|98973094975 |36338.63267471447 |Ornare Fusce Inc.            |Books, Media, Arts, Crafts, and Hobbies|\n",
      "+------------+------------------+-----------------------------+---------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "seg3_ranking = complete_ranking.filter(F.col('segments') == segments[2])\n",
    "seg3_ranking = seg3_ranking.orderBy(\"risk_adjusted_epv\", ascending=False)\n",
    "seg3_ranking.write.parquet(f\"../data/curated/seg3_ranking.parquet\", mode='overwrite')\n",
    "seg3_ranking.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----------------------------+-------------------------------------------------+\n",
      "|merchant_abn|risk_adjusted_epv |name                         |segments                                         |\n",
      "+------------+------------------+-----------------------------+-------------------------------------------------+\n",
      "|48534649627 |54713.17129337545 |Dignissim Maecenas Foundation|Fashion, Personal Accessories, Health, and Beauty|\n",
      "|86578477987 |54579.04496012397 |Leo In Consulting            |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|49322182190 |45390.3585100905  |Gravida Mauris Incorporated  |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|93558142492 |27957.937277723926|Dolor Quisque Inc.           |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|11439466003 |24345.4354053516  |Blandit At LLC               |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|95574756848 |21336.60624685837 |At Pede Inc.                 |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|99976658299 |19586.089687544805|Sociosqu Corp.               |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|62224020443 |18998.98769599394 |Hendrerit A Corporation      |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|46804135891 |18890.05620563631 |Suspendisse Dui Corporation  |Fashion, Personal Accessories, Health, and Beauty|\n",
      "|81761494572 |17862.539408761997|Nulla Facilisis Institute    |Fashion, Personal Accessories, Health, and Beauty|\n",
      "+------------+------------------+-----------------------------+-------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seg4_ranking = complete_ranking.filter(F.col('segments') == segments[3])\n",
    "seg4_ranking = seg4_ranking.orderBy(\"risk_adjusted_epv\", ascending=False)\n",
    "seg4_ranking.write.parquet(f\"../data/curated/seg4_ranking.parquet\", mode='overwrite')\n",
    "seg4_ranking.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 840:>                (0 + 3) / 3][Stage 843:>                (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------------------+---------------------------------------------+\n",
      "|merchant_abn|risk_adjusted_epv |name                          |segments                                     |\n",
      "+------------+------------------+------------------------------+---------------------------------------------+\n",
      "|96680767841 |49906.659905709246|Ornare Limited                |Vehicles, Repairs, and Miscellaneous Services|\n",
      "|38700038932 |49510.49110209373 |Etiam Bibendum Industries     |Vehicles, Repairs, and Miscellaneous Services|\n",
      "|89726005175 |47880.09446935283 |Est Nunc Consulting           |Vehicles, Repairs, and Miscellaneous Services|\n",
      "|80551528183 |37912.869172420076|Ac Ipsum LLC                  |Vehicles, Repairs, and Miscellaneous Services|\n",
      "|49891706470 |36535.65054086117 |Non Vestibulum Industries     |Vehicles, Repairs, and Miscellaneous Services|\n",
      "|90568944804 |35928.5012315087  |Diam Eu Dolor LLC             |Vehicles, Repairs, and Miscellaneous Services|\n",
      "|13514558491 |35267.10118083442 |Magna Praesent PC             |Vehicles, Repairs, and Miscellaneous Services|\n",
      "|75454398468 |32470.586703261   |Tempus Non Lacinia Corporation|Vehicles, Repairs, and Miscellaneous Services|\n",
      "|68559320474 |29984.835543031975|Aliquam Auctor Associates     |Vehicles, Repairs, and Miscellaneous Services|\n",
      "|49549583265 |29787.8818223957  |Luctus Et Incorporated        |Vehicles, Repairs, and Miscellaneous Services|\n",
      "+------------+------------------+------------------------------+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "seg5_ranking = complete_ranking.filter(F.col('segments') == segments[4])\n",
    "seg5_ranking = seg5_ranking.orderBy(\"risk_adjusted_epv\", ascending=False)\n",
    "seg5_ranking.write.parquet(f\"../data/curated/seg5_ranking.parquet\", mode='overwrite')\n",
    "seg5_ranking.show(10, truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
