{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.consumer_transaction_model import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python3.11\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/local/bin/python3.11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"consumer transaction model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.execturo.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_info = spark.read.parquet('../data/curated/consumer_info.parquet')\n",
    "transaction_records = spark.read.parquet('../data/curated/transactions.parquet')\n",
    "fraudulent_consumer_rate = spark.read.parquet('../data/curated/consumer_fraud_prob.parquet')\n",
    "personal_fraud = spark.read.csv('../data/curated/personal_fraud.csv', header=True, inferSchema=True)\n",
    "postcode_info = spark.read.csv('../data/curated/postcode_info.csv', header=True, inferSchema=True)\n",
    "\n",
    "personal_fraud = personal_fraud.drop(personal_fraud.columns[0])\n",
    "postcode_info = postcode_info.drop(postcode_info.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data frame for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add consumer info to transaction records\n",
    "consumer_transaction_records = transaction_records.join(consumer_info, on=\"consumer_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transaction with consumer infomation\n",
    "transaction_fraudulent_consumer = transaction_records.join(fraudulent_consumer_rate, on=[\"order_datetime\", \"consumer_id\"], how=\"inner\")\n",
    "transaction_fraudulent_consumer_with_info = consumer_info.join(transaction_fraudulent_consumer, on=\"consumer_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique available fraudulent consumer (does not lost any consumer after merging with transaction records)\n",
    "transaction_fraudulent_consumer_with_info.select(\"consumer_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average fraud probability in each postcode or state\n",
    "fraudulent_consumer_group_by_postcode = transaction_fraudulent_consumer_with_info.groupBy([\"postcode\"]).agg(F.avg(\"fraud_probability\").alias(\"average_fraud_prob_of_postcode\"))\n",
    "\n",
    "fraudulent_consumer_group_by_state = transaction_fraudulent_consumer_with_info.groupBy([\"state\"]).agg(F.avg(\"fraud_probability\").alias(\"average_fraud_prob_of_state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis order value, consider the variance of order value and purchase frequency\n",
    "consumer_transaction_value_analysis =  consumer_transaction_records.groupBy(\"consumer_id\", \"state\", \"postcode\") \\\n",
    "                                        .agg(\n",
    "                                            F.avg(\"dollar_value\").alias(\"average_dollar_value\"),\n",
    "                                            F.min(\"dollar_value\").alias(\"min_dollar_value\"),\n",
    "                                            F.max(\"dollar_value\").alias(\"max_dollar_value\"),\n",
    "                                            F.count(\"dollar_value\").alias(\"transaction_count\"),\n",
    "                                            F.stddev(\"dollar_value\").alias(\"stddev_dollar_value\")\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_with_info \\\n",
    "    .join(consumer_transaction_value_analysis, on=[\"consumer_id\", \"state\",\"postcode\"], how=\"left\") \\\n",
    "    .join(fraudulent_consumer_group_by_postcode, on=\"postcode\", how=\"inner\") \\\n",
    "    .join(fraudulent_consumer_group_by_state, on=\"state\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing 2755 in postcode_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get infomation about personal fraud and income from external dataset\n",
    "postcode_info = postcode_info.drop(\"state\", \"long\", \"lat\", \"lgacode\")\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.join(personal_fraud, on=\"state\", how=\"inner\")\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.join(postcode_info, on=\"postcode\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proportion of the money used to purchase item with respect to income (one and a half year)\n",
    "# average income\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"Proportion_between_max_order_value_mean_income\", F.col(\"max_dollar_value\") / (F.col(\"mean_income\") * 1.5) )\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"Proportion_between_max_order_value_median_income\", F.col(\"max_dollar_value\") / (F.col(\"median_income\") * 1.5))\n",
    "\n",
    "# Total income\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"Proportion_between_total_order_value_mean_income\", F.col(\"average_dollar_value\") * F.col(\"transaction_count\") / (F.col(\"mean_income\") * 1.5))\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"Proportion_between_total_order_value_median_income\", F.col(\"average_dollar_value\") * F.col(\"transaction_count\") / (F.col(\"median_income\") * 1.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_fraudulent_consumer_summary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis datetime information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'order_datetime' from string to date format\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"order_datetime\", F.to_date(\"order_datetime\", \"yyyy-MM-dd\"))\n",
    "cutoff_date = \"2021-03-07\"\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.filter(F.col(\"order_datetime\") >= F.lit(cutoff_date))\n",
    "\n",
    "# Add a new column 'transaction_count_last_n_days' that counts the transactions within n days before each transaction\n",
    "window_spec = Window.partitionBy(\"consumer_id\").orderBy(F.col(\"order_datetime\").cast(\"long\")) \\\n",
    "    .rangeBetween(-7 * 86400, 0)  # 7 days in seconds (86400 seconds = 1 day)\n",
    "\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"transaction_count_last_7_days\", F.count(\"order_datetime\").over(window_spec))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the corresponding day of the week for the given date in the DataFrame.\n",
    "\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"day_of_week\", F.date_format(\"order_datetime\", \"EEEE\"))\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"is_weekend\", F.date_format(\"order_datetime\", \"EEEE\").isin(\"Saturday\", \"Sunday\"))\n",
    "\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"month\", F.month(\"order_datetime\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Month has a weak relatinonship with fraud probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_summary = transaction_fraudulent_consumer_summary.withColumn(\"month\", F.month(\"order_datetime\")) \\\n",
    "    .groupBy(\"month\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.avg(\"fraud_probability\").alias(\"average_fraud_probability\")\n",
    "    )\n",
    "\n",
    "monthly_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizes features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "\n",
    "cols_to_scale = [\"dollar_value\", \"min_dollar_value\", \"max_dollar_value\", \"stddev_dollar_value\",\"average_dollar_value\"]\n",
    "cols_to_keep_unscaled = [col for col in transaction_fraudulent_consumer_summary.columns if col not in cols_to_scale] + [\"dollar_value\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols_to_scale, outputCol=\"features\")\n",
    "sdf_transformed = assembler.transform(transaction_fraudulent_consumer_summary)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(sdf_transformed.select(\"features\"))\n",
    "sdf_scaled = scaler_model.transform(sdf_transformed)\n",
    "scaled_array_col = vector_to_array(F.col(\"scaledFeatures\"))\n",
    "\n",
    "# Create new columns for each scaled feature\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    sdf_scaled = sdf_scaled.withColumn(f\"scaled_{col}\", scaled_array_col[i])\n",
    "\n",
    "# Combine original Dataframe and the scaled features\n",
    "dollar_value_df = transaction_fraudulent_consumer_summary.select(\"dollar_value\")\n",
    "transaction_fraudulent_consumer_summary = sdf_scaled.select(cols_to_keep_unscaled + [f\"scaled_{col}\" for col in cols_to_scale])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Log transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_log = ['scaled_dollar_value', 'scaled_average_dollar_value', 'scaled_min_dollar_value',\n",
    "                'scaled_max_dollar_value', 'scaled_stddev_dollar_value',\n",
    "                \"Proportion_between_max_order_value_mean_income\",\n",
    "                \"Proportion_between_max_order_value_median_income\",\n",
    "                \"Proportion_between_total_order_value_mean_income\",\n",
    "                \"Proportion_between_total_order_value_median_income\"\n",
    "                ] \n",
    "\n",
    "\n",
    "for col in cols_to_log:\n",
    "    transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary \\\n",
    "        .withColumn(f'{col}', F.when(transaction_fraudulent_consumer_summary[col] > 0, F.log(transaction_fraudulent_consumer_summary[col])).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_fraudulent_consumer_summary.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions or Observations:\n",
    "1. The gender plot shows that there is a similar number of male and female consumers.\n",
    "2. The number of consumers varies significantly across different states.\n",
    "3. Consumers make a similar number of purchases on each day of the week, whether it’s a weekday or a weekend.\n",
    "4. Both fraud probability and the dollar value of an order are strongly right-skewed and should be normalized.\n",
    "5. Proportion features exhibit a linear relationship with fraud probability but may need transformation to clarify this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to Pandas\n",
    "df_pandas = transaction_fraudulent_consumer_summary.select(\n",
    "    \"dollar_value\", \"scaled_dollar_value\", \"fraud_probability\", \"scaled_average_dollar_value\", \n",
    "    \"scaled_min_dollar_value\", \"scaled_max_dollar_value\", \"transaction_count\", \n",
    "    \"median_income\", \"mean_income\", \"state\", \"gender\", \"scaled_stddev_dollar_value\",\n",
    "    \"day_of_week\", \"is_weekend\", \"Proportion_between_max_order_value_mean_income\",\n",
    "    \"Proportion_between_max_order_value_median_income\", \n",
    "    \"Proportion_between_total_order_value_mean_income\", \n",
    "    \"Proportion_between_total_order_value_median_income\"\n",
    ").toPandas()\n",
    "\n",
    "# Define plots in a dictionary for looping\n",
    "plots = {\n",
    "    \"Dollar Value Distribution\": (\"dollar_value\", \"hist\"),\n",
    "    \"Scaled Dollar Value Distribution\": (\"scaled_dollar_value\", \"hist\"),\n",
    "    \"Max Dollar Value Distribution\": (\"scaled_max_dollar_value\", \"hist\"),\n",
    "    \"Min Dollar Value Distribution\": (\"scaled_min_dollar_value\", \"hist\"),\n",
    "    \"Std Dollar Value Distribution\": (\"scaled_stddev_dollar_value\", \"hist\"),\n",
    "    \"Average Dollar Value Distribution\": (\"scaled_average_dollar_value\", \"hist\"),\n",
    "    \"Fraud Probability Distribution\": (\"fraud_probability\", \"hist\"),\n",
    "    \"Transaction Count Distribution\": (\"transaction_count\", \"hist\"),\n",
    "    \"Gender Count\": (\"gender\", \"count\"),\n",
    "    \"State Count\": (\"state\", \"count\"),\n",
    "    \"Day of Week Count\": (\"day_of_week\", \"count\"),\n",
    "    \"Is Weekend Count\": (\"is_weekend\", \"count\"),\n",
    "    \"Scatter 1 (Max Order Value vs Fraud Prob - Mean Income)\": (\"Proportion_between_max_order_value_mean_income\", \"scatter1\"),\n",
    "    \"Scatter 2 (Max Order Value vs Fraud Prob - Median Income)\": (\"Proportion_between_max_order_value_median_income\", \"scatter2\"),\n",
    "    \"Scatter 3 (Total Order Value vs Fraud Prob - Mean Income)\": (\"Proportion_between_total_order_value_mean_income\", \"scatter3\"),\n",
    "    \"Scatter 4 (Total Order Value vs Fraud Prob - Median Income)\": (\"Proportion_between_total_order_value_median_income\", \"scatter4\")\n",
    "}\n",
    "feature_visualisation(df_pandas, plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = transaction_fraudulent_consumer_summary.select(\n",
    "    \"scaled_dollar_value\", \"fraud_probability\", \"scaled_average_dollar_value\", \n",
    "    \"scaled_min_dollar_value\", \"scaled_max_dollar_value\", \"transaction_count\", \n",
    "    \"median_income\", \"mean_income\", \"Proportion_between_max_order_value_mean_income\",\n",
    "    \"Proportion_between_max_order_value_median_income\", \n",
    "    \"Proportion_between_total_order_value_mean_income\", \n",
    "    \"Proportion_between_total_order_value_median_income\"\n",
    ").toPandas()\n",
    "corr_matrix = df_pandas.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "1. Time Frequency feature: https://ieeexplore.ieee.org/document/9399421/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_fraudulent_consumer_summary.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to be used in the model\n",
    "\n",
    "features = [\"average_fraud_prob_of_postcode\", \"victimisation_rate\", \"rse_percent\",\n",
    "            \"Proportion_between_max_order_value_mean_income\", \"Proportion_between_max_order_value_median_income\",\n",
    "            \"Proportion_between_total_order_value_mean_income\", \"Proportion_between_total_order_value_median_income\",\n",
    "            \"transaction_count_last_7_days\", \"scaled_dollar_value\", \"scaled_min_dollar_value\",\n",
    "            \"scaled_max_dollar_value\", \"scaled_stddev_dollar_value\", \"scaled_average_dollar_value\", \"month_index\"]\n",
    "\n",
    "features_dt = [\n",
    "    \"scaled_dollar_value\", \"scaled_max_dollar_value\", \"average_fraud_prob_of_postcode\",\n",
    "    \"month\", \"transaction_count_last_7_days\", \"scaled_average_dollar_value\"\n",
    "\n",
    "]\n",
    "\n",
    "features_rf = [\n",
    "    \"scaled_dollar_value\", \"scaled_max_dollar_value\", \"average_fraud_prob_of_postcode\",\n",
    "    \"scaled_stddev_dollar_value\", \"Proportion_between_max_order_value_median_income\",\n",
    "    \"month\", \"transaction_count_last_7_days\", \"Proportion_between_max_order_value_mean_income\"\n",
    "\n",
    "]\n",
    "\n",
    "features_lr = [\n",
    "    \"transaction_count_last_7_days\",\n",
    "    \"Proportion_between_total_order_value_mean_income\",\n",
    "    \"scaled_max_dollar_value\",\n",
    "    \"scaled_average_dollar_value\",\n",
    "    \"scaled_stddev_dollar_value\",\n",
    "    \"Proportion_between_max_order_value_median_income\",\n",
    "    \"average_fraud_prob_of_postcode\",\n",
    "    \"Proportion_between_max_order_value_mean_income\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"is_weekend\", transaction_fraudulent_consumer_summary[\"is_weekend\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_index = StringIndexer(inputCol=\"month\", outputCol=\"month_index\")\n",
    "\n",
    "\n",
    "# day_of_week_index, \n",
    "# is_weekend_index, \n",
    "assembler_dt = VectorAssembler(inputCols=features_dt, outputCol=\"features\")\n",
    "pipeline_dt = Pipeline(stages = [month_index, assembler_dt])\n",
    "\n",
    "assembler_rf = VectorAssembler(inputCols=features_rf, outputCol=\"features\")\n",
    "pipeline_rf = Pipeline(stages = [month_index, assembler_rf])\n",
    "\n",
    "assembler_lr = VectorAssembler(inputCols=features_lr, outputCol=\"features\")\n",
    "pipeline_lr = Pipeline(stages = [month_index, assembler_lr])\n",
    "\n",
    "data_dt = pipeline_dt.fit(transaction_fraudulent_consumer_summary).transform(transaction_fraudulent_consumer_summary)\n",
    "data_rf = pipeline_rf.fit(transaction_fraudulent_consumer_summary).transform(transaction_fraudulent_consumer_summary)\n",
    "data_lr = pipeline_lr.fit(transaction_fraudulent_consumer_summary).transform(transaction_fraudulent_consumer_summary)\n",
    "\n",
    "train_data_dt, test_data_dt = data_dt.randomSplit([0.8, 0.2])\n",
    "train_data_rf, test_data_rf = data_rf.randomSplit([0.8, 0.2])\n",
    "train_data_lr, test_data_lr = data_lr.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model regressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"fraud_probability\", featuresCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"fraud_probability\", featuresCol=\"features\")\n",
    "\n",
    "lr = LinearRegression(labelCol=\"fraud_probability\", featuresCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 7]) \\\n",
    "    .build()\n",
    "\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator for regression models\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"fraud_probability\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"  \n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"fraud_probability\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\" \n",
    ")\n",
    "\n",
    "# Cross-validation \n",
    "dt_cv = CrossValidator(\n",
    "    estimator=dt,\n",
    "    estimatorParamMaps=dt_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=lr_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline \n",
    "dt_pipeline = Pipeline(stages=[dt_cv])\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[rf_cv])\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[lr_cv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "\n",
    "# 6 mins\n",
    "dt_model = dt_pipeline.fit(train_data_dt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 mins\n",
    "rf_model = rf_pipeline.fit(train_data_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4 mins\n",
    "lr_model = lr_pipeline.fit(train_data_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "dt_predictions = dt_model.transform(test_data_dt)\n",
    "rf_predictions = rf_model.transform(test_data_rf)\n",
    "lr_predictions = lr_model.transform(test_data_lr)\n",
    "\n",
    "\n",
    "dt_rmse = rmse_evaluator.evaluate(dt_predictions)\n",
    "dt_r2 = r2_evaluator.evaluate(dt_predictions)\n",
    "print(f\"Decision Tree RMSE: {dt_rmse}\")\n",
    "print(f\"Decision Tree R2: {dt_r2}\")         \n",
    "\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "print(f\"Random Forest R2: {rf_r2}\")           \n",
    "\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
    "print(f\"Linear Regression R2: {lr_r2}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Best model hyperparameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt_model = dt_model.stages[-1].bestModel\n",
    "print(f\"Best Decision Tree maxDepth: {best_dt_model._java_obj.getMaxDepth()}\")\n",
    "print(f\"Best Decision Tree maxBins: {best_dt_model._java_obj.getMaxBins()}\")\n",
    "\n",
    "\n",
    "best_rf_model = rf_model.stages[-1].bestModel\n",
    "print(f\"Best Random Forest numTrees: {best_rf_model.getNumTrees}\")\n",
    "print(f\"Best Random Forest maxDepth: {best_rf_model.getMaxDepth()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Feature importances__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_dt = assembler_dt.getInputCols()\n",
    "feature_names_rf = assembler_rf.getInputCols()\n",
    "feature_names_lr = assembler_lr.getInputCols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random forest and decision tree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = rf_model.stages[0].bestModel\n",
    "best_dt_model = dt_model.stages[0].bestModel\n",
    "\n",
    "dt_feature_importances = best_dt_model.featureImportances\n",
    "rf_feature_importances = best_rf_model.featureImportances\n",
    "\n",
    "rf_importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names_rf,\n",
    "    \"Importance\": rf_feature_importances.toArray()\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "dt_importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names_dt,\n",
    "    \"Importance\": dt_feature_importances.toArray()\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(rf_importances_df)\n",
    "print()\n",
    "print(dt_importances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get coefficients \n",
    "best_lr_model = lr_model.stages[0].bestModel\n",
    "coefficients = best_lr_model.coefficients\n",
    "# Get feature names from the VectorAssembler\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": feature_names_lr,\n",
    "    \"Coefficient\": coefficients\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(feature_importances)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
