{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"consumer model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.execturo.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "consumer_info = spark.read.parquet('../data/curated/consumer_info.parquet')\n",
    "transaction_records = spark.read.parquet('../data/curated/transaction_records.parquet')\n",
    "fraudulent_consumer_rate = spark.read.parquet('../data/curated/consumer_fraud_rate.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_with_info = consumer_info.join(fraudulent_consumer_rate, on=\"consumer_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average fraud probability in each postcode or state\n",
    "fraudulent_consumer_group_by_postcode = fraudulent_consumer_with_info.groupBy([\"postcode\"]).agg(F.avg(\"fraud_probability\").alias(\"average_fraud_prob_of_postcode\"))\n",
    "\n",
    "fraudulent_consumer_group_by_state = fraudulent_consumer_with_info.groupBy([\"state\"]).agg(F.avg(\"fraud_probability\").alias(\"average_fraud_prob_of_state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average fraud prob for each consumer\n",
    "average_fraudulent_consumer_rate = fraudulent_consumer_rate.groupBy(\"consumer_id\").agg(F.avg(\"fraud_probability\").alias(\"average_fraud_probability\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_rate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique available fraudulent consumer\n",
    "average_fraudulent_consumer_rate.where(average_fraudulent_consumer_rate[\"average_fraud_probability\"]>0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add consumer info to transaction records\n",
    "consumer_transaction_records = transaction_records.join(consumer_info, on=\"consumer_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order has really high variance and value\n",
    "consumer_transaction_records.select(\"dollar_value\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis order value, consider the variance of order value and purchase frequency\n",
    "consumer_transaction_value_analysis =  consumer_transaction_records.groupBy(\"consumer_id\", \"state\", \"postcode\") \\\n",
    "                                        .agg(\n",
    "                                            F.avg(\"dollar_value\").alias(\"average_dollar_value\"),\n",
    "                                            F.min(\"dollar_value\").alias(\"min_dollar_value\"),\n",
    "                                            F.max(\"dollar_value\").alias(\"max_dollar_value\"),\n",
    "                                            F.count(\"dollar_value\").alias(\"transaction_count\"),\n",
    "                                            F.stddev(\"dollar_value\").alias(\"stddev_dollar_value\")\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data frame for modelling\n",
    "fraudulent_consumer_summary = average_fraudulent_consumer_rate \\\n",
    "    .join(consumer_transaction_value_analysis, on=\"consumer_id\", how=\"left\") \\\n",
    "    .join(fraudulent_consumer_group_by_postcode, on=\"postcode\", how=\"inner\") \\\n",
    "    .join(fraudulent_consumer_group_by_state, on=\"state\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_summary.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "1. Time Frequency feature: https://ieeexplore.ieee.org/document/9399421/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to be used in the model\n",
    "features = [\n",
    "    \"average_dollar_value\", \"min_dollar_value\", \"max_dollar_value\", \n",
    "    \"transaction_count\", \"stddev_dollar_value\", \n",
    "    \"average_fraud_prob_of_postcode\", \"average_fraud_prob_of_state\"\n",
    "]\n",
    "\n",
    "# VectorAssembler to combine the features into a single vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Prepare the data\n",
    "data = assembler.transform(fraudulent_consumer_summary)\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.select(\"features\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model regressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"average_fraud_probability\", featuresCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"average_fraud_probability\", featuresCol=\"features\")\n",
    "\n",
    "lr = LinearRegression(labelCol=\"average_fraud_probability\", featuresCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 7]) \\\n",
    "    .build()\n",
    "\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator for regression models\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"average_fraud_probability\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"  \n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"average_fraud_probability\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\" \n",
    ")\n",
    "\n",
    "# Cross-validation \n",
    "dt_cv = CrossValidator(\n",
    "    estimator=dt,\n",
    "    estimatorParamMaps=dt_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=lr_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3  # Use 3 folds for cross-validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline \n",
    "dt_pipeline = Pipeline(stages=[dt_cv])\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[rf_cv])\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[lr_cv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/08 11:24:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/09/08 11:24:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/09/08 11:24:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "# 6 mins\n",
    "#dt_model = dt_pipeline.fit(train_data)\n",
    "\n",
    "# 7 mins\n",
    "#rf_model = rf_pipeline.fit(train_data)\n",
    "\n",
    "# 4 mins\n",
    "lr_model = lr_pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 974:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE: 7.547075797936904\n",
      "Random Forest R2: 0.400604603315338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "#dt_predictions = dt_model.transform(test_data)\n",
    "#rf_predictions = rf_model.transform(test_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "\n",
    "# dt_rmse = rmse_evaluator.evaluate(dt_predictions)\n",
    "# dt_r2 = r2_evaluator.evaluate(dt_predictions)\n",
    "# print(f\"Decision Tree RMSE: {dt_rmse}\")\n",
    "# print(f\"Decision Tree R2: {dt_r2}\")           # RMSE: 6.93 R2: 0.45\n",
    "\n",
    "# rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "# rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "# print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "# print(f\"Random Forest R2: {rf_r2}\")            # RMSE: 6.438397547267348 R2: 0.544903685111079\n",
    "\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
    "print(f\"Linear Regression R2: {lr_r2}\")  # RMSE: 7.547075797936904 R2: 0.400604603315338\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model hyperparameters\n",
    "# best_dt_model = dt_model.stages[-1].bestModel\n",
    "# print(f\"Best Decision Tree maxDepth: {best_dt_model._java_obj.getMaxDepth()}\")\n",
    "# print(f\"Best Decision Tree maxBins: {best_dt_model._java_obj.getMaxBins()}\")\n",
    "\n",
    "\n",
    "# best_rf_model = rf_model.stages[-1].bestModel\n",
    "# print(f\"Best Random Forest numTrees: {best_rf_model.getNumTrees}\")\n",
    "# print(f\"Best Random Forest maxDepth: {best_rf_model.getMaxDepth()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
