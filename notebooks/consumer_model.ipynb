{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.consumer_model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/28 21:36:27 WARN Utils: Your hostname, qinsitaodeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.4.51 instead (on interface en0)\n",
      "24/09/28 21:36:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"consumer model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.execturo.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "consumer_info = spark.read.parquet('../data/curated/consumer_info.parquet')\n",
    "transaction_records = spark.read.parquet('../data/curated/transactions.parquet')\n",
    "fraudulent_consumer_rate = spark.read.parquet('../data/curated/consumer_fraud_prob.parquet')\n",
    "personal_fraud = spark.read.csv('../data/curated/personal_fraud.csv', header=True, inferSchema=True)\n",
    "postcode_info = spark.read.csv('../data/curated/postcode_info.csv', header=True, inferSchema=True)\n",
    "\n",
    "personal_fraud = personal_fraud.drop(personal_fraud.columns[0])\n",
    "postcode_info = postcode_info.drop(postcode_info.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data frame for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_with_info = consumer_info.join(fraudulent_consumer_rate, on=\"consumer_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average fraud probability in each postcode or state\n",
    "fraudulent_consumer_group_by_postcode = fraudulent_consumer_with_info.groupBy([\"postcode\"]).agg(F.avg(\"fraud_probability\").alias(\"average_fraud_prob_of_postcode\"))\n",
    "\n",
    "fraudulent_consumer_group_by_state = fraudulent_consumer_with_info.groupBy([\"state\"]).agg(F.avg(\"fraud_probability\").alias(\"average_fraud_prob_of_state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average fraud prob for each consumer\n",
    "average_fraudulent_consumer_rate = fraudulent_consumer_rate.groupBy(\"consumer_id\").agg(F.avg(\"fraud_probability\").alias(\"average_fraud_probability\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_rate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique available fraudulent consumer (does not lost any consumer after merging with transaction records)\n",
    "average_fraudulent_consumer_rate.where(average_fraudulent_consumer_rate[\"average_fraud_probability\"]>0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add consumer info to transaction records\n",
    "consumer_transaction_records = transaction_records.join(consumer_info, on=\"consumer_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order has really high variance and value\n",
    "consumer_transaction_records.select(\"dollar_value\").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis order value, consider the variance of order value and purchase frequency\n",
    "consumer_transaction_value_analysis =  consumer_transaction_records.groupBy(\"consumer_id\", \"state\", \"postcode\") \\\n",
    "                                        .agg(\n",
    "                                            F.avg(\"dollar_value\").alias(\"average_dollar_value\"),\n",
    "                                            F.min(\"dollar_value\").alias(\"min_dollar_value\"),\n",
    "                                            F.max(\"dollar_value\").alias(\"max_dollar_value\"),\n",
    "                                            F.count(\"dollar_value\").alias(\"transaction_count\"),\n",
    "                                            F.stddev(\"dollar_value\").alias(\"stddev_dollar_value\")\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_transaction_value_analysis.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_summary = average_fraudulent_consumer_rate \\\n",
    "    .join(consumer_transaction_value_analysis, on=\"consumer_id\", how=\"left\") \\\n",
    "    .join(fraudulent_consumer_group_by_postcode, on=\"postcode\", how=\"inner\") \\\n",
    "    .join(fraudulent_consumer_group_by_state, on=\"state\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_summary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing 2755 in postcode_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get infomation about personal fraud and income from external dataset\n",
    "postcode_info = postcode_info.drop(\"state\", \"long\", \"lat\", \"lgacode\")\n",
    "fraudulent_consumer_summary = fraudulent_consumer_summary.join(personal_fraud, on=\"state\", how=\"inner\")\n",
    "fraudulent_consumer_summary = fraudulent_consumer_summary.join(postcode_info, on=\"postcode\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proportion of the money used to purchase item with respect to income\n",
    "# average income\n",
    "fraudulent_consumer_summary = fraudulent_consumer_summary.withColumn(\"Proportion_between_max_order_value_mean_income\", F.col(\"max_dollar_value\") / (F.col(\"mean_income\") * 1.5) )\n",
    "fraudulent_consumer_summary = fraudulent_consumer_summary.withColumn(\"Proportion_between_max_order_value_median_income\", F.col(\"max_dollar_value\") / (F.col(\"median_income\") * 1.5))\n",
    "\n",
    "# Total income\n",
    "fraudulent_consumer_summary = fraudulent_consumer_summary.withColumn(\"Proportion_between_total_order_value_mean_income\", F.col(\"average_dollar_value\") * F.col(\"transaction_count\") / (F.col(\"mean_income\") * 1.5))\n",
    "fraudulent_consumer_summary = fraudulent_consumer_summary.withColumn(\"Proportion_between_total_order_value_median_income\", F.col(\"average_dollar_value\") * F.col(\"transaction_count\") / (F.col(\"median_income\") * 1.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(F.col(\"average_fraud_probability\").desc())\n",
    "summary_rank_by_fraud_prob = fraudulent_consumer_summary.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "summary_rank_by_fraud_prob.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_summary.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizes features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "\n",
    "cols_to_scale = [\"min_dollar_value\", \"max_dollar_value\", \"stddev_dollar_value\",\"average_dollar_value\"]\n",
    "cols_to_keep_unscaled = [col for col in fraudulent_consumer_summary.columns if col not in cols_to_scale] + [\"average_dollar_value\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols_to_scale, outputCol=\"features\")\n",
    "sdf_transformed = assembler.transform(fraudulent_consumer_summary)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(sdf_transformed.select(\"features\"))\n",
    "sdf_scaled = scaler_model.transform(sdf_transformed)\n",
    "scaled_array_col = vector_to_array(F.col(\"scaledFeatures\"))\n",
    "\n",
    "# Create new columns for each scaled feature\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    sdf_scaled = sdf_scaled.withColumn(f\"scaled_{col}\", scaled_array_col[i])\n",
    "\n",
    "# Combine original Dataframe and the scaled features\n",
    "dollar_value_df = fraudulent_consumer_summary.select(\"average_dollar_value\")\n",
    "fraudulent_consumer_summary = sdf_scaled.select(cols_to_keep_unscaled + [f\"scaled_{col}\" for col in cols_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_summary.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Log transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_log = ['scaled_average_dollar_value', 'scaled_min_dollar_value',\n",
    "                'scaled_max_dollar_value', 'scaled_stddev_dollar_value',\n",
    "                \"Proportion_between_max_order_value_mean_income\",\n",
    "                \"Proportion_between_max_order_value_median_income\",\n",
    "                \"Proportion_between_total_order_value_mean_income\",\n",
    "                \"Proportion_between_total_order_value_median_income\"\n",
    "                ] \n",
    "\n",
    "\n",
    "for col in cols_to_log:\n",
    "    fraudulent_consumer_summary = fraudulent_consumer_summary \\\n",
    "        .withColumn(f'{col}', F.when(fraudulent_consumer_summary[col] > 0, F.log(fraudulent_consumer_summary[col])).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_consumer_summary.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions or Observations:\n",
    "1. The gender plot shows that there is a similar number of male and female consumers.\n",
    "2. The number of consumers varies significantly across different states.\n",
    "3. Consumers make a similar number of purchases on each day of the week, whether itâ€™s a weekday or a weekend.\n",
    "4. Both fraud probability and the dollar value of an order are strongly right-skewed and should be normalized.\n",
    "5. Proportion features exhibit a linear relationship with fraud probability but may need transformation to clarify this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to Pandas\n",
    "df_pandas = fraudulent_consumer_summary.select(\n",
    "    \"average_fraud_probability\", \"scaled_average_dollar_value\", \n",
    "    \"scaled_min_dollar_value\", \"scaled_max_dollar_value\", \"transaction_count\", \n",
    "    \"median_income\", \"mean_income\", \"state\", \"scaled_stddev_dollar_value\",\n",
    "    \"Proportion_between_max_order_value_mean_income\",\n",
    "    \"Proportion_between_max_order_value_median_income\", \n",
    "    \"Proportion_between_total_order_value_mean_income\", \n",
    "    \"Proportion_between_total_order_value_median_income\"\n",
    ").toPandas()\n",
    "\n",
    "# Define plots in a dictionary for looping\n",
    "plots = {\n",
    "    \"Dollar Value Distribution\": (\"scaled_average_dollar_value\", \"hist\"),\n",
    "    \"Max Dollar Value Distribution\": (\"scaled_max_dollar_value\", \"hist\"),\n",
    "    \"Min Dollar Value Distribution\": (\"scaled_min_dollar_value\", \"hist\"),\n",
    "    \"Std Dollar Value Distribution\": (\"scaled_stddev_dollar_value\", \"hist\"),\n",
    "    \"Fraud Probability Distribution\": (\"average_fraud_probability\", \"hist\"),\n",
    "    \"Transaction Count Distribution\": (\"transaction_count\", \"hist\"),\n",
    "    \"State Count\": (\"state\", \"count\"),\n",
    "    \"Scatter 1 (Max Order Value vs Fraud Prob - Mean Income)\": (\"Proportion_between_max_order_value_mean_income\", \"scatter1\"),\n",
    "    \"Scatter 2 (Max Order Value vs Fraud Prob - Median Income)\": (\"Proportion_between_max_order_value_median_income\", \"scatter2\"),\n",
    "    \"Scatter 3 (Total Order Value vs Fraud Prob - Mean Income)\": (\"Proportion_between_total_order_value_mean_income\", \"scatter3\"),\n",
    "    \"Scatter 4 (Total Order Value vs Fraud Prob - Median Income)\": (\"Proportion_between_total_order_value_median_income\", \"scatter4\")\n",
    "}\n",
    "feature_visualisation(df_pandas, plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = fraudulent_consumer_summary.select(\n",
    "    \"average_fraud_probability\", \"scaled_average_dollar_value\", \n",
    "    \"scaled_min_dollar_value\", \"scaled_max_dollar_value\", \"transaction_count\", \n",
    "    \"median_income\", \"mean_income\", \"scaled_stddev_dollar_value\",\n",
    "    \"Proportion_between_max_order_value_mean_income\",\n",
    "    \"Proportion_between_max_order_value_median_income\", \n",
    "    \"Proportion_between_total_order_value_mean_income\", \n",
    "    \"Proportion_between_total_order_value_median_income\"\n",
    ").toPandas()\n",
    "corr_matrix = df_pandas.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "1. Time Frequency feature: https://ieeexplore.ieee.org/document/9399421/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to be used in the model\n",
    "features = [\n",
    "    \"scaled_average_dollar_value\", \"scaled_min_dollar_value\", \n",
    "    \"scaled_max_dollar_value\", \"transaction_count\", \n",
    "    \"average_fraud_prob_of_postcode\", \"scaled_stddev_dollar_value\",\n",
    "    \"Proportion_between_max_order_value_mean_income\",\n",
    "    \"Proportion_between_max_order_value_median_income\", \n",
    "    \"Proportion_between_total_order_value_mean_income\", \n",
    "    \"Proportion_between_total_order_value_median_income\"\n",
    "]\n",
    "\n",
    "features_dt = [\n",
    "    \"scaled_max_dollar_value\", \n",
    "    \"average_fraud_prob_of_postcode\"\n",
    "]\n",
    "\n",
    "features_rf = [\n",
    "    \"scaled_max_dollar_value\", \n",
    "    \"average_fraud_prob_of_postcode\", \n",
    "    \"Proportion_between_max_order_value_median_income\", \n",
    "    \"scaled_stddev_dollar_value\", \n",
    "    \"Proportion_between_max_order_value_mean_income\", \n",
    "    \"scaled_average_dollar_value\"\n",
    "]\n",
    "\n",
    "features_lr = [\n",
    "    \"scaled_stddev_dollar_value\", \n",
    "    \"Proportion_between_max_order_value_median_income\", \n",
    "    \"average_fraud_prob_of_postcode\", \n",
    "    \"Proportion_between_max_order_value_mean_income\", \n",
    "    \"Proportion_between_total_order_value_mean_income\", \n",
    "    \"Proportion_between_total_order_value_median_income\", \n",
    "    \"scaled_average_dollar_value\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# VectorAssembler to combine the features into a single vector\n",
    "assembler_dt = VectorAssembler(inputCols=features_dt, outputCol=\"features\")\n",
    "assembler_rf = VectorAssembler(inputCols=features_rf, outputCol=\"features\")\n",
    "assembler_lr = VectorAssembler(inputCols=features_lr, outputCol=\"features\")\n",
    "\n",
    "# Prepare the data\n",
    "data_dt = assembler_dt.transform(fraudulent_consumer_summary)\n",
    "data_rf = assembler_rf.transform(fraudulent_consumer_summary)\n",
    "data_lr = assembler_lr.transform(fraudulent_consumer_summary)\n",
    "\n",
    "train_data_dt, test_data_dt = data_dt.randomSplit([0.8, 0.2])\n",
    "train_data_rf, test_data_rf = data_rf.randomSplit([0.8, 0.2])\n",
    "train_data_lr, test_data_lr = data_lr.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model regressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"average_fraud_probability\", featuresCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"average_fraud_probability\", featuresCol=\"features\")\n",
    "\n",
    "lr = LinearRegression(labelCol=\"average_fraud_probability\", featuresCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 7]) \\\n",
    "    .build()\n",
    "\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator for regression models\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"average_fraud_probability\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"  \n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"average_fraud_probability\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\" \n",
    ")\n",
    "\n",
    "# Cross-validation \n",
    "dt_cv = CrossValidator(\n",
    "    estimator=dt,\n",
    "    estimatorParamMaps=dt_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=lr_param_grid,\n",
    "    evaluator=r2_evaluator,\n",
    "    numFolds=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline \n",
    "dt_pipeline = Pipeline(stages=[dt_cv])\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[rf_cv])\n",
    "\n",
    "lr_pipeline = Pipeline(stages=[lr_cv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "\n",
    "# 6 mins\n",
    "dt_model = dt_pipeline.fit(train_data_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 mins\n",
    "rf_model = rf_pipeline.fit(train_data_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 mins\n",
    "lr_model = lr_pipeline.fit(train_data_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "dt_predictions = dt_model.transform(test_data_dt)\n",
    "rf_predictions = rf_model.transform(test_data_rf)\n",
    "lr_predictions = lr_model.transform(test_data_lr)\n",
    "\n",
    "\n",
    "dt_rmse = rmse_evaluator.evaluate(dt_predictions)\n",
    "dt_r2 = r2_evaluator.evaluate(dt_predictions)\n",
    "print(f\"Decision Tree RMSE: {dt_rmse}\")\n",
    "print(f\"Decision Tree R2: {dt_r2}\")           # RMSE: 6.321439678625029 R2: 0.5426608139433955\n",
    "\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
    "print(f\"Random Forest R2: {rf_r2}\")            # RMSE: 6.2324836442813565 R2: 0.5554417100291846\n",
    "\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
    "print(f\"Linear Regression R2: {lr_r2}\")  # RMSE: 7.031157543875003 R2: 0.434203734698708\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Best model hyperparameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt_model = dt_model.stages[-1].bestModel\n",
    "print(f\"Best Decision Tree maxDepth: {best_dt_model._java_obj.getMaxDepth()}\")\n",
    "print(f\"Best Decision Tree maxBins: {best_dt_model._java_obj.getMaxBins()}\")\n",
    "\n",
    "\n",
    "best_rf_model = rf_model.stages[-1].bestModel\n",
    "print(f\"Best Random Forest numTrees: {best_rf_model.getNumTrees}\")\n",
    "print(f\"Best Random Forest maxDepth: {best_rf_model.getMaxDepth()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Feature importances__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_dt = assembler_dt.getInputCols()\n",
    "feature_names_rf = assembler_rf.getInputCols()\n",
    "feature_names_lr = assembler_lr.getInputCols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random forest and decision tree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = rf_model.stages[0].bestModel\n",
    "best_dt_model = dt_model.stages[0].bestModel\n",
    "\n",
    "dt_feature_importances = best_dt_model.featureImportances\n",
    "rf_feature_importances = best_rf_model.featureImportances\n",
    "\n",
    "rf_importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names_rf,\n",
    "    \"Importance\": rf_feature_importances.toArray()\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "dt_importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names_dt,\n",
    "    \"Importance\": dt_feature_importances.toArray()\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(rf_importances_df)\n",
    "print()\n",
    "print(dt_importances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get coefficients \n",
    "best_lr_model = lr_model.stages[0].bestModel\n",
    "coefficients = best_lr_model.coefficients\n",
    "# Get feature names from the VectorAssembler\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": feature_names_lr,\n",
    "    \"Coefficient\": coefficients\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions.select(\"average_fraud_probability\", \"prediction\").show(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
